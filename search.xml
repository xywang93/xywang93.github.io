<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>树莓派NCNN环境搭建</title>
      <link href="/2018/05/07/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/02-%E6%A0%91%E8%8E%93%E6%B4%BEncnn%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/05/07/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/02-%E6%A0%91%E8%8E%93%E6%B4%BEncnn%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      <content type="html"><![CDATA[<h2 id="树莓派NCNN环境搭建"><a href="#树莓派NCNN环境搭建" class="headerlink" title="树莓派NCNN环境搭建"></a>树莓派NCNN环境搭建</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>镜像已经做好了，传到百度网盘中了（请大家及时保存，不定期删除！）</p><p>链接: <a href="https://pan.baidu.com/s/1fhiX86L8iL8tsLbsiVa6Wg" target="_blank" rel="noopener">https://pan.baidu.com/s/1fhiX86L8iL8tsLbsiVa6Wg</a> 密码: e64s</p><p><strong>SD卡要求：至少16GB，板卡型号为树莓派3B+（其他型号未知）</strong></p><h3 id="板卡介绍"><a href="#板卡介绍" class="headerlink" title="板卡介绍"></a>板卡介绍</h3><p>本系列教程采用树莓派3B+开发板：<br><img src="http://xukeqiniu.xukeai.cn/0a676958d61c43741f8cc45f04d3ea1b.png" alt=""><br>板卡特点：</p><ul><li>1.4GHz 64位4核 ARM Cortex-A53 CPU</li><li>双频 802.11ac 无线网卡和蓝牙 4.2</li><li>更快的以太网（千兆以太网 over USB 2.0）</li><li>1G LPDDR2</li><li>PoE 支持（Power-over-Ethernet，with PoE HAT）</li><li>改进 PXE 网络与 USB 大容量存储启动</li></ul><a id="more"></a><h3 id="系统安装"><a href="#系统安装" class="headerlink" title="系统安装"></a>系统安装</h3><h4 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h4><ul><li>SDFormatter(格式化SD卡)</li><li>win32diskimager（为SD卡烧写程序）<br>-<h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4></li></ul><p>首先用SDFormatter将插入的SD卡格式化。<br>然后用win32diskimager找到对应的镜像烧写到SD卡中（原始镜像采用2018-04-18-raspbian-stretch树莓派官方系统）</p><h3 id="更换源"><a href="#更换源" class="headerlink" title="更换源"></a>更换源</h3><h4 id="一步操作"><a href="#一步操作" class="headerlink" title="一步操作"></a>一步操作</h4><p>直接执行以下两步，即可替换将官方默认软件源替换为<br>中科大镜像源<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sed -i <span class="string">'s#://mirrordirector.raspbian.org#s://mirrors.ustc.edu.cn/raspbian#g'</span> /etc/apt/sources.list</span><br><span class="line">$ sudo sed -i <span class="string">'s#://archive.raspberrypi.org/debian#s://mirrors.ustc.edu.cn/archive.raspberrypi.org#g'</span> /etc/apt/sources.list.d/raspi.list</span><br></pre></td></tr></table></figure></p><p>或换为清华镜像源<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sed -i <span class="string">'s#://mirrordirector.raspbian.org#s://mirrors.tuna.tsinghua.edu.cn/raspbian#g'</span> /etc/apt/sources.list</span><br><span class="line">$ sudo sed -i <span class="string">'s#://archive.raspberrypi.org/debian#s://mirrors.tuna.tsinghua.edu.cn/raspberrypi#g'</span> /etc/apt/sources.list.d/raspi.list</span><br></pre></td></tr></table></figure></p><h4 id="或手动修改源"><a href="#或手动修改源" class="headerlink" title="或手动修改源"></a>或手动修改源</h4><ul><li>第一步：修改<code>sources.list</code><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/apt/sources.list</span><br></pre></td></tr></table></figure></li></ul><p>进入之后，屏蔽掉其他的源，输入以下源：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi</span><br><span class="line">$ deb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi</span><br></pre></td></tr></table></figure></p><ul><li>第二步：修改<code>raspi.list</code><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/apt/sources.list.d/raspi.list</span><br></pre></td></tr></table></figure></li></ul><p>进入之后，屏蔽掉其他的源，输入以下源：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ deb http://mirror.tuna.tsinghua.edu.cn/raspberrypi/ stretch main ui</span><br><span class="line">$ deb-src http://mirror.tuna.tsinghua.edu.cn/raspberrypi/ stretch main ui</span><br></pre></td></tr></table></figure></p><ul><li>第三部修改完源后更新升级系统<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get upgrade</span><br></pre></td></tr></table></figure></li></ul><h3 id="安装VScode"><a href="#安装VScode" class="headerlink" title="安装VScode"></a>安装VScode</h3><p>提供了VScode ARM安装包，直接安装！<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo dpkg -i code-oss_1.14.0-1497990172_armhf.deb</span><br></pre></td></tr></table></figure></p><h3 id="安装cmake工具"><a href="#安装cmake工具" class="headerlink" title="安装cmake工具"></a>安装cmake工具</h3><h4 id="安装cmake"><a href="#安装cmake" class="headerlink" title="安装cmake"></a>安装cmake</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install cmake</span><br></pre></td></tr></table></figure><h4 id="安装cmake-gui"><a href="#安装cmake-gui" class="headerlink" title="安装cmake-gui"></a>安装cmake-gui</h4><p>cmake-gui是可视化的cmake工具，便于配置。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install cmake-gui</span><br></pre></td></tr></table></figure></p><h3 id="安装-protobuf"><a href="#安装-protobuf" class="headerlink" title="安装 protobuf"></a>安装 protobuf</h3><p>下载 protobuf-2.6.1.tar.gz<br>安装<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#解压文件</span></span><br><span class="line">$ tar zxvf protobuf-2.6.1.tar.gz</span><br><span class="line">$ <span class="comment">#进入文件</span></span><br><span class="line">$ <span class="built_in">cd</span> protobuf-2.6.1/</span><br><span class="line">$ <span class="comment"># 配置</span></span><br><span class="line">$ ./configure</span><br><span class="line">$ <span class="comment"># 编译（编译过程尽量只用一个核 不要加 j4,j3,j2）</span></span><br><span class="line">$ make</span><br><span class="line">$ <span class="comment"># 编译检查</span></span><br><span class="line">$ make check</span><br><span class="line">$ <span class="comment"># 安装</span></span><br><span class="line">$ sudo make install</span><br><span class="line">$ <span class="comment"># 添加库路径 在/etc/ld.so.conf.d/目录下创建文件bprotobuf.conf文件</span></span><br><span class="line">$ <span class="built_in">cd</span> /etc/ld.so.conf.d/</span><br><span class="line">$ sudo touch bprotobuf.conf</span><br><span class="line">$ <span class="comment"># vim打开bprotobuf.conf文件输入/usr/local/lib</span></span><br><span class="line">$ sudo ldconfig</span><br><span class="line">$ <span class="comment"># 查看版本</span></span><br><span class="line">$ protoc –-version</span><br></pre></td></tr></table></figure></p><h3 id="安装opencv3-4"><a href="#安装opencv3-4" class="headerlink" title="安装opencv3.4"></a>安装opencv3.4</h3><h4 id="预装依赖库"><a href="#预装依赖库" class="headerlink" title="预装依赖库"></a>预装依赖库</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install build-essential git cmake pkg-config -y</span><br><span class="line">$ sudo apt-get install libjpeg8-dev -y</span><br><span class="line">$ sudo apt-get install libtiff5-dev -y</span><br><span class="line">$ sudo apt-get install libjasper-dev -y</span><br><span class="line">$ sudo apt-get install libpng12-dev -y</span><br><span class="line">$ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev -y</span><br><span class="line">$ sudo apt-get install libgtk2.0-dev -y</span><br><span class="line">$ sudo apt-get install libatlas-base-dev gfortran -y</span><br><span class="line">$ sudo apt-get install qt5-default -y</span><br></pre></td></tr></table></figure><p>安装numpy（命令行），第一次使用pip安装时可能会比较慢，耐心等待<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo pip3 install numpy</span><br></pre></td></tr></table></figure></p><p>查看numpy的安装目录并记录（<strong>后面编译时需要PYTHON3_NUMPY_INCLUDE_DIRS路径，如果不带numpy编译可能会卡住</strong>）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ python3</span><br><span class="line">&gt;&gt; <span class="keyword">import</span> numpy</span><br><span class="line">&gt;&gt; numpy.__path__</span><br><span class="line">&gt;&gt; quit()</span><br></pre></td></tr></table></figure></p><h4 id="下载并解压OpenCV"><a href="#下载并解压OpenCV" class="headerlink" title="下载并解压OpenCV"></a>下载并解压OpenCV</h4><p>然后下载OpenCV库和Contrib库（强烈建议在其他环境下载然后拷贝过来），如果需要其他版本，就修改后面的版本号<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/Itseez/opencv/archive/3.4.0.zip</span><br><span class="line">https://github.com/Itseez/opencv_contrib/archive/3.4.0.zip</span><br></pre></td></tr></table></figure></p><h4 id="cmake配置"><a href="#cmake配置" class="headerlink" title="cmake配置"></a>cmake配置</h4><p>解压后在opencv-3.4.0文件夹里创建build文件夹，然后在命令行里面cd到此文件夹，开始cmake，以下内容为一行<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_LIBV4L=ON PYTHON3_EXECUTABLE=/usr/bin/python3.5 PYTHON_INCLUDE_DIR=/usr/include/python3.5 PYTHON_LIBRARY=/usr/lib/arm-linux-gnueabihf/libpython3.5m.so PYTHON3_NUMPY_INCLUDE_DIRS=/home/pi/.<span class="built_in">local</span>/lib/python3.5/site-packages/numpy/core/include ..</span><br><span class="line"></span><br><span class="line"><span class="comment">## 备选方案</span></span><br><span class="line">sudo aptitude search libgtk2.0-dev</span><br><span class="line"></span><br><span class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_QT=ON -D WITH_GTK=ON -D WITH_OPENGL=ON ..</span><br></pre></td></tr></table></figure></p><h4 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h4><p>执行如下操作：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编译（编译过程尽量只用一个核 不要加 j4,j3,j2）</span></span><br><span class="line">$ make</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure></p><h3 id="编译NCNN"><a href="#编译NCNN" class="headerlink" title="编译NCNN"></a>编译NCNN</h3><h4 id="下载NCNN"><a href="#下载NCNN" class="headerlink" title="下载NCNN"></a>下载NCNN</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/Tencent/ncnn.github</span><br></pre></td></tr></table></figure><h4 id="需要安装的依赖库"><a href="#需要安装的依赖库" class="headerlink" title="需要安装的依赖库"></a>需要安装的依赖库</h4><ul><li>protobuf</li><li>opencv</li></ul><h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ncnn</span><br><span class="line">$ mkdir build</span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make</span><br></pre></td></tr></table></figure><h3 id="NCNN测试"><a href="#NCNN测试" class="headerlink" title="NCNN测试"></a>NCNN测试</h3><p>修改项目根目录下的<code>CMakeLists.txt</code>文件，定位到最后几行<br><img src="http://xukeqiniu.xukeai.cn/22dde3b5cb5f30c9939efc595c59c389.png" alt=""></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make</span><br><span class="line">$ cp examples/squeezenet_v1.1.param  build/examples</span><br><span class="line">$ cp examples/squeezenet_v1.1.bin    build/examples</span><br><span class="line">$ <span class="built_in">cd</span> build/examples</span><br><span class="line">$ ./squeezenet cat.jpg</span><br></pre></td></tr></table></figure><p>结果：<br><img src="http://xukeqiniu.xukeai.cn/408d7bac009ae9ba743ed1bff5cb996c.png" alt=""></p><h3 id="制作img系统镜像"><a href="#制作img系统镜像" class="headerlink" title="制作img系统镜像"></a>制作img系统镜像</h3><ul><li><p>安装dcfldd工具</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install dcfldd</span><br></pre></td></tr></table></figure></li><li><p>将已经安装好软件的内存卡插到pc机上，在虚拟机中打开可移动设备。</p></li><li>查看备份设备<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /dev</span><br><span class="line">ls</span><br></pre></td></tr></table></figure></li></ul><p>可以看到插入设备(整体)为sdb，其中有几个分区(sdb1、sdb2、sdb3.)<br><img src="http://xukeqiniu.xukeai.cn/4ecc8595baa8f4eb357a3fee595864d8.png" alt=""></p><ul><li>备份分区</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dcfldd <span class="keyword">if</span>=/dev/sdb of=/root/pi.img</span><br></pre></td></tr></table></figure><blockquote><ul><li>由于我们是将整个内存卡的全部做成镜像(<strong>启动+系统</strong>)，所以输入文件为sdb，若只做系统部分的镜像，可以只选择sdb2.</li><li>备份包括输入文件(if)以及被设置为/root目录下名为pi.img的输出文件(of)。</li></ul></blockquote><h3 id="调整SD卡分区"><a href="#调整SD卡分区" class="headerlink" title="调整SD卡分区"></a>调整SD卡分区</h3><h4 id="使用工具："><a href="#使用工具：" class="headerlink" title="使用工具："></a>使用工具：</h4><p>Linux Ubuntu gparted分区工具</p><h4 id="安装gparted"><a href="#安装gparted" class="headerlink" title="安装gparted"></a>安装gparted</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install gparted</span><br></pre></td></tr></table></figure><h4 id="调整分区容量大小"><a href="#调整分区容量大小" class="headerlink" title="调整分区容量大小"></a>调整分区容量大小</h4><p><img src="http://xukeqiniu.xukeai.cn/df4e237083510783efa110102bbde7b9.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/72c3d1a1f09fbe64faf3ce8af4acce9b.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/34b7f42f56e8d72a93139c6adf6717c5.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/c47170d5af9ce81d2dad4ae897b3145e.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/6e6481769c1a86d76c8d6df3e087fdb0.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.jianshu.com/p/67b9e6ebf8a0" target="_blank" rel="noopener">https://www.jianshu.com/p/67b9e6ebf8a0</a><br><a href="http://www.rabbit8.cn/609.html" target="_blank" rel="noopener">http://www.rabbit8.cn/609.html</a><br><a href="https://linux.cn/article-8477-1.html" target="_blank" rel="noopener">https://linux.cn/article-8477-1.html</a><br><a href="https://www.cnblogs.com/Pyrokine/p/8921285.html" target="_blank" rel="noopener">https://www.cnblogs.com/Pyrokine/p/8921285.html</a><br><a href="https://stackoverflow.com/questions/28776053/opencv-gtk2-x-error" target="_blank" rel="noopener">https://stackoverflow.com/questions/28776053/opencv-gtk2-x-error</a></p><p>相关人工智能与异构计算的知识分享，欢迎关注我的公众号<strong>AI异构</strong><br><img src="http://xukeqiniu.xukeai.cn/64b3b3bf7b3e12ed0f62f94fd001cffb.png" alt="动动手指关注下吧！"></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> 嵌入式AI </category>
          
          <category> NCNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> 嵌入式AI </tag>
            
            <tag> NCNN </tag>
            
            <tag> 入门 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NCNN ResNet-50 部署</title>
      <link href="/2018/05/05/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/01-ncnn%20resnet%E9%83%A8%E7%BD%B2/"/>
      <url>/2018/05/05/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/01-ncnn%20resnet%E9%83%A8%E7%BD%B2/</url>
      <content type="html"><![CDATA[<h3 id="说在前面"><a href="#说在前面" class="headerlink" title="说在前面"></a>说在前面</h3><p>因为NCNN的部署是不依赖任何第三方库的，所以在模型部署的过程中，只需要了解CMake编译工程的步骤。具体详见<a href="http://www.hahack.com/codes/cmake/" target="_blank" rel="noopener">CMake 入门实践</a></p><h3 id="下载ResNet-50模型"><a href="#下载ResNet-50模型" class="headerlink" title="下载ResNet-50模型"></a>下载ResNet-50模型</h3><p><a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank" rel="noopener">caffe model zoo</a>中有对应的各种不同版本的训练好的model,因为是部署，所以我们需要的仅仅是对应的caffemodel文件和deploy.prototxt文件。</p><a id="more"></a><h3 id="转换ncnn网络和模型"><a href="#转换ncnn网络和模型" class="headerlink" title="转换ncnn网络和模型"></a>转换ncnn网络和模型</h3><ul><li>首先建立resnet源文件目录</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">$ <span class="built_in">cd</span> examples</span><br><span class="line">$ mkdir resnet</span><br><span class="line">$ <span class="built_in">cd</span> resnet</span><br><span class="line">$ mkdir model</span><br></pre></td></tr></table></figure><p>将下载好的ResNet-50源文件复制到model文件夹中如下所示：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ root@a488d431ffee:/home/dl/ncnn/examples/resnet/model<span class="comment"># ls</span></span><br><span class="line">ResNet-50-deploy.prototxt  ResNet-50-model.caffemodel</span><br></pre></td></tr></table></figure></p><ul><li>caffe 自带了工具可以把老版本的 caffe 网络和模型转换为新版（ncnn的工具只认识新版）</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">upgrade_net_proto_text ResNet-50-deploy.prototxt  ncnn-ResNet-50-deploy.prototxt</span><br><span class="line">upgrade_net_proto_binary ResNet-50-model.caffemodel ncnn-ResNet-50-model.caffemodel</span><br></pre></td></tr></table></figure><ul><li>输入层改用 Input，因为每次只需要做一个图片，所以第一个 dim 设为 1</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"input"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Input"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  input_param &#123;</span><br><span class="line">    shape &#123;</span><br><span class="line">      dim: 1</span><br><span class="line">      dim: 3</span><br><span class="line">      dim: 224</span><br><span class="line">      dim: 224</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用 caffe2ncnn 工具转换为 ncnn 的网络描述和模型</li></ul><p>注意caffe2ncnn工具在</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">$ <span class="built_in">cd</span> build/tools/caffe/</span><br><span class="line">$ caffe2ncnn ../../../examples/resnet/model/ncnn-ResNet-50-deploy.prototxt  ../../../examples/resnet/model/ncnn-ResNet-50-model.caffemodel ../../../examples/resnet/model/resnet-50.param  ../../../examples/resnet/model/resnet-50.bin</span><br></pre></td></tr></table></figure><p>准备工作全部完成如下,我们最终需要的是<code>resnet-50.bin</code>和<code>resnet-50.param</code>两个文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@a488d431ffee:/home/dl/ncnn/examples/resnet/model<span class="comment"># ls</span></span><br><span class="line">ResNet-50-deploy.prototxt  ResNet-50-model.caffemodel  ncnn-ResNet-50-deploy.prototxt  ncnn-ResNet-50-model.caffemodel  resnet-50.bin  resnet-50.param</span><br></pre></td></tr></table></figure></p><h3 id="建立ResNet工程"><a href="#建立ResNet工程" class="headerlink" title="建立ResNet工程"></a>建立ResNet工程</h3><ul><li><p>需要修改对应的CMakeLists.txt</p><ul><li><p>首先修改examples文件夹下的CMakeLists.txt文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最后一行添加</span></span><br><span class="line">add_subdirectory(resnet)</span><br></pre></td></tr></table></figure></li><li><p>然后进入resnet文件夹，创建CMakeLists.txt文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">find_package(OpenCV REQUIRED core highgui )</span><br><span class="line"></span><br><span class="line">include_directories(<span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/../../src)</span><br><span class="line">include_directories(<span class="variable">$&#123;CMAKE_CURRENT_BINARY_DIR&#125;</span>/../../src)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">include_directories(<span class="variable">$&#123;CMAKE_CURRENT_BINARY_DIR&#125;</span>)</span><br><span class="line"></span><br><span class="line">add_executable(resnet-50 resnet_50.cpp)</span><br><span class="line">target_link_libraries(resnet-50 ncnn <span class="variable">$&#123;OpenCV_LIBS&#125;</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>编写resnet_50.cpp（仿照SqueezeNet）</p></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"net.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">detect_resnet</span><span class="params">(<span class="keyword">const</span> cv::Mat&amp; bgr, <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;&amp; cls_scores)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ncnn::Net resnet;</span><br><span class="line">    resnet.load_param(<span class="string">"resnet-50.param"</span>);</span><br><span class="line">    resnet.load_model(<span class="string">"resnet-50.bin"</span>);</span><br><span class="line"></span><br><span class="line">    ncnn::Mat in = ncnn::Mat::from_pixels_resize(bgr.data, ncnn::Mat::PIXEL_BGR, bgr.cols, bgr.rows, <span class="number">224</span>, <span class="number">224</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> mean_vals[<span class="number">3</span>] = &#123;<span class="number">104.f</span>, <span class="number">117.f</span>, <span class="number">123.f</span>&#125;;</span><br><span class="line">    in.substract_mean_normalize(mean_vals, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    ncnn::Extractor ex = resnet.create_extractor();</span><br><span class="line">    ex.set_light_mode(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">    ex.input(<span class="string">"data"</span>, in);</span><br><span class="line"></span><br><span class="line">    ncnn::Mat out;</span><br><span class="line">    ex.extract(<span class="string">"prob"</span>, out);</span><br><span class="line"></span><br><span class="line">    cls_scores.resize(out.c);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;out.c; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">float</span>* prob = out.data + out.cstep * j;</span><br><span class="line">        cls_scores[j] = prob[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">print_topk</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;&amp; cls_scores, <span class="keyword">int</span> topk, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; index_result, <span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;&amp; score_result)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// partial sort topk with index</span></span><br><span class="line">    <span class="keyword">int</span> size = cls_scores.size();</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt; <span class="built_in">std</span>::pair&lt;<span class="keyword">float</span>, <span class="keyword">int</span>&gt; &gt; vec;</span><br><span class="line">    vec.resize(size);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;size; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        vec[i] = <span class="built_in">std</span>::make_pair(cls_scores[i], i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::partial_sort(vec.begin(), vec.begin() + topk, vec.end(), <span class="built_in">std</span>::greater&lt; <span class="built_in">std</span>::pair&lt;<span class="keyword">float</span>, <span class="keyword">int</span>&gt; &gt;());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// print topk and score</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;topk; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">float</span> score = vec[i].first;</span><br><span class="line">        <span class="keyword">int</span> index = vec[i].second;</span><br><span class="line">index_result.push_back(index);</span><br><span class="line">        score_result.push_back(score);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//fprintf(stderr, "%d = %f\n", index, score);</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">load_labels</span><span class="params">(<span class="built_in">string</span> path, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; labels)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    FILE* fp = fopen(path.c_str(), <span class="string">"r"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (!feof(fp))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">char</span> str[<span class="number">1024</span>];</span><br><span class="line">        fgets(str, <span class="number">1024</span>, fp);</span><br><span class="line">        <span class="function"><span class="built_in">string</span> <span class="title">str_s</span><span class="params">(str)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (str_s.length() &gt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; str_s.length(); i++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span> (str_s[i] == <span class="string">' '</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="built_in">string</span> strr = str_s.substr(i, str_s.length() - i - <span class="number">1</span>);</span><br><span class="line">                    labels.push_back(strr);</span><br><span class="line">                    i = str_s.length();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span>* imagepath = argv[<span class="number">1</span>];</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; labels;</span><br><span class="line">    load_labels(<span class="string">"synset_words.txt"</span>, labels);</span><br><span class="line">    cv::Mat m = cv::imread(imagepath, CV_LOAD_IMAGE_COLOR);</span><br><span class="line">    <span class="keyword">if</span> (m.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"cv::imread %s failed\n"</span>, imagepath);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; cls_scores;</span><br><span class="line">    detect_resnet(m, cls_scores);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; index;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; score;</span><br><span class="line">    print_topk(cls_scores, <span class="number">3</span>, index, score);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index.size(); i++)</span><br><span class="line">    &#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; labels[index[i]] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make</span><br><span class="line">$ <span class="built_in">cd</span> resnet</span><br><span class="line">$ cp ../../../examples/resnet/model/resnet-50.param  ./</span><br><span class="line">$ cp ../../../examples/resnet/model/resnet-50.bin    ./</span><br><span class="line">$ ./resnet-50 cat.jpg</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Pembroke, Pembroke Welsh corgi</span><br><span class="line">Egyptian cat</span><br><span class="line">red fox, Vulpes vulpes</span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://www.hahack.com/codes/cmake/" target="_blank" rel="noopener">CMake 入门实践</a><br><a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank" rel="noopener">caffe model zoo</a><br><a href="https://github.com/Tencent/ncnn/wiki/ncnn-%E7%BB%84%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97-alexnet" target="_blank" rel="noopener">ncnn 组件使用指北 alexnet</a><br><a href="http://blog.csdn.net/best_coder/article/details/76201275" target="_blank" rel="noopener">Ubuntu16.04—腾讯NCNN框架入门到应用</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> 嵌入式AI </category>
          
          <category> NCNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> 嵌入式AI </tag>
            
            <tag> NCNN </tag>
            
            <tag> ResNet </tag>
            
            <tag> 部署 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NCNN入门</title>
      <link href="/2018/05/04/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/00-ncnn%E5%85%A5%E9%97%A8/"/>
      <url>/2018/05/04/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/00-ncnn%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<h3 id="NCNN-简介"><a href="#NCNN-简介" class="headerlink" title="NCNN 简介"></a>NCNN 简介</h3><p>ncnn 是一个为手机端极致优化的高性能神经网络前向计算框架。ncnn 从设计之初深刻考虑手机端的部署和使用。无第三方依赖，跨平台，手机端 cpu 的速度快于目前所有已知的开源框架。基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP，将 AI 带到你的指尖。ncnn 目前已在腾讯多款应用中使用，如 QQ，Qzone，微信，天天P图等。<br> ncnn与同类框架对比：<br><img src="http://xukeqiniu.xukeai.cn/340953f4416410ed2091d1de5f9946e6.png" alt=""></p><a id="more"></a><h3 id="功能概述"><a href="#功能概述" class="headerlink" title="功能概述"></a>功能概述</h3><ul><li>支持卷积神经网络，支持多输入和多分支结构，可计算部分分支</li></ul><p>ncnn 支持卷积神经网络结构，以及多分支多输入的复杂网络结构，如主流的 vgg、googlenet、resnet、squeezenet 等。计算时可以依据需求，先计算公共部分和 prob 分支，待 prob 结果超过阈值后，再计算 bbox 分支。如果 prob 低于阈值，则可以不计算 bbox 分支，减少计算量。</p><ul><li>无任何第三方库依赖，不依赖 BLAS/NNPACK 等计算框架</li></ul><p>cnn 不依赖任何第三方库，完全独立实现所有计算过程，不需要 BLAS/NNPACK 等数学计算库。</p><ul><li>纯 C++ 实现，跨平台，支持 android ios 等</li></ul><p>ncnn 代码全部使用 C/C++ 实现，跨平台的 cmake 编译系统，可在已知的绝大多数平台编译运行，如 Linux，Windows，MacOS，Android，iOS 等。由于 ncnn 不依赖第三方库，且采用 C++ 03 标准实现，只用到了 std::vector 和 std::string 两个 STL 模板，可轻松移植到其他系统和设备上。</p><ul><li>ARM NEON 汇编级良心优化，计算速度极快</li></ul><p>ncnn 为手机端 CPU 运行做了深度细致的优化，使用 ARM NEON 指令集实现卷积层，全连接层，池化层等大部分 CNN 关键层。对于寄存器压力较大的 armv7 架构，手工编写 neon 汇编，内存预对齐，cache 预缓存，排列流水线，充分利用一切硬件资源，防止编译器意外负优化。</p><ul><li>精细的内存管理和数据结构设计，内存占用极低</li></ul><p>在 ncnn 设计之初已考虑到手机上内存的使用限制，在卷积层、全连接层等计算量较大的层实现中，没有采用通常框架中的 im2col + 矩阵乘法，因为这种方式会构造出非常大的矩阵，消耗大量内存。因此，ncnn 采用原始的滑动窗口卷积实现，并在此基础上进行优化，大幅节省了内存。在前向网络计算过程中，ncnn 可自动释放中间结果所占用的内存，进一步减少内存占用。</p><ul><li>支持多核并行计算加速，ARM big.LITTLE cpu 调度优化</li></ul><p>ncnn 提供了基于 openmp 的多核心并行计算加速，在多核心 cpu 上启用后能够获得很高的加速收益。ncnn 提供线程数控制接口，可以针对每个运行实例分别调控，满足不同场景的需求。针对 ARM big.LITTLE 架构的手机 cpu，ncnn 提供了更精细的调度策略控制功能，能够指定使用大核心或者小核心，或者一起使用，获得极限性能和耗电发热之间的平衡。例如，只使用1个小核心，或只使用2个小核心，或只使用2个大核心，都尽在掌控之中。<br><img src="http://xukeqiniu.xukeai.cn/dcdd662eef37b3dd83357f07fa9831c2.png" alt=""></p><ul><li>整体库体积小于 500K，并可轻松精简到小于 300K</li></ul><p>ncnn 自身没有依赖项，且体积很小，默认编译选项下的库体积小于 500K，能够有效减轻手机 APP 安装包大小负担。此外，ncnn 在编译时可自定义是否需要文件加载和字符串输出功能，还可自定义去除不需要的层实现，轻松精简到小于 300K。</p><ul><li>可扩展的模型设计，支持 8bit 量化和半精度浮点存储，可导入 caffe 模型</li></ul><p>ncnn 使用自有的模型格式，模型主要存储模型中各层的权重值。ncnn 模型中含有扩展字段，用于兼容不同权重值的存储方式，如常规的单精度浮点，以及占用更小的半精度浮点和 8bit 量化数。大部分深度模型都可以采用半精度浮点减小一半的模型体积，减少 APP 安装包大小和在线下载模型的耗时。ncnn 带有 caffe 模型转换器，可以转换为 ncnn 的模型格式，方便研究成果快速落地。<br><img src="http://xukeqiniu.xukeai.cn/0b7c0d0fb225db788d66d3e2a0468fca.png" alt=""></p><ul><li>支持直接内存零拷贝引用加载网络模型</li></ul><p>在某些特定应用场景中，如因平台层 API 只能以内存形式访问模型资源，或者希望将模型本身作为静态数据写在代码里，ncnn 提供了直接从内存引用方式加载网络模型的功能。这种加载方式不会拷贝已在内存中的模型，也无需将模型先写入实体的文件再读入，效率极高。</p><ul><li>可注册自定义层实现并扩展</li></ul><p>ncnn 提供了注册自定义层实现的扩展方式，可以将自己实现的特殊层内嵌到 ncnn 的前向计算过程中，组合出更自由的网络结构和更强大的特性。</p><h3 id="入门实践"><a href="#入门实践" class="headerlink" title="入门实践"></a>入门实践</h3><h4 id="Build-for-Linux-x86"><a href="#Build-for-Linux-x86" class="headerlink" title="Build for Linux x86"></a>Build for Linux x86</h4><ul><li><p>实验环境</p></li><li><p>预先配置好Caffe(其实之前预先安装 g++ cmake protobuf即可)</p></li><li><p>安装opencv</p></li><li><p>实践example</p></li></ul><p>首先编译一个工程目录,生成对应的各种工具<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make -j4</span><br></pre></td></tr></table></figure></p><p>然后修改项目根目录下的CMakeLists.txt文件，定位到最后几行<br><img src="http://xukeqiniu.xukeai.cn/22dde3b5cb5f30c9939efc595c59c389.png" alt=""></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make</span><br><span class="line">$ cp examples/squeezenet_v1.1.param  build/examples</span><br><span class="line">$ cp examples/squeezenet_v1.1.bin    build/examples</span><br><span class="line">$ <span class="built_in">cd</span> build/examples</span><br><span class="line">$ ./squeezenet cat.jpg</span><br></pre></td></tr></table></figure><p>结果：<br><img src="http://xukeqiniu.xukeai.cn/408d7bac009ae9ba743ed1bff5cb996c.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Tencent/ncnn/wiki/how-to-build" target="_blank" rel="noopener">https://github.com/Tencent/ncnn/wiki/how-to-build</a><br><a href="http://blog.csdn.net/fuwenyan/article/details/76576708" target="_blank" rel="noopener">http://blog.csdn.net/fuwenyan/article/details/76576708</a><br>新智元·腾讯优图首度开源深度学习框架ncnn 主打手机端，同类cpu框架最快</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> 嵌入式AI </category>
          
          <category> NCNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> 嵌入式AI </tag>
            
            <tag> NCNN </tag>
            
            <tag> 入门 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CMake 入门</title>
      <link href="/2018/05/02/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/03-CMake%E5%85%A5%E9%97%A8/"/>
      <url>/2018/05/02/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/03-CMake%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<h2 id="CMake-入门实践"><a href="#CMake-入门实践" class="headerlink" title="CMake 入门实践"></a>CMake 入门实践</h2><h3 id="什么是-CMake"><a href="#什么是-CMake" class="headerlink" title="什么是 CMake"></a>什么是 CMake</h3><p>你或许听过好几种 <code>Make</code> 工具，例如 <code>GNU Make</code> ，QT 的 <code>qmake</code> ，微软的 MS <code>nmake</code>，BSD Make（<code>pmake</code>），<code>Makepp</code>，等等。这些 <code>Make</code> 工具遵循着不同的规范和标准，所执行的 <code>Makefile</code> 格式也千差万别。这样就带来了一个严峻的问题：如果软件想跨平台，必须要保证能够在不同平台编译。而如果使用上面的 <code>Make</code> 工具，就得为每一种标准写一次 <code>Makefile</code> ，这将是一件让人抓狂的工作。<br><code>CMake</code>就是针对上面问题所设计的工具：它首先允许开发者编写一种平台无关的 <code>CMakeList.txt</code> 文件来定制整个编译流程，然后再根据目标用户的平台进一步生成所需的本地化 <code>Makefile</code> 和工程文件，如 Unix 的 Makefile 或 Windows 的 Visual Studio 工程。从而做到“<strong>Write once, run everywhere</strong>”。显然，<code>CMake</code> 是一个比上述几种 <code>make</code> 更高级的编译配置工具。一些使用 <code>CMake</code> 作为项目架构系统的知名开源项目有 VTK、ITK、KDE、OpenCV、OSG 等。<br>在 linux 平台下使用 CMake 生成 Makefile 并编译的流程如下：</p><ul><li>编写 <code>CMake</code> 配置文件 <code>CMakeLists.txt</code> 。</li><li>执行命令 <code>cmake PATH</code> 或者 <code>ccmake PATH</code> 生成 <code>Makefile</code>。其中， <code>PATH</code> 是 <code>CMakeLists.txt</code> 所在的目录。</li><li>使用 make 命令进行编译。</li></ul><a id="more"></a><h3 id="入门案例：单个源文件"><a href="#入门案例：单个源文件" class="headerlink" title="入门案例：单个源文件"></a>入门案例：单个源文件</h3><p>对于简单的项目，只需要写几行代码就可以了。例如，假设现在我们的项目中只有一个源文件 <code>main.cc</code> ，该程序的用途是计算一个数的指数幂。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">power</span><span class="params">(<span class="keyword">double</span> base, <span class="keyword">int</span> exponent)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> result = base;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (exponent == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; exponent; ++i)&#123;</span><br><span class="line">        result = result * base;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Usage: %s base exponent \n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> base = atof(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">int</span> exponent = atoi(argv[<span class="number">2</span>]);</span><br><span class="line">    <span class="keyword">double</span> result = power(base, exponent);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%g ^ %d is %g\n"</span>, base, exponent, result);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="编写-CMakeLists-txt"><a href="#编写-CMakeLists-txt" class="headerlink" title="编写 CMakeLists.txt"></a>编写 CMakeLists.txt</h4><p>首先编写 <code>CMakeLists.txt</code> 文件，并保存在与 <code>main.cc</code> 源文件同个目录下：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="comment"># 项目信息</span></span><br><span class="line">project (Demo1)</span><br><span class="line"><span class="comment"># 指定生成目标</span></span><br><span class="line">add_executable(Demo main.cc)</span><br></pre></td></tr></table></figure></p><p><code>CMakeLists.txt</code> 的语法比较简单，由命令、注释和空格组成，其中<strong>命令是不区分大小写的</strong>。符号 # 后面的内容被认为是注释。命令由命令名称、小括号和参数组成，参数之间使用空格进行间隔。<br>对于上面的 <code>CMakeLists.txt</code> 文件，依次出现了几个命令：</p><ul><li><code>cmake_minimum_required</code>：指定运行此配置文件所需的 <code>CMake</code> 的最低版本；</li><li><code>project</code>：参数值是 <code>Demo1</code>，该命令表示项目的名称是 <code>Demo1</code> 。</li><li><code>add_executable</code>： 将名为 <code>main.cc</code> 的源文件编译成一个名称为 <code>Demo</code> 的可执行文件。</li></ul><h4 id="编译项目"><a href="#编译项目" class="headerlink" title="编译项目"></a>编译项目</h4><p>之后，在当前目录执行 <code>cmake .</code> ，得到 <code>Makefile</code> 后再使用 <code>make</code> 命令编译得到 <code>Demo1</code> 可执行文件。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo1$ cmake .</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo1</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo1$ ls</span><br><span class="line">CMakeCache.txt  cmake_install.cmake  main.cc</span><br><span class="line">CMakeFiles      CMakeLists.txt       Makefile</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo1$ make</span><br><span class="line">Scanning dependencies of target Demo</span><br><span class="line">[ 50%] Building CXX object CMakeFiles/Demo.dir/main.cc.o</span><br><span class="line">[100%] Linking CXX executable Demo</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo1$ ls</span><br><span class="line">CMakeCache.txt  cmake_install.cmake  Demo     Makefile</span><br><span class="line">CMakeFiles      CMakeLists.txt       main.cc</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo1$ ./Demo 3 2</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure></p><h3 id="多个源文件"><a href="#多个源文件" class="headerlink" title="多个源文件"></a>多个源文件</h3><h4 id="同一目录，多个源文件"><a href="#同一目录，多个源文件" class="headerlink" title="同一目录，多个源文件"></a>同一目录，多个源文件</h4><p>上面的例子只有单个源文件。现在假如把 <code>power</code> 函数单独写进一个名为 <code>MathFunctions.c</code> 的源文件里，使得这个工程变成如下的形式：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./Demo2</span><br><span class="line">    |</span><br><span class="line">    +--- main.cc</span><br><span class="line">    |</span><br><span class="line">    +--- MathFunctions.cc</span><br><span class="line">    |</span><br><span class="line">    +--- MathFunctions.h</span><br></pre></td></tr></table></figure></p><p>这个时候，CMakeLists.txt 可以改成如下的形式：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="comment"># 项目信息</span></span><br><span class="line">project (Demo2)</span><br><span class="line"><span class="comment"># 指定生成目标</span></span><br><span class="line">add_executable(Demo main.cc MathFunctions.cc)</span><br></pre></td></tr></table></figure></p><p>唯一的改动只是在 <code>add_executable</code> 命令中增加了一个 <code>MathFunctions.cc</code> 源文件。这样写当然没什么问题，但是如果源文件很多，把所有源文件的名字都加进去将是一件烦人的工作。更省事的方法是使用 <code>aux_source_directory</code> 命令，该命令会查找指定目录下的所有源文件，然后将结果存进指定变量名。其语法如下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aux_source_directory(&lt;dir&gt; &lt;variable&gt;)</span><br></pre></td></tr></table></figure></p><p>因此，可以修改 CMakeLists.txt 如下：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="comment"># 项目信息</span></span><br><span class="line">project (Demo2)</span><br><span class="line"><span class="comment"># 查找当前目录下的所有源文件</span></span><br><span class="line"><span class="comment"># 并将名称保存到 DIR_SRCS 变量</span></span><br><span class="line">aux_source_directory(. DIR_SRCS)</span><br><span class="line"><span class="comment"># 指定生成目标</span></span><br><span class="line">add_executable(Demo $&#123;DIR_SRCS&#125;)</span><br></pre></td></tr></table></figure></p><p>这样，CMake 会将当前目录所有源文件的文件名赋值给变量 <code>DIR_SRCS</code> ，再指示变量 <code>DIR_SRCS</code> 中的源文件需要编译成一个名称为 <code>Demo</code> 的可执行文件。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo2$ ls</span><br><span class="line">CMakeLists.txt  main.cc  MathFunctions.cc  MathFunctions.h</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo2$ cmake .</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo2</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo2$ ls</span><br><span class="line">CMakeCache.txt  cmake_install.cmake  main.cc   MathFunctions.cc</span><br><span class="line">CMakeFiles      CMakeLists.txt       Makefile  MathFunctions.h</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo2$ make</span><br><span class="line">Scanning dependencies of target Demo</span><br><span class="line">[ 33%] Building CXX object CMakeFiles/Demo.dir/main.cc.o</span><br><span class="line">[ 66%] Building CXX object CMakeFiles/Demo.dir/MathFunctions.cc.o</span><br><span class="line">[100%] Linking CXX executable Demo</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo2$ ./Demo 3 2</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure></p><h4 id="多个目录，多个源文件"><a href="#多个目录，多个源文件" class="headerlink" title="多个目录，多个源文件"></a>多个目录，多个源文件</h4><p>现在进一步将 <code>MathFunctions.h</code> 和 <code>MathFunctions.cc</code> 文件移动到 <code>math</code> 目录下。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./Demo3</span><br><span class="line">    |</span><br><span class="line">    +--- main.cc</span><br><span class="line">    |</span><br><span class="line">    +--- math/</span><br><span class="line">          |</span><br><span class="line">          +--- MathFunctions.cc</span><br><span class="line">          |</span><br><span class="line">          +--- MathFunctions.h</span><br></pre></td></tr></table></figure></p><p>对于这种情况，需要分别在项目根目录 <code>Demo3</code> 和 <code>math</code> 目录里各编写一个 <code>CMakeLists.txt</code> 文件。为了方便，我们可以先将 <code>math</code> 目录里的文件编译成静态库再由 <code>main</code> 函数调用。</p><h5 id="根目录中的-CMakeLists-txt-："><a href="#根目录中的-CMakeLists-txt-：" class="headerlink" title="根目录中的 CMakeLists.txt ："></a>根目录中的 CMakeLists.txt ：</h5><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="comment"># 项目信息</span></span><br><span class="line">project (Demo3)</span><br><span class="line"><span class="comment"># 查找当前目录下的所有源文件</span></span><br><span class="line"><span class="comment"># 并将名称保存到 DIR_SRCS 变量</span></span><br><span class="line">aux_source_directory(. DIR_SRCS)</span><br><span class="line"><span class="comment"># 添加 math 子目录</span></span><br><span class="line">add_subdirectory(math)</span><br><span class="line"><span class="comment"># 指定生成目标</span></span><br><span class="line">add_executable(Demo main.cc)</span><br><span class="line"><span class="comment"># 添加链接库</span></span><br><span class="line">target_link_libraries(Demo MathFunctions)</span><br></pre></td></tr></table></figure><p>该文件添加了下面的内容: 使用命令 <code>add_subdirectory</code> 指明本项目包含一个子目录 <code>math</code>，这样 <code>math</code> 目录下的 <code>CMakeLists.txt</code> 文件和源代码也会被处理 。使用命令 <code>target_link_libraries</code> 指明可执行文件 <code>main</code> 需要连接一个名为 <code>MathFunctions</code> 的链接库 。</p><h5 id="子目录中的-CMakeLists-txt："><a href="#子目录中的-CMakeLists-txt：" class="headerlink" title="子目录中的 CMakeLists.txt："></a>子目录中的 CMakeLists.txt：</h5><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找当前目录下的所有源文件</span></span><br><span class="line"><span class="comment"># 并将名称保存到 DIR_LIB_SRCS 变量</span></span><br><span class="line">aux_source_directory(. DIR_LIB_SRCS)</span><br><span class="line"><span class="comment"># 生成链接库</span></span><br><span class="line">add_library (MathFunctions $&#123;DIR_LIB_SRCS&#125;)</span><br></pre></td></tr></table></figure><p>在该文件中使用命令 <code>add_library</code> 将 <code>math</code> 目录中的源文件编译为静态链接库。</p><h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ ls</span><br><span class="line">CMakeLists.txt  main.cc  math</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ cmake .</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo3</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ ls</span><br><span class="line">CMakeCache.txt  cmake_install.cmake  main.cc   math</span><br><span class="line">CMakeFiles      CMakeLists.txt       Makefile</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ <span class="built_in">cd</span> math/</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3/math$ ls</span><br><span class="line">CMakeFiles           CMakeLists.txt  MathFunctions.cc</span><br><span class="line">cmake_install.cmake  Makefile        MathFunctions.h</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3/math$ <span class="built_in">cd</span> ..</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ ls</span><br><span class="line">CMakeCache.txt  cmake_install.cmake  main.cc   math</span><br><span class="line">CMakeFiles      CMakeLists.txt       Makefile</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ make</span><br><span class="line">Scanning dependencies of target MathFunctions</span><br><span class="line">[ 25%] Building CXX object math/CMakeFiles/MathFunctions.dir/MathFunctions.cc.o</span><br><span class="line">[ 50%] Linking CXX static library libMathFunctions.a</span><br><span class="line">[ 50%] Built target MathFunctions</span><br><span class="line">Scanning dependencies of target Demo</span><br><span class="line">[ 75%] Building CXX object CMakeFiles/Demo.dir/main.cc.o</span><br><span class="line">[100%] Linking CXX executable Demo</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ ./Demo 3 2</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure><h3 id="自定义编译选项"><a href="#自定义编译选项" class="headerlink" title="自定义编译选项"></a>自定义编译选项</h3><p>CMake 允许为项目增加编译选项，从而可以根据用户的环境和需求选择最合适的编译方案。</p><p>例如，可以将 <code>MathFunctions</code> 库设为一个可选的库，如果该选项为 <code>ON</code> ，就使用该库定义的数学函数来进行运算。否则就调用标准库中的数学函数库。</p><h4 id="修改-CMakeLists-文件"><a href="#修改-CMakeLists-文件" class="headerlink" title="修改 CMakeLists 文件"></a>修改 CMakeLists 文件</h4><p>我们要做的第一步是在根目录的 <code>CMakeLists.txt</code> 文件中添加该选项：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="comment"># 项目信息</span></span><br><span class="line">project (Demo4)</span><br><span class="line"><span class="comment"># 是否使用自己的 MathFunctions 库</span></span><br><span class="line">option (USE_MYMATH</span><br><span class="line">       <span class="string">"Use provided math implementation"</span> ON)</span><br><span class="line"><span class="comment"># 加入一个配置头文件，用于处理 CMake 对源码的设置</span></span><br><span class="line">configure_file (</span><br><span class="line">  <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/config.h.in"</span></span><br><span class="line">  <span class="string">"$&#123;PROJECT_BINARY_DIR&#125;/config.h"</span></span><br><span class="line">  )</span><br><span class="line"><span class="comment"># 是否加入 MathFunctions 库</span></span><br><span class="line">if (USE_MYMATH)</span><br><span class="line">  include_directories (<span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/math"</span>)</span><br><span class="line">  add_subdirectory (math)</span><br><span class="line">  set (EXTRA_LIBS $&#123;EXTRA_LIBS&#125; MathFunctions)</span><br><span class="line"><span class="keyword">endif</span> (USE_MYMATH)</span><br><span class="line"><span class="comment"># 查找当前目录下的所有源文件</span></span><br><span class="line"><span class="comment"># 并将名称保存到 DIR_SRCS 变量</span></span><br><span class="line">aux_source_directory(. DIR_SRCS)</span><br><span class="line"><span class="comment"># 指定生成目标</span></span><br><span class="line">add_executable(Demo $&#123;DIR_SRCS&#125;)</span><br><span class="line">target_link_libraries (Demo  $&#123;EXTRA_LIBS&#125;)</span><br></pre></td></tr></table></figure></p><p>其中：</p><ul><li>第6行的 <code>option</code> 命令添加了一个 <code>USE_MYMATH</code> 选项，并且默认值为 <code>ON</code> 。</li><li>第9行的 <code>configure_file</code> 命令用于加入一个配置头文件 <code>config.h</code> ，这个文件由 <code>CMake</code> 从 <code>config.h.in</code> 生成，通过这样的机制，将可以通过预定义一些参数和变量来控制代码的生成。</li><li>第14行根据 <code>USE_MYMATH</code> 变量的值来决定是否使用我们自己编写的 <code>MathFunctions</code> 库。</li></ul><h4 id="修改-main-cc-文件"><a href="#修改-main-cc-文件" class="headerlink" title="修改 main.cc 文件"></a>修改 main.cc 文件</h4><p>之后修改 main.cc 文件，让其根据 <code>USE_MYMATH</code> 的预定义值来决定是否调用标准库还是 <code>MathFunctions</code> 库：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;config.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MYMATH</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;MathFunctions.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Usage: %s base exponent \n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> base = atof(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">int</span> exponent = atoi(argv[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MYMATH</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use our own Math library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = power(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use the standard library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = <span class="built_in">pow</span>(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%g ^ %d is %g\n"</span>, base, exponent, result);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="编写-config-h-in-文件"><a href="#编写-config-h-in-文件" class="headerlink" title="编写 config.h.in 文件"></a>编写 config.h.in 文件</h4><p>上面的程序值得注意的是第2行，这里引用了一个 <code>config.h</code> 文件，这个文件预定义了 <code>USE_MYMATH</code> 的值。但我们并不直接编写这个文件，为了方便从 <code>CMakeLists.txt</code> 中导入配置，我们编写一个 <code>config.h.in</code> 文件，内容如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#cmakedefine USE_MYMATH</span></span><br></pre></td></tr></table></figure></p><p>这样 CMake 会自动根据 CMakeLists 配置文件中的设置自动生成 config.h 文件。</p><h4 id="编译项目-1"><a href="#编译项目-1" class="headerlink" title="编译项目"></a>编译项目</h4><p>现在编译一下这个项目，为了便于交互式的选择该变量的值，可以使用 <code>ccmake .</code> 命令<br><img src="http://xukeqiniu.xukeai.cn/3e5f44d3eaa3c1480e7937f8e2bef4ba.png" alt=""><br>从中可以找到刚刚定义的 <code>USE_MYMATH</code> 选项，按键盘的方向键可以在不同的选项窗口间跳转，按下 <code>enter</code> 键可以修改该选项。修改完成后可以按下 <code>c</code> 选项完成配置，之后再按 <code>g</code> 键确认生成 <code>Makefile</code> 。<code>ccmake</code> 的其他操作可以参考窗口下方给出的指令提示。<br>我们可以试试分别将 USE_MYMATH 设为 ON 和 OFF 得到的结果：</p><h5 id="USE-MYMATH-为-ON"><a href="#USE-MYMATH-为-ON" class="headerlink" title="USE_MYMATH 为 ON"></a>USE_MYMATH 为 ON</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ cmake .</span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo4</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ make</span><br><span class="line">[ 50%] Built target MathFunctions</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ ./Demo 3 2</span><br><span class="line">Now we use our own Math library.</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure><h5 id="USE-MYMATH-为-OFF"><a href="#USE-MYMATH-为-OFF" class="headerlink" title="USE_MYMATH 为 OFF"></a>USE_MYMATH 为 OFF</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ cmake .</span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo4</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ make</span><br><span class="line">Scanning dependencies of target Demo</span><br><span class="line">[ 50%] Building CXX object CMakeFiles/Demo.dir/main.cc.o</span><br><span class="line">[100%] Linking CXX executable Demo</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ ./Demo 3 2</span><br><span class="line">Now we use the standard library.</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure><h3 id="安装和测试"><a href="#安装和测试" class="headerlink" title="安装和测试"></a>安装和测试</h3><p><code>CMake</code> 也可以指定安装规则，以及添加测试。这两个功能分别可以通过在产生 <code>Makefile</code> 后使用 <code>make install</code> 和 <code>make test</code> 来执行。在以前的 <code>GNU Makefile</code> 里，你可能需要为此编写 <code>install</code> 和 <code>test</code> 两个伪目标和相应的规则，但在 <code>CMake</code> 里，这样的工作同样只需要简单的调用几条命令。</p><h4 id="定制安装规则"><a href="#定制安装规则" class="headerlink" title="定制安装规则"></a>定制安装规则</h4><p>首先先在 math/CMakeLists.txt 文件里添加下面两行：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定 MathFunctions 库的安装路径</span></span><br><span class="line">install (TARGETS MathFunctions DESTINATION bin)</span><br><span class="line">install (FILES MathFunctions.h DESTINATION <span class="keyword">include</span>)</span><br></pre></td></tr></table></figure></p><p>指明 MathFunctions 库的安装路径。<br>之后同样修改根目录的 CMakeLists 文件，在末尾添加下面几行：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定安装路径</span></span><br><span class="line">install (TARGETS Demo DESTINATION bin)</span><br><span class="line">install (FILES <span class="string">"$&#123;PROJECT_BINARY_DIR&#125;/config.h"</span></span><br><span class="line">         DESTINATION <span class="keyword">include</span>)</span><br></pre></td></tr></table></figure></p><p>通过上面的定制，生成的 Demo 文件和 MathFunctions 函数库 libMathFunctions.o 文件将会被复制到 /usr/local/bin 中，而 MathFunctions.h 和生成的 config.h 文件则会被复制到 /usr/local/include 中。</p><blockquote><p>顺带一提的是，这里的 /usr/local/ 是默认安装到的根目录，可以通过修改 CMAKE_INSTALL_PREFIX 变量的值来指定这些文件应该拷贝到哪个根目录。</p></blockquote><p>我们可以验证一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu: sudo make install</span><br><span class="line">[ 50%] Built target MathFunctions</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">Install the project...</span><br><span class="line">-- Install configuration: <span class="string">""</span></span><br><span class="line">-- Installing: /usr/<span class="built_in">local</span>/bin/Demo</span><br><span class="line">-- Installing: /usr/<span class="built_in">local</span>/include/config.h</span><br><span class="line">-- Installing: /usr/<span class="built_in">local</span>/bin/libMathFunctions.a</span><br><span class="line">-- Up-to-date: /usr/<span class="built_in">local</span>/include/MathFunctions.h</span><br><span class="line">xuke@ubuntu: ls /usr/<span class="built_in">local</span>/bin</span><br><span class="line">Demo  libMathFunctions.a</span><br><span class="line">xuke@ubuntu: ls /usr/<span class="built_in">local</span>/include</span><br><span class="line">config.h  MathFunctions.h</span><br></pre></td></tr></table></figure></p><h4 id="为工程添加测试"><a href="#为工程添加测试" class="headerlink" title="为工程添加测试"></a>为工程添加测试</h4><p>添加测试同样很简单。<code>CMake</code> 提供了一个称为 <code>CTest</code> 的测试工具。我们要做的只是在项目根目录的 <code>CMakeLists</code> 文件中调用一系列的 <code>add_test</code> 命令。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启用测试</span></span><br><span class="line">enable_testing()</span><br><span class="line"><span class="comment"># 测试程序是否成功运行</span></span><br><span class="line">add_test (test_run Demo 5 2)</span><br><span class="line"><span class="comment"># 测试帮助信息是否可以正常提示</span></span><br><span class="line">add_test (test_usage Demo)</span><br><span class="line">set_tests_properties (test_usage</span><br><span class="line">  PROPERTIES PASS_REGULAR_EXPRESSION <span class="string">"Usage: .* base exponent"</span>)</span><br><span class="line"><span class="comment"># 测试 5 的平方</span></span><br><span class="line">add_test (test_5_2 Demo 5 2)</span><br><span class="line">set_tests_properties (test_5_2</span><br><span class="line"> PROPERTIES PASS_REGULAR_EXPRESSION <span class="string">"is 25"</span>)</span><br><span class="line"><span class="comment"># 测试 10 的 5 次方</span></span><br><span class="line">add_test (test_10_5 Demo 10 5)</span><br><span class="line">set_tests_properties (test_10_5</span><br><span class="line"> PROPERTIES PASS_REGULAR_EXPRESSION <span class="string">"is 100000"</span>)</span><br><span class="line"><span class="comment"># 测试 2 的 10 次方</span></span><br><span class="line">add_test (test_2_10 Demo 2 10)</span><br><span class="line">set_tests_properties (test_2_10</span><br><span class="line"> PROPERTIES PASS_REGULAR_EXPRESSION <span class="string">"is 1024"</span>)</span><br></pre></td></tr></table></figure></p><p>上面的代码包含了四个测试。第一个测试 <code>test_run</code> 用来测试程序是否成功运行并返回 0 值。剩下的三个测试分别用来测试 5 的 平方、10 的 5 次方、2 的 10 次方是否都能得到正确的结果。其中 <code>PASS_REGULAR_EXPRESSION</code> 用来测试输出是否包含后面跟着的字符串。</p><p>让我们看看测试的结果：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo5$ make <span class="built_in">test</span></span><br><span class="line">Running tests...</span><br><span class="line">Test project /home/xuke/work/cmake-demo/Demo5</span><br><span class="line">    Start 1: test_run</span><br><span class="line">1/5 Test <span class="comment">#1: test_run .........................   Passed    0.00 sec</span></span><br><span class="line">    Start 2: test_usage</span><br><span class="line">2/5 Test <span class="comment">#2: test_usage .......................   Passed    0.00 sec</span></span><br><span class="line">    Start 3: test_5_2</span><br><span class="line">3/5 Test <span class="comment">#3: test_5_2 .........................   Passed    0.00 sec</span></span><br><span class="line">    Start 4: test_10_5</span><br><span class="line">4/5 Test <span class="comment">#4: test_10_5 ........................   Passed    0.00 sec</span></span><br><span class="line">    Start 5: test_2_10</span><br><span class="line">5/5 Test <span class="comment">#5: test_2_10 ........................   Passed    0.00 sec</span></span><br><span class="line"></span><br><span class="line">100% tests passed, 0 tests failed out of 5</span><br><span class="line"></span><br><span class="line">Total Test time (real) =   0.01 sec</span><br></pre></td></tr></table></figure></p><p>如果要测试更多的输入数据，像上面那样一个个写测试用例未免太繁琐。这时可以通过编写<strong>宏</strong>来实现：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个宏，用来简化测试工作</span></span><br><span class="line">macro (do_test arg1 arg2 result)</span><br><span class="line">  add_test (test_$&#123;arg1&#125;_$&#123;arg2&#125; Demo $&#123;arg1&#125; $&#123;arg2&#125;)</span><br><span class="line">  set_tests_properties (test_$&#123;arg1&#125;_$&#123;arg2&#125;</span><br><span class="line">    PROPERTIES PASS_REGULAR_EXPRESSION $&#123;result&#125;)</span><br><span class="line">endmacro (do_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用该宏进行一系列的数据测试</span></span><br><span class="line">do_test (5 2 <span class="string">"is 25"</span>)</span><br><span class="line">do_test (10 5 <span class="string">"is 100000"</span>)</span><br><span class="line">do_test (2 10 <span class="string">"is 1024"</span>)</span><br></pre></td></tr></table></figure></p><h3 id="支持-gdb"><a href="#支持-gdb" class="headerlink" title="支持 gdb"></a>支持 gdb</h3><p>让 CMake 支持 <code>gdb</code> 的设置也很容易，只需要指定 <code>Debug</code> 模式下开启 <code>-g</code> 选项：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set(CMAKE_BUILD_TYPE <span class="string">"Debug"</span>)</span><br><span class="line">set(CMAKE_CXX_FLAGS_DEBUG <span class="string">"$ENV&#123;CXXFLAGS&#125; -O0 -Wall -g -ggdb"</span>)</span><br><span class="line">set(CMAKE_CXX_FLAGS_RELEASE <span class="string">"$ENV&#123;CXXFLAGS&#125; -O3 -Wall"</span>)</span><br></pre></td></tr></table></figure></p><p>之后可以直接对生成的程序使用 <code>gdb</code> 来调试。</p><h3 id="添加环境检查"><a href="#添加环境检查" class="headerlink" title="添加环境检查"></a>添加环境检查</h3><p>有时候可能要对系统环境做点检查，例如要使用一个平台相关的特性的时候。在这个例子中，我们检查系统是否自带 <code>pow</code> 函数。如果带有 <code>pow</code> 函数，就使用它；否则使用我们定义的 <code>power</code> 函数。</p><h4 id="添加-CheckFunctionExists-宏"><a href="#添加-CheckFunctionExists-宏" class="headerlink" title="添加 CheckFunctionExists 宏"></a>添加 CheckFunctionExists 宏</h4><p>首先在顶层 <code>CMakeLists</code> 文件中添加 <code>CheckFunctionExists.cmake</code> 宏，并调用 <code>check_function_exists</code> 命令测试链接器是否能够在链接阶段找到 <code>pow</code> 函数。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查系统是否支持 pow 函数</span></span><br><span class="line"><span class="keyword">include</span> ($&#123;CMAKE_ROOT&#125;/Modules/CheckFunctionExists.cmake)</span><br><span class="line">check_function_exists (pow HAVE_POW)</span><br></pre></td></tr></table></figure></p><p>将上面这段代码放在 <code>configure_file</code> 命令前。</p><h4 id="预定义相关宏变量"><a href="#预定义相关宏变量" class="headerlink" title="预定义相关宏变量"></a>预定义相关宏变量</h4><p>接下来修改 <code>config.h.in</code> 文件，预定义相关的宏变量。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// does the platform provide pow function?</span><br><span class="line"><span class="comment">#cmakedefine HAVE_POW</span></span><br></pre></td></tr></table></figure></p><h4 id="在代码中使用宏和函数"><a href="#在代码中使用宏和函数" class="headerlink" title="在代码中使用宏和函数"></a>在代码中使用宏和函数</h4><p>最后一步是修改 main.cc ，在代码中使用宏和函数：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> HAVE_POW</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use the standard library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = <span class="built_in">pow</span>(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use our own Math library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = power(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></p><h3 id="添加版本号"><a href="#添加版本号" class="headerlink" title="添加版本号"></a>添加版本号</h3><p>给项目添加和维护版本号是一个好习惯，这样有利于用户了解每个版本的维护情况，并及时了解当前所用的版本是否过时，或是否可能出现不兼容的情况。</p><p>首先修改顶层 <code>CMakeLists</code> 文件，在 <code>project</code> 命令之后加入如下两行：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set (Demo_VERSION_MAJOR 1)</span><br><span class="line">set (Demo_VERSION_MINOR 0)</span><br></pre></td></tr></table></figure></p><p>分别指定当前的项目的主版本号和副版本号。<br>之后，为了在代码中获取版本信息，我们可以修改 config.h.in 文件，添加两个预定义变量：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// the configured options and settings for Tutorial</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Demo_VERSION_MAJOR @Demo_VERSION_MAJOR@</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Demo_VERSION_MINOR @Demo_VERSION_MINOR@</span></span><br></pre></td></tr></table></figure></p><p>这样就可以直接在代码中打印版本信息了：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"config.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"math/MathFunctions.h"</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>)&#123;</span><br><span class="line">        <span class="comment">// print version info</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%s Version %d.%d\n"</span>,</span><br><span class="line">            argv[<span class="number">0</span>],</span><br><span class="line">            Demo_VERSION_MAJOR,</span><br><span class="line">            Demo_VERSION_MINOR);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Usage: %s base exponent \n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> base = atof(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">int</span> exponent = atoi(argv[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> defined (HAVE_POW)</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use the standard library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = <span class="built_in">pow</span>(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use our own Math library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = power(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%g ^ %d is %g\n"</span>, base, exponent, result);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>测试的结果：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo7$ ls</span><br><span class="line">CMakeLists.txt  config.h.in  main.cc  math</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo7$ cmake .</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Looking <span class="keyword">for</span> pow</span><br><span class="line">-- Looking <span class="keyword">for</span> pow - not found</span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo7</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo7$ make</span><br><span class="line">Scanning dependencies of target MathFunctions</span><br><span class="line">[ 25%] Building CXX object math/CMakeFiles/MathFunctions.dir/MathFunctions.cc.o</span><br><span class="line">[ 50%] Linking CXX static library libMathFunctions.a</span><br><span class="line">[ 50%] Built target MathFunctions</span><br><span class="line">Scanning dependencies of target Demo</span><br><span class="line">[ 75%] Building CXX object CMakeFiles/Demo.dir/main.cc.o</span><br><span class="line">[100%] Linking CXX executable Demo</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo7$ ./Demo 3 2</span><br><span class="line">Now we use our own Math library.</span><br><span class="line">3 ^ 2 is 9</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo7$ ./Demo 3</span><br><span class="line">./Demo Version 1.0</span><br><span class="line">Usage: ./Demo base exponent</span><br></pre></td></tr></table></figure></p><h3 id="生成安装包"><a href="#生成安装包" class="headerlink" title="生成安装包"></a>生成安装包</h3><p>本节将学习如何配置生成各种平台上的安装包，包括二进制安装包和源码安装包。为了完成这个任务，我们需要用到 <code>CPack</code> ，它同样也是由 <code>CMake</code> 提供的一个工具，专门用于打包。<br>首先在顶层的 <code>CMakeLists.txt</code> 文件尾部添加下面几行：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个 CPack 安装包</span></span><br><span class="line"><span class="keyword">include</span> (InstallRequiredSystemLibraries)</span><br><span class="line">set (CPACK_RESOURCE_FILE_LICENSE</span><br><span class="line">  <span class="string">"$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/License.txt"</span>)</span><br><span class="line">set (CPACK_PACKAGE_VERSION_MAJOR <span class="string">"$&#123;Demo_VERSION_MAJOR&#125;"</span>)</span><br><span class="line">set (CPACK_PACKAGE_VERSION_MINOR <span class="string">"$&#123;Demo_VERSION_MINOR&#125;"</span>)</span><br><span class="line"><span class="keyword">include</span> (CPack)</span><br></pre></td></tr></table></figure></p><p>上面的代码做了以下几个工作：</p><ul><li>导入 <code>InstallRequiredSystemLibraries</code> 模块，以便之后导入 <code>CPack</code> 模块；</li><li>设置一些 <code>CPack</code> 相关变量，包括版权信息和版本信息，其中版本信息用了上一节定义的版本号；</li><li>导入 <code>CPack</code> 模块。</li></ul><p>接下来的工作是像往常一样构建工程，并执行 <code>cpack</code> 命令。</p><ul><li><p>生成二进制安装包：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpack -C CPackConfig.cmake</span><br></pre></td></tr></table></figure></li><li><p>生成源码安装包</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpack -C CPackSourceConfig.cmake</span><br></pre></td></tr></table></figure></li></ul><p>我们可以试一下。在生成项目后，执行 cpack -C CPackConfig.cmake 命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ ls</span><br><span class="line">CMakeLists.txt  config.h.in  License.txt  main.cc  math</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ cmake .</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Looking <span class="keyword">for</span> pow</span><br><span class="line">-- Looking <span class="keyword">for</span> pow - not found</span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo8</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ cpack -C CPackSourceConfig.cmake</span><br><span class="line">CPack: Create package using STGZ</span><br><span class="line">CPack: Install projects</span><br><span class="line">CPack: - Run preinstall target <span class="keyword">for</span>: Demo8</span><br><span class="line">CPack: - Install project: Demo8</span><br><span class="line">CPack: Create package</span><br><span class="line">CPack: - package: /home/xuke/work/cmake-demo/Demo8/Demo8-1.0.1-Linux.sh generated.</span><br><span class="line">CPack: Create package using TGZ</span><br><span class="line">CPack: Install projects</span><br><span class="line">CPack: - Run preinstall target <span class="keyword">for</span>: Demo8</span><br><span class="line">CPack: - Install project: Demo8</span><br><span class="line">CPack: Create package</span><br><span class="line">CPack: - package: /home/xuke/work/cmake-demo/Demo8/Demo8-1.0.1-Linux.tar.gz generated.</span><br><span class="line">CPack: Create package using TZ</span><br><span class="line">CPack: Install projects</span><br><span class="line">CPack: - Run preinstall target <span class="keyword">for</span>: Demo8</span><br><span class="line">CPack: - Install project: Demo8</span><br><span class="line">CPack: Create package</span><br><span class="line">CPack: - package: /home/xuke/work/cmake-demo/Demo8/Demo8-1.0.1-Linux.tar.Z generated.</span><br></pre></td></tr></table></figure></p><p>此时会在该目录下创建 3 个不同格式的二进制包文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ ls</span><br><span class="line">CMakeCache.txt       _CPack_Packages           install_manifest.txt</span><br><span class="line">CMakeFiles           CPackSourceConfig.cmake   License.txt</span><br><span class="line">cmake_install.cmake  CTestTestfile.cmake       main.cc</span><br><span class="line">CMakeLists.txt       Demo                      Makefile</span><br><span class="line">config.h             Demo8-1.0.1-Linux.sh      math</span><br><span class="line">config.h.in          Demo8-1.0.1-Linux.tar.gz</span><br><span class="line">CPackConfig.cmake    Demo8-1.0.1-Linux.tar.Z</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ ls Demo8-*</span><br><span class="line">Demo8-1.0.1-Linux.sh  Demo8-1.0.1-Linux.tar.gz  Demo8-1.0.1-Linux.tar.Z</span><br></pre></td></tr></table></figure></p><p>这 3 个二进制包文件所包含的内容是完全相同的。我们可以执行其中一个。此时会出现一个由 CPack 自动生成的交互式安装界面：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ sh Demo8-1.0.1-Linux.sh</span><br><span class="line">Demo8 Installer Version: 1.0.1, Copyright (c) Humanity</span><br><span class="line">This is a self-extracting archive.</span><br><span class="line">The archive will be extracted to: /home/xuke/work/cmake-demo/Demo8</span><br><span class="line"></span><br><span class="line">If you want to stop extracting, please press &lt;ctrl-C&gt;.</span><br><span class="line">The MIT License (MIT)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2013 Joseph Pan(http://hahack.com)</span><br><span class="line"></span><br><span class="line">Permission is hereby granted, free of charge, to any person obtaining a copy of</span><br><span class="line">this software and associated documentation files (the <span class="string">"Software"</span>), to deal <span class="keyword">in</span></span><br><span class="line">the Software without restriction, including without limitation the rights to</span><br><span class="line">use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of</span><br><span class="line">the Software, and to permit persons to whom the Software is furnished to <span class="keyword">do</span> so,</span><br><span class="line">subject to the following conditions:</span><br><span class="line"></span><br><span class="line">The above copyright notice and this permission notice shall be included <span class="keyword">in</span> all</span><br><span class="line">copies or substantial portions of the Software.</span><br><span class="line"></span><br><span class="line">THE SOFTWARE IS PROVIDED <span class="string">"AS IS"</span>, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span><br><span class="line">IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS</span><br><span class="line">FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR</span><br><span class="line">COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER</span><br><span class="line">IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN</span><br><span class="line">CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Do you accept the license? [yN]:</span><br><span class="line">y</span><br><span class="line">By default the Demo8 will be installed <span class="keyword">in</span>:</span><br><span class="line">  <span class="string">"/home/xuke/work/cmake-demo/Demo8/Demo8-1.0.1-Linux"</span></span><br><span class="line">Do you want to include the subdirectory Demo8-1.0.1-Linux?</span><br><span class="line">Saying no will install <span class="keyword">in</span>: <span class="string">"/home/xuke/work/cmake-demo/Demo8"</span> [Yn]:</span><br><span class="line">y</span><br><span class="line"></span><br><span class="line">Using target directory: /home/xuke/work/cmake-demo/Demo8/Demo8-1.0.1-Linux</span><br><span class="line">Extracting, please <span class="built_in">wait</span>...</span><br><span class="line"></span><br><span class="line">Unpacking finished successfully</span><br></pre></td></tr></table></figure></p><p>完成后提示安装到了 Demo8-1.0.1-Linux 子目录中，我们可以进去执行该程序：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8/Demo8-1.0.1-Linux$ tree</span><br><span class="line">.</span><br><span class="line">├── bin</span><br><span class="line">│   └── Demo</span><br><span class="line">├── include</span><br><span class="line">│   ├── config.h</span><br><span class="line">│   └── MathFunctions.h</span><br><span class="line">└── lib</span><br><span class="line">    └── libMathFunctions.a</span><br><span class="line"></span><br><span class="line">3 directories, 4 files</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8/Demo8-1.0.1-Linux$ ./bin/Demo 3 2</span><br><span class="line">Now we use our own Math library.</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://www.hahack.com/codes/cmake/" target="_blank" rel="noopener">CMake 入门实战</a><br><a href="https://github.com/wzpan/cmake-demo" target="_blank" rel="noopener">代码参考</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> 嵌入式AI </category>
          
          <category> NCNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> 嵌入式AI </tag>
            
            <tag> NCNN </tag>
            
            <tag> 入门 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>PyTorch 60分钟入门</title>
      <link href="/2018/05/01/DeepLearning/PyTorch/PyTorch+60%E5%88%86%E9%92%9F%E5%85%A5%E9%97%A8/"/>
      <url>/2018/05/01/DeepLearning/PyTorch/PyTorch+60%E5%88%86%E9%92%9F%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<h2 id="PyTorch-60分钟入门"><a href="#PyTorch-60分钟入门" class="headerlink" title="PyTorch 60分钟入门"></a>PyTorch 60分钟入门</h2><h3 id="PyTorch简介"><a href="#PyTorch简介" class="headerlink" title="PyTorch简介"></a>PyTorch简介</h3><p>这是一个基于Python的科学计算包，主要针对两类人群：</p><ul><li>替代Numpy以发挥GPU的强大能力</li><li>一个提供最大灵活性和速度的深度学习研究平台</li></ul><a id="more"></a><h4 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h4><h5 id="张量（Tensors）"><a href="#张量（Tensors）" class="headerlink" title="张量（Tensors）"></a>张量（Tensors）</h5><p>Tensors类似于numpy的ndarray，但是带了一些附加的功能，例如可以使用GPU加速计算等等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><p>构建一个未初始化的5*3的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-1.0593e-05,  4.5849e-41,  3.4723e-37],        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],        [ 0.0000e+00,  0.0000e+00,  7.1941e+28],        [ 3.1036e+27,  0.0000e+00,  0.0000e+00],        [ 0.0000e+00,  0.0000e+00,  3.4568e-37]])</code></pre><p>构建一个随机初始化的5*3的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[ 0.7556,  0.7558,  0.2999],        [ 0.7304,  0.3527,  0.1911],        [ 0.8654,  0.4880,  0.1987],        [ 0.5456,  0.9359,  0.2071],        [ 0.1025,  0.5249,  0.3758]])</code></pre><p>构建一个初始化为零类型为long的5*3的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0]])</code></pre><p>从数据构造一个张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([ 5.5000,  3.0000])</code></pre><p>根据现有张量创建张量。这些方法将重用输入张量的属性，例如dtype，除非用户提供了新的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(x) <span class="comment">#打印之前的x值</span></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)      <span class="comment"># new_* 方法可以更改x的值，维度和类型</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)    <span class="comment"># 类型与值进行覆盖</span></span><br><span class="line">print(x)                                      <span class="comment"># 不改变维度</span></span><br></pre></td></tr></table></figure><hr><pre><code>tensor([ 5.5000,  3.0000])tensor([[ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.]], dtype=torch.float64)tensor([[-1.4230, -0.7907, -0.0556],        [-0.9403, -0.2937,  1.9447],        [ 0.2958,  0.9914, -0.9550],        [ 1.2439, -0.1390,  0.2889],        [-0.1790, -0.0003,  0.5241]])</code></pre><p>获取尺寸</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure><hr><pre><code>torch.Size([5, 3])</code></pre><blockquote><p>torch.Size 实际上是一个元组（tuple），因此它支持所有的元祖（tuple）的操作。</p></blockquote><h5 id="操作（Operations）"><a href="#操作（Operations）" class="headerlink" title="操作（Operations）"></a>操作（Operations）</h5><p>Pytorch具有100多种操作符（加减乘除，转置，索引，切片，等等），在这里我们以最简单的加法操作，了解Pytorch的操作方法。</p><ul><li>加法：语法1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-1.1514, -0.5880, -0.0083],        [-0.4967,  0.2964,  2.5860],        [ 0.7163,  1.0643,  0.0210],        [ 1.8021,  0.6697,  0.8263],        [ 0.3601,  0.3765,  1.3859]])</code></pre><ul><li>加法：语法2</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-1.1514, -0.5880, -0.0083],        [-0.4967,  0.2964,  2.5860],        [ 0.7163,  1.0643,  0.0210],        [ 1.8021,  0.6697,  0.8263],        [ 0.3601,  0.3765,  1.3859]])</code></pre><ul><li>加法：提供输出张量作为参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-1.1514, -0.5880, -0.0083],        [-0.4967,  0.2964,  2.5860],        [ 0.7163,  1.0643,  0.0210],        [ 1.8021,  0.6697,  0.8263],        [ 0.3601,  0.3765,  1.3859]])</code></pre><ul><li>加法：就地解决（(in-place)）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># adds x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-1.1514, -0.5880, -0.0083],        [-0.4967,  0.2964,  2.5860],        [ 0.7163,  1.0643,  0.0210],        [ 1.8021,  0.6697,  0.8263],        [ 0.3601,  0.3765,  1.3859]])</code></pre><blockquote><p>任何就地改变一个tensor的操作都以<code>_</code>为后缀。例如：<code>x.copy_(y)</code>, <code>x.t_()</code>，都会改变x。</p></blockquote><h4 id="Numpy与Torch张量的相互转换"><a href="#Numpy与Torch张量的相互转换" class="headerlink" title="Numpy与Torch张量的相互转换"></a>Numpy与Torch张量的相互转换</h4><blockquote><p>Torch的Tensor和Numpy的数组会共享它们的底层存储位置，改变其中一个，另外一个也会改变。</p></blockquote><h5 id="Torch张量转换成Numpy数组"><a href="#Torch张量转换成Numpy数组" class="headerlink" title="Torch张量转换成Numpy数组"></a>Torch张量转换成Numpy数组</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>) <span class="comment"># 创建一个torch张量</span></span><br><span class="line">print(a)</span><br><span class="line">b = a.numpy() <span class="comment"># 将torch张量转化为numpy数组</span></span><br><span class="line">print(b)</span><br><span class="line">a.add_(<span class="number">1</span>) <span class="comment"># 就地改变torch张量的值</span></span><br><span class="line">print(a) <span class="comment"># a torch张量发生改变</span></span><br><span class="line">print(b) <span class="comment"># b numpy数组因为共享底层存储所以也同时改变</span></span><br></pre></td></tr></table></figure><hr><pre><code>tensor([ 1.,  1.,  1.,  1.,  1.])[1. 1. 1. 1. 1.]tensor([ 2.,  2.,  2.,  2.,  2.])[2. 2. 2. 2. 2.]</code></pre><h5 id="Numpy数组T转换成orch张量"><a href="#Numpy数组T转换成orch张量" class="headerlink" title="Numpy数组T转换成orch张量"></a>Numpy数组T转换成orch张量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入numpy</span></span><br><span class="line">a = np.ones(<span class="number">5</span>) <span class="comment">#创建numpy数组</span></span><br><span class="line">b = torch.from_numpy(a) <span class="comment">#numpy转化成torch张量</span></span><br><span class="line">np.add(a, <span class="number">1</span>, out=a) <span class="comment">#numpy数组数据加一</span></span><br><span class="line">print(a) <span class="comment"># numpy数组发生变化</span></span><br><span class="line">print(b) <span class="comment"># torch张量因为与numpy共享底层存储因此也发生变化</span></span><br></pre></td></tr></table></figure><hr><pre><code>[2. 2. 2. 2. 2.]tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)</code></pre><h4 id="CUDA张量（CUDA-Tensors）"><a href="#CUDA张量（CUDA-Tensors）" class="headerlink" title="CUDA张量（CUDA Tensors）"></a>CUDA张量（CUDA Tensors）</h4><blockquote><p>可以使用.to方法将张量移动到任何设备上。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 我们使用 ``torch.device`` 对象 将张量移入和移出GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># 一个CUDA设备对象</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># 直接在GPU上创建一个张量对象</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># 或者使用``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))       <span class="comment"># ``.to`` 将结果转回cpu存储，还可以改变数据类型</span></span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-0.4230,  0.2093,  0.9444],        [ 0.0597,  0.7063,  2.9447],        [ 1.2958,  1.9914,  0.0450],        [ 2.2439,  0.8610,  1.2889],        [ 0.8210,  0.9997,  1.5241]], device=&apos;cuda:0&apos;)tensor([[-0.4230,  0.2093,  0.9444],        [ 0.0597,  0.7063,  2.9447],        [ 1.2958,  1.9914,  0.0450],        [ 2.2439,  0.8610,  1.2889],        [ 0.8210,  0.9997,  1.5241]], dtype=torch.float64)</code></pre><h3 id="Autograd-自动求导"><a href="#Autograd-自动求导" class="headerlink" title="Autograd:自动求导"></a>Autograd:自动求导</h3><p>在PyTorch中所有神经网络的核心是<code>autograd</code>软件包。我们先来简单介绍一下这个，然后再构建第一个神经网络。<br><code>autograd</code>包为Tensors上的所有操作提供了自动求导。它是一个运行过程中定义的框架（define-by-run），这意味着反向传播是由代码的运行方式来定义的，并且每一次迭代都可能不同。</p><h4 id="张量（Tensor）-gt-0-4版本前是Variable"><a href="#张量（Tensor）-gt-0-4版本前是Variable" class="headerlink" title="张量（Tensor）-&gt;0.4版本前是Variable"></a>张量（Tensor）-&gt;0.4版本前是Variable</h4><p><code>torch.Tensor</code>是包的中心类。如果你将属性<code>.requires_grad</code>设置为<code>True</code>，它将开始追踪所有的操作。当你完成了计算过程，你可以调用<code>.backward()</code>，之后所有的梯度计算都是自动的。<code>Tensor</code>的梯度将累积到<code>.grad</code>属性中。</p><p>要停止跟踪历史记录的<code>Tensor</code>，可以调用<code>.detach()</code>将其从计算历史记录中分离出来，并防止跟踪将来的计算。</p><p>为了防止跟踪历史记录（和使用内存），你也可以用<code>torch.no_grad()</code>包装代码块。 这在评估模型时特别有用，因为该模型可能具有<code>require_grad = True</code>的可训练参数，但我们不需要梯度值。</p><p>还有一个类对于<code>autograd</code>实现非常重要：一个<code>Function</code>。</p><p><code>Tensor</code>和<code>Function</code>是相互关联的，并建立一个非循环图，它编码构建了完整的计算过程。 每个变量都有一个<code>.grad_fn</code>属性，该属性反应在已创建<code>Tensor</code>的函数上（用户创建的<code>Tensor</code>除外 - 它们的<code>grad_fn</code>为<code>None</code>）。</p><p>如果你想计算导数，可以在<code>Tensor</code>上调用<code>.backward()</code>。如果<code>Tensor</code>是个标量（一个单元素数据），那么你不用为<code>backward()</code>指定任何参数，然而如果它有多个元素，你需要指定一个<code>gradient</code>参数，它是一个匹配尺寸的<code>Tensor</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>) <span class="comment"># 创建一个张量并设置`requires_grad = True`来跟踪计算</span></span><br><span class="line">print(x)  <span class="comment"># 打印x的值</span></span><br><span class="line">y = x + <span class="number">2</span> <span class="comment"># 对x张量进行计算操作</span></span><br><span class="line">print(y)  <span class="comment"># 打印y值</span></span><br><span class="line">print(y.grad_fn)       <span class="comment"># y是一个操作的结果，所以它有一个grad_fn。</span></span><br><span class="line">print(y.requires_grad) <span class="comment"># 打印y的requires_grad标志状态</span></span><br><span class="line">z = y * y * <span class="number">3</span>  <span class="comment"># 继续实现复杂的操作</span></span><br><span class="line">out = z.mean() <span class="comment"># 输出z的均值</span></span><br><span class="line">print(z, out)  <span class="comment"># 打印计算输出结果</span></span><br><span class="line">print(z.grad_fn)<span class="comment"># y是一个操作的结果，所以它有一个grad_fn。</span></span><br><span class="line">print(z.requires_grad) <span class="comment"># 打印z的requires_grad标志状态</span></span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[ 1.,  1.],        [ 1.,  1.]])tensor([[ 3.,  3.],        [ 3.,  3.]])&lt;AddBackward0 object at 0x7f181420a978&gt;Truetensor([[ 27.,  27.],        [ 27.,  27.]]) tensor(27.)&lt;MulBackward0 object at 0x7f180409e400&gt;True</code></pre><p><code>.requires_grad_(...)</code>就地更改现有张量的<code>requires_grad</code>标志。如果没有给出，函数输入标志默认为<code>True</code>。需要注意的是：python 的默认参数，调用的时候，test( ) 与 test(True)等价跟内部flag默认值无关。从打印看，内部flag默认值是False，但是输出结果flag为<code>True</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 创建一个2*2的张量a</span></span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))<span class="comment"># 计算</span></span><br><span class="line">print(a.requires_grad) <span class="comment"># 打印a的requires_grad标志状态</span></span><br><span class="line">a.requires_grad_(<span class="keyword">True</span>) <span class="comment"># 就地设置a的requires_grad标志状态</span></span><br><span class="line">print(a.requires_grad) <span class="comment"># 再次打印a的requires_grad标志状态</span></span><br><span class="line">b = (a * a).sum()      <span class="comment"># 由a计算引入b</span></span><br><span class="line">print(b.grad_fn)       <span class="comment"># b是一个操作的结果，所以它有一个grad_fn。</span></span><br><span class="line">print(b.requires_grad) <span class="comment"># 打印a的requires_grad标志状态</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = x*<span class="number">2</span></span><br><span class="line">    print(x.requires_grad) <span class="comment"># False</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(x.requires_grad) <span class="comment"># False</span></span><br><span class="line">y = test(x) <span class="comment"># False</span></span><br><span class="line">print(y.requires_grad) <span class="comment"># True</span></span><br></pre></td></tr></table></figure><hr><pre><code>FalseTrue&lt;SumBackward0 object at 0x7f17b4c2d518&gt;TrueFalseFalseTrue</code></pre><h4 id="梯度（Gradients）"><a href="#梯度（Gradients）" class="headerlink" title="梯度（Gradients）"></a>梯度（Gradients）</h4><p>让我们使用反向传播<code>out.backward()</code>，它等同于<code>out.backward(torch.Tensor(1)</code>)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>) <span class="comment"># 创建一个张量并设置`requires_grad = True`来跟踪计算</span></span><br><span class="line">y = x + <span class="number">2</span> <span class="comment"># 对x张量进行计算操作</span></span><br><span class="line">z = y * y * <span class="number">3</span>  <span class="comment"># 继续实现复杂的操作</span></span><br><span class="line">out = z.mean() <span class="comment"># 输出z的均值</span></span><br><span class="line">out.backward() <span class="comment"># 实现反向传播</span></span><br><span class="line">print(x.grad)  <span class="comment"># 打印梯度 d(out)/dx</span></span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[ 4.5000,  4.5000],        [ 4.5000,  4.5000]])</code></pre><blockquote><p>4.5矩阵的计算过程如下所示：<br><img src="http://xukeqiniu.xukeai.cn/d760894019f896f40c8db376a9eb91a8.png" alt=""></p></blockquote><p>我们还可以使用autograd做一些疯狂的事情！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(type(y))</span><br><span class="line">print(type(y.data))</span><br><span class="line">print(y.data.norm())</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line">gradients = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.float)</span><br><span class="line">y.backward(gradients) <span class="comment"># 沿着某方向的梯度</span></span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([-0.3905,  1.3533,  1.0339])&lt;class &apos;torch.Tensor&apos;&gt;&lt;class &apos;torch.Tensor&apos;&gt;tensor(3.4944)tensor([ -399.9199,  1385.7303,  1058.7094])tensor([  102.4000,  1024.0000,     0.1024])</code></pre><p>我们还可以通过使用<code>torch.no_grad()</code>包装代码块来停止<code>autograd</code>跟踪在张量上的历史记录，其中<code>require_grad = True</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure><hr><pre><code>TrueTrueFalse</code></pre><h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>神经网络可以使用<code>torch.nn</code>包来构建。</p><p>前面的学习大致了解了<code>autograd</code>，<code>nn</code>依赖于<code>autograd</code>来定义模型并进行求导。一个<code>nn.Module</code>包含多个神经网络层，以及一个<code>forward(input)</code>方法来返回<code>output</code>。</p><p>例如，看看以下这个分类数字图像的网络：<br><img src="http://xukeqiniu.xukeai.cn/b55446e4c713cb025be7696dc6993bde.png" alt="LeNet"><br>它是一个简单的前馈网络。它将输入逐步地传递给多个层，然后给出输出。<br>一个典型的神经网络训练过程如下：</p><ul><li>定义一个拥有可学习参数（或权重）的神经网络</li><li>在输入数据集上进行迭代</li><li>在网络中处理输入数据</li><li>计算损失（输出离分类正确有多大距离）</li><li>梯度反向传播给网络的参数</li><li>更新网络的权重，通常使用一个简单的更新规则(SGD)：<code>weight = weight + learning_rate * gradient</code></li></ul><h4 id="定义网络结构"><a href="#定义网络结构" class="headerlink" title="定义网络结构"></a>定义网络结构</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># nn.Module子类的函数必须在构造函数中执行父类的构造函数</span></span><br><span class="line">        <span class="comment"># 等价于nn.Model.__init__(self)</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 一个图像输入通道（灰度图）, 6 输出通道（6张FeatureMap）, 5x5 卷积核</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 定义卷积层：输入6张特征图，输出16张特征图，卷积核5x5</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层：线性连接(y = Wx + b)，16*5*5个节点连接到120个节点上</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层：线性连接(y = Wx + b)，120个节点连接到84个节点上</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层：线性连接(y = Wx + b)，84个节点连接到10个节点上</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义向前传播函数，并自动生成向后传播函数(autograd)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 输入x-&gt;conv1-&gt;relu-&gt;2x2窗口的最大池化-&gt;更新到x</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 输入x-&gt;conv2-&gt;relu-&gt;2x2窗口的最大池化-&gt;更新到x</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># view函数将张量x变形成一维向量形式，总特征数不变，为全连接层做准备</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</span><br><span class="line">        <span class="comment"># 输入x-&gt;fc1-&gt;relu，更新到x</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        <span class="comment"># 输入x-&gt;fc2-&gt;relu，更新到x</span></span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        <span class="comment"># 输入x-&gt;fc3，更新到x</span></span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 除了批处理维度之外的所有维度。</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]</span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><hr><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><p>在实现过程中只需要定义<code>forward</code>函数，<code>backward</code>函数（用来计算梯度）是使用<code>autograd</code><strong>自动</strong>定义的。并且可以在<code>forward</code>中使用任意的<code>Tensor</code>运算操作。</p><p>模型中可学习的参数是通过<code>net.parameters()</code>返回的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[<span class="number">0</span>].size())  <span class="comment"># conv1's .weight</span></span><br></pre></td></tr></table></figure><hr><pre><code>10torch.Size([6, 1, 5, 5])</code></pre><p>让我们尝试一个随机的<code>32x32</code>输入!<br>注意：这个网络（LeNet）的预期输入大小是<code>32x32</code>。要在<code>MNIST</code>数据集上使用此网络，请将数据集中的图像调整为<code>32x32</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-0.0819,  0.1214,  0.0144, -0.0429,  0.0046,  0.0520, -0.0673,          0.0878, -0.1724, -0.1151]])</code></pre><p>将梯度缓冲区中所有的参数置0，并使用随机的梯度进行反向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><blockquote><p><code>torch.nn</code>仅支持<code>mini-batch</code>。整个的<code>torch.nn</code>包仅支持小批量的数据，而不是一个单独的样本。例如，<code>nn.Conv2d</code>应传入一个<code>4D</code>的<code>Tensor</code>，维度为（<code>nSamples X nChannels X Height X Width</code>）。如果你有一个单独的样本，使用<code>input.unsqueeze(0)</code>来添加一个伪批维度。</p></blockquote><p>回顾：</p><ul><li><code>torch.Tensor</code> 一个支持<code>autograd</code>操作（如<code>backward()</code>）的多维数组</li><li><code>nn.Module</code> 神经网络模块。封装参数的便捷方式，帮助者将它们移动到GPU，导出，加载等。</li><li><code>nn.Parameter</code> 一种<code>Tensor</code>，当给<code>Module</code>赋值时自动注册一个参数。</li><li><code>autograd.Function</code> 实现一个<code>autograd</code> 操作的 <code>forward</code> 和 <code>backward</code> 定义。每一个<code>Tensor</code>操作，创建至少一个<code>Function</code>节点，来连接那些创建<code>Tensor</code>的函数，并且记录其历史。</li></ul><p>在这里，我们涵盖了：</p><ul><li>定义神经网络</li><li>处理输入并调用<code>backward</code></li></ul><h4 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h4><p>一个损失函数以一个(<code>output</code>, <code>target</code>)对为输入，然后计算一个值用以估计输出结果离目标结果多远。<br>在nn的包里存在定义了多种损失函数。一个简单的损失函数：<code>nn.MSELoss</code> 它计算输出与目标的均方误差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.arange(<span class="number">1</span>, <span class="number">11</span>)  <span class="comment"># 一个虚拟的目标</span></span><br><span class="line">target = target.view(<span class="number">1</span>, <span class="number">-1</span>)  <span class="comment"># 使其形状与输出相同。</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure><pre><code>tensor(38.9289)</code></pre><p>现在，如果使用其<code>.grad_fn</code>属性反向追踪损失，您将看到一个如下所示的计算图：</p><blockquote><p>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d<br>      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear<br>      -&gt; MSELoss<br>      -&gt; loss</p></blockquote><p>因此，当我们调用<code>loss.backward()</code>时，损失对应的整个图都被求导，并且图中所有的<code>Tensor</code>都会带有累积了梯度的<code>.grad</code>属性<code>requres_grad=True</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure><hr><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;MseLossBackward object at 0x000002AE1A0953C8&gt;</span><br><span class="line">&lt;AddmmBackward object at 0x000002AE1A0954A8&gt;</span><br><span class="line">&lt;ExpandBackward object at 0x000002AE1A0953C8&gt;</span><br></pre></td></tr></table></figure><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>要进行反向传播，我们只需要调用<code>loss.backward()</code>。注意：<strong>需要清除现有的梯度，否则梯度将累积到现有梯度</strong>。</p><p>现在我们将调用<code>loss.backward()</code>，并看看<code>conv1</code>偏置在<code>backward</code>之前和之后的梯度变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># 清除现有的梯度</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad) <span class="comment"># 打印之前的梯度值</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad after backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad) <span class="comment"># 打印反向传播之后的梯度值</span></span><br></pre></td></tr></table></figure><hr><pre><code>conv1.bias.grad before backwardtensor([ 0.,  0.,  0.,  0.,  0.,  0.])conv1.bias.grad after backwardtensor([ 0.0383,  0.1029,  0.0044,  0.1332,  0.0659, -0.0402])</code></pre><h4 id="权值更新"><a href="#权值更新" class="headerlink" title="权值更新"></a>权值更新</h4><p>实践中最简单的更新规则是随机梯度下降（SGD）：<br><code>weight = weight - learning_rate * gradient</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure><p>然而，当使用神经网络时，希望使用各种不同的更新规则，例如<code>SGD</code>，<code>Nesterov-SGD</code>，<code>Adam</code>，<code>RMSProp</code>等等。为了实现这一点，Pytorch构建一个优化包：<code>torch.optim</code>，来实现所有的方法。使用非常简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建优化器</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练的循环迭代中使用</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># 清除现有的梯度</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># 更新值</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：在观察梯度变化时，首先要通过<code>optimizer.zero_grad()</code><strong>清除现有的梯度，否则梯度将累积到现有梯度</strong>。</p></blockquote><h3 id="训练分类器"><a href="#训练分类器" class="headerlink" title="训练分类器"></a>训练分类器</h3><p>前面的教程中我们已经学习了如何定义神经网络，计算损失并更新网络的权重。接下来，我们完整的训练一个神经网络模型，并测试其性能。</p><h4 id="数据集说明"><a href="#数据集说明" class="headerlink" title="数据集说明"></a>数据集说明</h4><p>一般来说，当在处理图像，文本，音频或视频数据时，可以使用标准的<code>python</code>包将数据加载到一个<code>numpy</code>数组中。然后将这个数组转换成<code>torch.Tensor</code>。</p><ul><li>图像的话，可以用<code>Pillow</code>, <code>OpenCV</code>。</li><li>声音处理可以用<code>scipy</code>和<code>librosa</code>。</li><li>文本的处理使用原生<code>Python</code>或者<code>Cython</code>以及<code>NLTK</code>和<code>SpaCy</code>都可以。<br>特别是对于图像，<code>PyTorch</code>创建了一个名为<code>torchvision</code>的软件包，该软件包具有常用数据集（如<code>Imagenet</code>，<code>CIFAR10</code>，<code>MNIST</code>等）的数据加载器<code>torchvision.datasets</code>，以及用于图像的数据转换器<code>torch.utils.data.DataLoader</code>。这提供了巨大的便利并避免了编写样板代码。<br>本教程使用CIFAR10数据集。 我们要进行的分类的类别有：’airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。 这个数据集中的图像都是3通道，32x32像素的图片。<br><img src="http://xukeqiniu.xukeai.cn/dbb86359ca79fc4aaa66c65ef7258a04.png" alt=""></li></ul><h4 id="训练一个图像分类器"><a href="#训练一个图像分类器" class="headerlink" title="训练一个图像分类器"></a>训练一个图像分类器</h4><p>我们要按顺序做这几个步骤：</p><ul><li>使用torchvision来读取并预处理CIFAR10数据集</li><li>定义一个卷积神经网络</li><li>定义一个代价函数</li><li>在神经网络中训练训练集数据</li><li>使用测试集数据测试神经网络</li></ul><h5 id="1-加载和归一化CIFAR10"><a href="#1-加载和归一化CIFAR10" class="headerlink" title="1.加载和归一化CIFAR10"></a>1.加载和归一化CIFAR10</h5><p><code>torchvision</code>加载的数据集的输出是范围[0，1]的<code>PILImage</code>图像。我们将它们转换为归一化范围[-1，1]的张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="comment"># torchvision数据集的输出是在[0, 1]范围内的PILImage图片。</span></span><br><span class="line"><span class="comment"># 我们此处使用归一化的方法将其转化为Tensor，数据范围为[-1, 1]</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"><span class="comment">#加载训练集数据</span></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>,</span><br><span class="line">                                        download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="keyword">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#加载测试集数据</span></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">False</span>,</span><br><span class="line">                                       download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="keyword">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#分类类别定义</span></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Files already downloaded and verifiedFiles already downloaded and verified</code></pre><p>我们来从中找几张图片看看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#notebook模式下</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#显示图片的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取一些随机的训练图片</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图片</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># 打印类型</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><hr><pre><code>deer   cat   dog  ship</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pytorch_60_output_95_1.png" alt=""></p><h5 id="2-定义卷积神经网络结构"><a href="#2-定义卷积神经网络结构" class="headerlink" title="2.定义卷积神经网络结构"></a>2.定义卷积神经网络结构</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure><h5 id="3-定义损失函数和优化器"><a href="#3-定义损失函数和优化器" class="headerlink" title="3.定义损失函数和优化器"></a>3.定义损失函数和优化器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h5 id="4-训练网络"><a href="#4-训练网络" class="headerlink" title="4.训练网络"></a>4.训练网络</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># 训练集迭代次数</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 获取输入和标签</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度初始化置零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 正向+反向+优化</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印loss值</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># 每2000个batch打印一次</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>[1,  2000] loss: 2.199[1,  4000] loss: 1.887[1,  6000] loss: 1.707[1,  8000] loss: 1.614[1, 10000] loss: 1.536[1, 12000] loss: 1.504[2,  2000] loss: 1.449[2,  4000] loss: 1.411[2,  6000] loss: 1.372[2,  8000] loss: 1.349[2, 10000] loss: 1.325[2, 12000] loss: 1.306Finished Training</code></pre><h5 id="5-测试网络"><a href="#5-测试网络" class="headerlink" title="5.测试网络"></a>5.测试网络</h5><p>我们已经训练了两遍了。 此时需要测试一下到底结果如何。</p><p>通过对比神经网络给出的分类和已知的类别结果，可以得出正确与否。如果预测的正确，我们可以将样本加入正确预测的结果的列表中。</p><p>好的第一步，让我们展示几张照片来熟悉一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印图片</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><hr><pre><code>GroundTruth:    cat  ship  ship plane</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pyrorch_60_output_104_1.png" alt=""></p><p>现在让我们看看神经网络认为这些例子是什么：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(images)</span><br><span class="line">print(outputs)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-0.4905, -1.5664,  1.0641,  2.4226,  0.1196,  1.9381,  0.9795,         -0.4404, -1.7645, -1.6992],        [ 6.1866,  5.8665, -2.2267, -3.2581, -2.6794, -4.9095, -4.2326,         -5.3548,  6.9980,  2.7097],        [ 1.9322,  3.0127, -1.2481, -1.1180, -1.4086, -1.7913, -1.8129,         -1.9674,  2.3132,  1.7559],        [ 3.6228,  0.1119,  0.6089, -1.5255, -0.5566, -2.7542, -1.1817,         -3.3743,  4.5489, -0.5763]])</code></pre><p>输出是10类对应的数值。一个类对应的数值越高，网络认为这个图像就是越接近这个类。那么，让我们得到最高数值对应的类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><hr><pre><code>Predicted:    cat  ship   car  ship</code></pre><p>结果看起来挺好。<br>看看神经网络在整个数据集上的表现结果如何:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure><hr><pre><code>Accuracy of the network on the 10000 test images: 54 %</code></pre><p>从结果上看，神经网络输出的结果比随机要好，随机选择的话从十个中选择一个出来，准确率大概只有10%。<br>看上去神经网络学到了点东西。<br>我们看一下那么到底哪些类别表现良好又是哪些类别不太行呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure><hr><pre><code>Accuracy of plane : 59 %Accuracy of   car : 82 %Accuracy of  bird : 44 %Accuracy of   cat : 44 %Accuracy of  deer : 48 %Accuracy of   dog : 43 %Accuracy of  frog : 59 %Accuracy of horse : 52 %Accuracy of  ship : 73 %Accuracy of truck : 40 %</code></pre><h4 id="GPU训练"><a href="#GPU训练" class="headerlink" title="GPU训练"></a>GPU训练</h4><h4 id="多GPU训练"><a href="#多GPU训练" class="headerlink" title="多GPU训练"></a>多GPU训练</h4><h3 id="数据并行（选学）"><a href="#数据并行（选学）" class="headerlink" title="数据并行（选学）"></a>数据并行（选学）</h3><h4 id="导入和参数"><a href="#导入和参数" class="headerlink" title="导入和参数"></a>导入和参数</h4><h4 id="虚拟DataSet"><a href="#虚拟DataSet" class="headerlink" title="虚拟DataSet"></a>虚拟DataSet</h4><h4 id="简单模块"><a href="#简单模块" class="headerlink" title="简单模块"></a>简单模块</h4><h4 id="创建模块和数据并行"><a href="#创建模块和数据并行" class="headerlink" title="创建模块和数据并行"></a>创建模块和数据并行</h4><h4 id="执行模块"><a href="#执行模块" class="headerlink" title="执行模块"></a>执行模块</h4><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tnesorpack入门教程</title>
      <link href="/2018/04/01/DeepLearning/Tensorpack/Tensorpack%E7%AE%80%E4%BB%8B/"/>
      <url>/2018/04/01/DeepLearning/Tensorpack/Tensorpack%E7%AE%80%E4%BB%8B/</url>
      <content type="html"><![CDATA[<h3 id="Tensorpack架构"><a href="#Tensorpack架构" class="headerlink" title="Tensorpack架构"></a>Tensorpack架构</h3><p><img src="http://xukeqiniu.xukeai.cn/19876566c073a98092648443139221e6.png" alt=""></p><ul><li><code>DataFlow</code>是一个用于在Python中高效地加载数据的库。除了DataFlow之外，本地TF运营商也可以用于数据加载。它们最终将被封装在相同的<code>InputSource</code>接口下并进行预取。</li><li>可以使用任何基于TF的符号函数库来定义模型，其中包括tensorpack中的一小组函数。 <code>ModelDesc</code>是连接模型的接口和<code>InputSource</code>的接口。</li><li>Tensorpack的<code>Trainers</code>用于管理训练过程中的循环迭代。它们还包括用于多GPU或分布式训练的数据并行逻辑。同时，也拥有很强大的定制能力。</li><li><code>Callbacks</code>就像<code>tf.train.SessionRunHook</code>或者<code>plugins</code>。在训练过程中，除了主迭代以外，您想要做的所有事情都可以通过回调进行定义并轻松重用。</li><li>所有组件尽管完美地结合在一起，但都具有高度的去相关性：您可以：<ul><li>单独使用DataFlow作为数据加载库，根本不用tensorfow。</li><li>使用tensorpack构建具有多GPU或分布式支持的图结构，然后使用自己的循环进行训练。</li><li>自行构建图形，并使用tensorpack回调进行训练。</li></ul></li></ul><a id="more"></a><h3 id="DataFlow-数据流接口"><a href="#DataFlow-数据流接口" class="headerlink" title="DataFlow(数据流接口)"></a>DataFlow(数据流接口)</h3><h4 id="DataFlow是什么？"><a href="#DataFlow是什么？" class="headerlink" title="DataFlow是什么？"></a>DataFlow是什么？</h4><p>DataFlow是一个用于构建Python迭代器以提高数据加载效率的库。</p><p><strong>定义</strong>：DataFlow有一个<code>get_data()</code>生成器方法，它产生数据点(<code>datapoints</code>)。datapoints是被称为数据点组件(components of a datapoin)的Python列表对象。</p><p><strong>例子</strong>：要训练MNIST数据集，需要使用<code>DataFlow</code>的<code>get_data()</code>方法，该方法需要生成两个组件的数据点（列表）：一个形状为<code>(64,28,28)</code>的numpy的数组(图片数据)和一个形状<code>(64， )</code>数组（图片标签）。</p><h4 id="DataFlow的组成"><a href="#DataFlow的组成" class="headerlink" title="DataFlow的组成"></a>DataFlow的组成</h4><p>有一个标准接口的好处是能够提供最大的代码可重用性。 tensorpack中有很多现有的DataFlow实用程序，您可以使用这些实用程序来组合具有长数据管道的复杂DataFlow。常见的流水线通常会<strong>从磁盘（或其他来源）读取</strong>，<strong>应用转换（ apply transformations）</strong>，<strong>分组批处理（group into batches）</strong> ，<strong>预取数据（group into batches）</strong>。一个简单的例子如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a DataFlow you implement to produce [tensor1, tensor2, ..] lists from whatever sources:</span></span><br><span class="line">df = MyDataFlow(dir=<span class="string">'/my/data'</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># resize the image component of each datapoint</span></span><br><span class="line">df = AugmentImageComponent(df, [imgaug.Resize((<span class="number">225</span>, <span class="number">225</span>))])</span><br><span class="line"><span class="comment"># group data into batches of size 128</span></span><br><span class="line">df = BatchData(df, <span class="number">128</span>)</span><br><span class="line"><span class="comment"># start 3 processes to run the dataflow in parallel</span></span><br><span class="line">df = PrefetchDataZMQ(df, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><h4 id="为什么要使用DataFlow"><a href="#为什么要使用DataFlow" class="headerlink" title="为什么要使用DataFlow"></a>为什么要使用DataFlow</h4><ul><li><strong>很简单</strong>：用纯Python编写所有内容，并重用现有的实用程序。相反，在TF操作员中编写数据加载器通常很痛苦，性能很难调整。</li><li><strong>速度非常快</strong>：可以构建具有并行性的快速DataFlow。在tensorpack中使用DataFlow，可以采用<code>Input Pipeline</code>,进一步加速图形中的数据加载。</li></ul><blockquote><p>尽管如此，tensorpack还支持用本地TF操作与TF数据集加载数据。</p></blockquote><h4 id="使用DataFlow（Tensorpack外部）"><a href="#使用DataFlow（Tensorpack外部）" class="headerlink" title="使用DataFlow（Tensorpack外部）"></a>使用DataFlow（Tensorpack外部）</h4><p>通常，tensorpack <code>InputSource</code>接口将DataFlow链接到图结构进行训练。如果在某些自定义代码中使用DataFlow，需要<strong>首先调用<code>reset_state()</code>初始化</strong>，然后使用生成器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">df = SomeDataFlow()</span><br><span class="line"></span><br><span class="line">df.reset_state() <span class="comment">#初始化</span></span><br><span class="line">generator = df.get_data()</span><br><span class="line"><span class="keyword">for</span> dp <span class="keyword">in</span> generator:</span><br><span class="line">    <span class="comment"># dp is now a list. do whatever</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>DataFlow独立于tensorpack和TensorFlow</strong>。要导入tensorpack.dataflow，甚至不必安装TensorFlow。<strong>可以简单地将其用作数据处理管道并将其插入任何其他框架</strong>。</p></blockquote><h3 id="Input-Pipeline（输入流水线）"><a href="#Input-Pipeline（输入流水线）" class="headerlink" title="Input Pipeline（输入流水线）"></a>Input Pipeline（输入流水线）</h3><blockquote><p>本教程包含关于“如何高效地读取数据以使用TensorFlow”以及tensorpack如何支持这些方法的主题的一般性讨论。 您不必阅读它，因为这些是tensorpack接口下的细节，但知道它可以帮助您在构建任务时，高效的选择最佳输入管道。</p></blockquote><h4 id="数据准备并行化"><a href="#数据准备并行化" class="headerlink" title="数据准备并行化"></a>数据准备并行化</h4><p><img src="http://xukeqiniu.xukeai.cn/98b00ec58d4658755b92f14dab5fa520.png" alt=""></p><p>无论使用什么框架，都可以有一个常识：<strong>Prepare data in parallel with the training!</strong><br>原因如下：</p><ul><li>数据准备通常会耗费不重要的时间（取决于实际问题）。</li><li><strong>数据准备与训练是独立的</strong>！这个并行的最根本前提！</li><li><strong>数据准备与训练通常使用完全不同的资源</strong>（参见上图）：Training过程使用GPU，加载数据的过程通过磁盘，处理数据采用CPU，拷贝数据到TF经过的是内存带宽，而拷贝到GPU是通过PCI-e总线。因此训练和数据准备两项任务一起完成并不会降低工作效率。事实上，你可以进一步并行化数据准备实现的不同阶段，因为他们也使用不同的资源。</li></ul><p>我们可以简单的计算一下：</p><blockquote><p>4台P100 GPU可以852张/秒的速度训练ResNet50，这些图像的大小为852 <em> 224 </em> 224 <em> 3 </em> 4bytes = 489MB。 假设你有 5GB / s的memcpy带宽（如果你运行单线程拷贝，大致就像这样），只需复制一次数据就需要0.1s ——<strong>将训练速度减慢10％</strong>。 并且在预处理期间还有很多运算成本依然需要耗费大量时间！。</p></blockquote><p><strong>未能隐藏数据准备延迟是看不到良好GPU利用率的主要原因</strong>。 因此一定要选择一个能够延迟隐藏的框架。 但是大多数其他TensorFlow封装都是基于<code>feed_dict</code>设计的。 Tensorpack有内置的机制来隐藏上述阶段的延迟。 这是Tensorpack速度更快的主要原因。</p><h4 id="Python-Reader-or-TF-Reader"><a href="#Python-Reader-or-TF-Reader" class="headerlink" title="Python Reader or TF Reader ?"></a>Python Reader or TF Reader ?</h4><p>无论您使用什么来加载/预处理数据，上述讨论都是有效的，无论是Python代码还是TensorFlow运算符，或者是两者的组合。这两个都支持tensorpack，推荐使用Python。</p><h5 id="TensorFlow-Reader优点"><a href="#TensorFlow-Reader优点" class="headerlink" title="TensorFlow Reader优点"></a>TensorFlow Reader优点</h5><p>人们经常认为他们应该使用tf.data，因为它很快。</p><ul><li>事实上，它一般情况下很快，但不一定。使用Python，您可以访问许多其他快速库，这些库在TF中可能不受支持。</li><li>Python足够快。只要数据准备与训练保持同步，并且上图中所有四个模块的延迟都隐藏起来，<strong>更快的读取不会对整体吞吐量产生任何影响</strong>。 对于大多数类型的问题，直到多GPU ImageNet训练的规模，如果您使用快速库（例如<code>tensorpack.dataflow</code>），Python可以提供足够的速度。</li></ul><h5 id="TensorFlow-Reader缺点"><a href="#TensorFlow-Reader缺点" class="headerlink" title="TensorFlow Reader缺点"></a>TensorFlow Reader缺点</h5><p>TensorFlow Reader的缺点显而易见——<strong>接口设计太复杂了！</strong><br>与运行数学模型不同，数据处理是一项复杂且结构不良（ poorly-structured）的任务。 您需要处理不同的格式，处理特殊案例，嘈杂的数据，数据组合。 这样做需要条件操作，循环，数据结构，有时甚至是异常处理。 这些操作自然不是符号图的该做的相关任务。</p><p>我们来看看用户对tf.data的要求：</p><ul><li>填充数据，混洗数据的不同方式</li><li>处理数据中的任何值</li><li>处理不是批量大小倍数的数据集</li><li>排序/跳过一些数据</li><li>将数据写入文件</li></ul><p>为了支持所有这些可以在Python中使用3行代码完成的功能，您需要一个新的TF API，或者向Dataset.from_generator（即Python再次）提出要求。<br>如果您的数据本来就非常干净并且格式正确，那么使用TF来读取数据才有意义。如果没有，你可能会想写一个脚本来格式化你的数据，但是你几乎已经写了一个Python加载器了！<br>考虑一下：编写一个Python脚本以便从某种格式转换为TF友好格式，然后是从这种格式转换为张量的TF脚本是浪费时间。 中间格式不一定存在。 您只需要正确的界面即可直接高效地将Python连接到图形。 <code>tensorpack.InputSource</code>就是这样的一个接口。</p><h4 id="InputSource"><a href="#InputSource" class="headerlink" title="InputSource"></a>InputSource</h4><p><code>InputSource</code>是tensorpack中的抽象接口，用于描述输入来自哪里以及它们如何进入图形。例如，</p><ul><li>FeedInput：来自DataFlow并获取图表（缓慢）。</li><li>QueueInput：来自DataFlow并由TF队列缓存在CPU上。</li><li>StagingInput：来自某个<code>InputSource</code>，然后由TF StagingArea在GPU上预取。</li><li>TFDatasetInput：来自<code>tf.data.Dataset</code>。</li><li>dataflow_to_dataset：来自DataFlow，并由<code>tf.data.Dataset</code>进一步处理。</li><li>TensorInput：来自你定义的张量（例如可以是读操作）。</li><li>ZMQInput：来自一些ZeroMQ管道，读取/预处理可能发生在不同的过程中，甚至不同的机器中。</li></ul><blockquote><p>通常，我们推荐<code>QueueInput + StagingInput</code>，因为它对大多数用例都很有用。 如果您的数据因任何原因必须来自单独的进程，请使用<code>ZMQInput</code>。 如果您仍然想使用TF读取操作，请定义一个<code>tf.data.Dataset</code>并使用<code>TFDatasetInput</code>。</p></blockquote><h3 id="Symbolic-Layers（符号层）"><a href="#Symbolic-Layers（符号层）" class="headerlink" title="Symbolic Layers（符号层）"></a>Symbolic Layers（符号层）</h3><p>Tensorpack包含一小部分通用模型基元，如conv / deconv，fc，bn，pooling层。 这些层的编写只是因为在tensorpack开发时没有其他选择。 现在，这些实现实际上可以<strong>直接调用<code>tf.layers</code></strong>。</p><p>现在，<strong>可以在tensorpack中使用tf.layers或任何其他符号库</strong>。使用tensorpack实现，您还可以通过<code>argscope</code>和<code>LinearWrap</code>，以简化代码。</p><blockquote><p>请注意，为了保持代码和预先训练模型的向后兼容性，tensorpack层与tf.layers有一些细微差别，包括变量名称和默认选项。有关详细信息，请参阅tensorpack API文档。</p></blockquote><h4 id="argscope-and-LinearWrap"><a href="#argscope-and-LinearWrap" class="headerlink" title="argscope and LinearWrap"></a>argscope and LinearWrap</h4><p><code>argscope</code>为您提供默认参数的上下文。 <code>LinearWrap</code>是简化构建“线性结构”模型的糖衣语法(syntax sugar)。</p><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> argscope(Conv2D, filters=<span class="number">32</span>, kernel_size=<span class="number">3</span>, activation=tf.nn.relu):</span><br><span class="line">  l = (LinearWrap(image)  <span class="comment"># the starting brace is only for line-breaking</span></span><br><span class="line">       .Conv2D(<span class="string">'conv0'</span>)</span><br><span class="line">       .MaxPooling(<span class="string">'pool0'</span>, <span class="number">2</span>)</span><br><span class="line">       .Conv2D(<span class="string">'conv1'</span>, padding=<span class="string">'SAME'</span>)</span><br><span class="line">       .Conv2D(<span class="string">'conv2'</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">       .FullyConnected(<span class="string">'fc0'</span>, <span class="number">512</span>, activation=tf.nn.relu)</span><br><span class="line">       .Dropout(<span class="string">'dropout'</span>, rate=<span class="number">0.5</span>)</span><br><span class="line">       .tf.multiply(<span class="number">0.5</span>)</span><br><span class="line">       .apply(func, *args, **kwargs)</span><br><span class="line">       .FullyConnected(<span class="string">'fc1'</span>, units=<span class="number">10</span>, activation=tf.identity)())</span><br></pre></td></tr></table></figure></p><p>上面示例代码等价如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">l = Conv2D(<span class="string">'conv0'</span>, image, <span class="number">32</span>, <span class="number">3</span>, activation=tf.nn.relu)</span><br><span class="line">l = MaxPooling(<span class="string">'pool0'</span>, l, <span class="number">2</span>)</span><br><span class="line">l = Conv2D(<span class="string">'conv1'</span>, l, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="string">'SAME'</span>, activation=tf.nn.relu)</span><br><span class="line">l = Conv2D(<span class="string">'conv2'</span>, l, <span class="number">32</span>, <span class="number">5</span>, activation=tf.nn.relu)</span><br><span class="line">l = FullyConnected(<span class="string">'fc0'</span>, l, <span class="number">512</span>, activation=tf.nn.relu)</span><br><span class="line">l = Dropout(<span class="string">'dropout'</span>, l, rate=<span class="number">0.5</span>)</span><br><span class="line">l = tf.multiply(l, <span class="number">0.5</span>)</span><br><span class="line">l = func(l, *args, **kwargs)</span><br><span class="line">l = FullyConnected(<span class="string">'fc1'</span>, l, <span class="number">10</span>, activation=tf.identity)</span><br></pre></td></tr></table></figure><h4 id="访问相关的张量"><a href="#访问相关的张量" class="headerlink" title="访问相关的张量"></a>访问相关的张量</h4><p>层中的变量将命名为<code>name / W</code>，<code>name / b</code>等。有关详细信息，请参阅每个图层的API文档。在构建图时，可以像这样访问变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">l = Conv2D(<span class="string">'conv1'</span>, l, <span class="number">32</span>, <span class="number">3</span>)</span><br><span class="line">print(l.variables.W)</span><br><span class="line">print(l.variables.b)</span><br></pre></td></tr></table></figure><blockquote><p>但请注意，这是一种很冒险的方式，可能不适用于未来版本的TensorFlow。此方法也不适用于LinearWrap，并且无法访问由激活函数创建的变量。除非在API中有不同的说明，否则层的输出通常被命名为<code>name/output</code>。你可以打印张量来查看它的名字。</p></blockquote><h4 id="在Tensorpack外使用模型"><a href="#在Tensorpack外使用模型" class="headerlink" title="在Tensorpack外使用模型"></a>在Tensorpack外使用模型</h4><p>您可以单独使用tensorpack模型作为简单的符号函数库。为此，只需在定义模型时输入<code>TowerContext</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> TowerContext(<span class="string">''</span>, is_training=<span class="keyword">True</span>):</span><br><span class="line">  <span class="comment"># call any tensorpack layer</span></span><br></pre></td></tr></table></figure><p>某些层（特别是BatchNorm）具有不同的训练/测试时间行为,此行为由TowerContext控制。如果您需要在测试时间使用它们的tensorpack版本，则需要在另一个上下文中为它们创建操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Open a `reuse=True` variable scope here if you're sharing variables, then:</span></span><br><span class="line"><span class="keyword">with</span> TowerContext(<span class="string">'some_name_or_empty_string'</span>, is_training=<span class="keyword">False</span>):</span><br><span class="line">  <span class="comment"># build the graph again</span></span><br></pre></td></tr></table></figure><h4 id="在Tensorpack中使用其他符号库"><a href="#在Tensorpack中使用其他符号库" class="headerlink" title="在Tensorpack中使用其他符号库"></a>在Tensorpack中使用其他符号库</h4><p>在定义模型的过程中可以使用任何觉得舒服的库来构建图结构。<br>通常，<code>slim</code> / <code>tflearn</code> / <code>tensorlayer</code>只是符号函数包装器，调用它们与调用<code>tf.add</code>没什么区别。 不过，需要小心如何正规化/ BN更新应该在这些库中的处理。<br>这与使用 <code>sonnet</code>/ <code>Keras</code>有点不同。 <code>sonnet</code> / <code>Keras</code> 通过自己的模型类来管理变量范围，调用它们的符号函数总是创建新的变量范围。 请参阅Keras示例以了解如何在tensorpack内使用它。</p><h3 id="Trainers"><a href="#Trainers" class="headerlink" title="Trainers"></a>Trainers</h3><p>Tensorpack训练器包含以下逻辑：</p><ul><li>构件图结构</li><li>运行迭代（带回调）<blockquote><p>通常我们不会直接触摸这些方法，而是在训练器上使用更高级的接口。我们只需选择要使用的训练器。但是他们如何工作的一些基本知识是有用的：</p></blockquote></li></ul><h4 id="Tower-Trainer"><a href="#Tower-Trainer" class="headerlink" title="Tower Trainer"></a>Tower Trainer</h4><p>遵循TensorFlow中的术语，<code>Tower Trainer</code>是一个可调用的函数，它接受输入张量并将模型的<strong>一个重复项</strong>添加到图中。这种概念可以描述大多数类型的神经网络训练。<code>Tower</code>的概念主要用于支持：</p><ul><li>数据并行多GPU训练，其中在每个GPU上构建复制品。</li><li>用于推理的图构造，其中复制是在推理模式下构建的。<br>用户需要提供Tower函数才能使用<code>TowerTrainer</code>。特别是，在使用<code>ModelDesc</code>接口时，<code>build_graph</code>方法将成为Tower函数。</li></ul><p>Tower函数需要遵循一些约定：</p><ul><li>它可能被<strong>多次调用</strong>以进行数据并行训练或推理。<ul><li>因此，要使用tensorflow-hub模块，需要在tower函数外初始化模块，并在tower函数内调用模块。</li></ul></li><li>它必须遵循变量集合<ul><li>（必需）只能将可通过梯度下降调整的变量放入<code>TRAINABLE_VARIABLES</code>中。</li><li>（推荐）将需要用于推理的不可训练变量放入<code>MODEL_VARIABLES</code>中。</li></ul></li><li>它必须遵循变量范围：<ul><li>在函数中创建的任何可训练变量的名称必须与“variable_scope_name / custom / name”类似。不要依赖于name_scope的名字。不要使用两次variable_scope的名字。</li></ul></li><li>创建任何可训练变量都必须遵守<strong>重用</strong>变量范围。要遵守变量重用，请在函数中使用<code>tf.get_variable</code>而不是<code>tf.Variable</code>。另一方面，对于不可训练的变量，可以使用<code>tf.Variable</code>确保在每个tower中创建新变量，即使<code>reuse= True</code>时也是如此。</li><li>总是在<code>TowerContext</code>下调用，可以通过<code>get_current_tower_contxt()</code>来访问它。上下文包含有关训练/推理模式，重用等信息。<blockquote><p>这些约定很容易遵循，并且大多数层封装（例如，tf.layers / slim / tensorlayer）确实遵循它们。 请注意，某些Keras层不遵循这些约定，如果在tensorpack中使用，将需要一些变通方法。</p></blockquote></li></ul><p>当然你也可以不这样编写，但所有现有的tensorpack trainers都是TowerTrainer的子类。</p><h4 id="MultiGPU-Trainers"><a href="#MultiGPU-Trainers" class="headerlink" title="MultiGPU Trainers"></a>MultiGPU Trainers</h4><p>对于数据并行多GPU训练，不同的多GPU训练将实施不同的分配策略。他们以有效的方式照顾器件布局，梯度平均和同步，并且都达到了官方TF基准测试的相同性能。只需要一行代码更改即可使用它们，即<code>trainer=SyncMultiGPUTrainerReplicated()</code>。<br>请注意使用这些trains时的一些<strong>常见问题</strong>：</p><ul><li><p>在每次迭代中，所有GPU（模型的所有复制品）从InputSource中获取张量，而不是全部和分割。 所以总的批量大小将变成<code>(batch size of InputSource) * #GPU</code></p><blockquote><p>为数据并行训练分割张量根本没有意义。 首先，为什么要将时间串联起来分成大批量然后再分开呢？ 其次，这会对数据造成不必要的形状限制。 通过让每个GPU训练自己的输入张量，他们可以同时训练不同形状的输入。</p></blockquote></li><li><p>tower函数（您的模型代码）将被称为乘法时间。 因此，在修改这些函数中的全局状态时需要小心，例如 将操作添加到TF集合中。</p></li></ul><h4 id="Distributed-Trainers"><a href="#Distributed-Trainers" class="headerlink" title="Distributed Trainers"></a>Distributed Trainers</h4><p>分布式培训需要提供高性能allreduce实现的horovod库。要运行分布式培训，首先正确安装horovod，然后参阅HorovodTrainer的文档。</p><p>Tensorpack已经使用TF的本地API实现了一些其他分布式训练，但即使在今天，TF对分布式训练的本地支持也不是很高的性能。 因此这些训练没有积极维护，不推荐使用。</p><h3 id="Training-Interface"><a href="#Training-Interface" class="headerlink" title="Training Interface"></a>Training Interface</h3><p>Tensorpack有一个详尽的接口以获得最大的灵活性。当不想过多定制，训练时可以使用tensorpack提供的接口来简化代码。</p><h4 id="使用ModelDesc和TrainConfig"><a href="#使用ModelDesc和TrainConfig" class="headerlink" title="使用ModelDesc和TrainConfig"></a>使用ModelDesc和TrainConfig</h4><p>这是旧Tensorpack用户最熟悉的接口，仅用于单一成本任务（ single-cost tasks ）。这个接口有很多例子。<br><code>SingleCost trainers</code>需要4个参数来设置图：<code>InputDesc</code>，<code>InputSource</code>，<code>get_cost</code>函数和<code>optimizer</code>。<code>ModelDesc</code>通过将它们中的三个打包成一个对象来描述一个模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span><span class="params">(ModelDesc)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_inputs</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [tf.placeholder(dtype, shape, name), tf.placeholder(dtype, shape, name), ... ]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_build_graph</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">    tensorA, tensorB = inputs</span><br><span class="line">    <span class="comment"># build the graph</span></span><br><span class="line">    self.cost = xxx   <span class="comment"># define the cost tensor</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_optimizer</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.GradientDescentOptimizer(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p><code>_get_inputs</code> 需要定义构建图的所有输入的元信息。</p><p><code>_build_graph</code>获取与<code>_get_inputs</code>匹配的<code>inputs</code>张量列表。</p><p>在<code>_build_graph</code>中可以使用任何符号函数，包括TensorFlow核心库函数和其他符号库。<code>_build_graph</code>是tower函数，所以需要<strong>遵循一些规则</strong>。同时还需要在此函数中设置<code>self.cost</code>。</p><p>定义了这样一个模型后，使用<code>TrainConfig</code>和<code>launch_train_with_config</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">config = TrainConfig(</span><br><span class="line">   model=MyModel()</span><br><span class="line">   dataflow=my_dataflow,</span><br><span class="line">   <span class="comment"># data=my_inputsource, # alternatively, use a customized InputSource</span></span><br><span class="line">   callbacks=[...],    <span class="comment"># some default callbacks are automatically applied</span></span><br><span class="line">   <span class="comment"># some default monitors are automatically applied</span></span><br><span class="line">   steps_per_epoch=<span class="number">300</span>,   <span class="comment"># default to the size of your InputSource/DataFlow</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = SomeTrainer()</span><br><span class="line"><span class="comment"># trainer = SyncMultiGPUTrainerParameterServer(8)</span></span><br><span class="line">launch_train_with_config(config, trainer)</span><br></pre></td></tr></table></figure><h4 id="Raw-Trainer-Interface"><a href="#Raw-Trainer-Interface" class="headerlink" title="Raw Trainer Interface"></a>Raw Trainer Interface</h4><p>要获得较低级别的控件，您还可以直接访问训练的方法：</p><ul><li><strong>构建图</strong>：对于一般训练，请自行构建图结构。 对于单成本训练（ single-cost trainer），请通过<code>SingleCostTrainer.setup_graph</code>构建图结构。</li><li><strong>运行迭代</strong>：调用<code>Trainer.train()</code>或`Trainer.train_with_defaults()``，它为正常用例提供了一些默认选项。</li></ul><h3 id="Callbacks"><a href="#Callbacks" class="headerlink" title="Callbacks"></a>Callbacks</h3><p>回调是除了训练迭代以外的<strong>其他所有事情</strong>的接口。<br>除了最小化cost的实际训练迭代之外，你可能想要做一些其他事情。如下：</p><ul><li>在训练开始之前（例如：初始化保存程序，转储图）</li><li>随着每次训练迭代（例如，在图中运行一些其他操作）</li><li>在训练迭代之间（例如更新进度条，更新超参数）</li><li>在周期之间（例如保存模型，运行一些验证）</li><li>训练后（例如，在某处发送模型，发送消息到您的手机）</li></ul><p>人们传统上倾向于将训练循环与这些额外功能一起编写。这会使循环冗长，同一特征的代码可能会分开（设想一个特征，它需要在开始时进行初始化，然后在迭代之间进行一些实际工作）。</p><p>通过编写回调来实现在每个地方做什么，tensorpack训练器将在适当的时候调用回调。因此，只要您使用tensorpack，这些功能就可以一条线（single line）的重复使用。</p><p>例如，这些在训练ResNet时使用的回调：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">callbacks=[</span><br><span class="line">  <span class="comment"># save the model every epoch</span></span><br><span class="line">  ModelSaver(),</span><br><span class="line">  <span class="comment"># backup the model with best validation error</span></span><br><span class="line">  MinSaver(<span class="string">'val-error-top1'</span>),</span><br><span class="line">  <span class="comment"># run inference on another Dataflow every epoch, compute classification error and log to monitors</span></span><br><span class="line">  InferenceRunner(dataset_val, [</span><br><span class="line">      ClassificationError(<span class="string">'wrong-top1'</span>, <span class="string">'val-error-top1'</span>),</span><br><span class="line">      ClassificationError(<span class="string">'wrong-top5'</span>, <span class="string">'val-error-top5'</span>)]),</span><br><span class="line">  <span class="comment"># schedule the learning rate based on epoch number</span></span><br><span class="line">  ScheduledHyperParamSetter(<span class="string">'learning_rate'</span>,</span><br><span class="line">                            [(<span class="number">30</span>, <span class="number">1e-2</span>), (<span class="number">60</span>, <span class="number">1e-3</span>), (<span class="number">85</span>, <span class="number">1e-4</span>), (<span class="number">95</span>, <span class="number">1e-5</span>)]),</span><br><span class="line">  <span class="comment"># can manually change the learning rate through a file, without interrupting training</span></span><br><span class="line">  HumanHyperParamSetter(<span class="string">'learning_rate'</span>),</span><br><span class="line">  <span class="comment"># send validation error to my phone through pushbullet</span></span><br><span class="line">  SendStat(<span class="string">'curl -u your_id_xxx: https://api.pushbullet.com/v2/pushes \\</span></span><br><span class="line"><span class="string">             -d type=note -d title="validation error" \\</span></span><br><span class="line"><span class="string">             -d body=&#123;val-error-top1&#125; &gt; /dev/null 2&gt;&amp;1'</span>,</span><br><span class="line">             <span class="string">'val-error-top1'</span>),</span><br><span class="line">  <span class="comment"># record GPU utilizations during training</span></span><br><span class="line">  GPUUtilizationTracker(),</span><br><span class="line">  <span class="comment"># touch a file to pause the training and start a debug shell, to observe what's going on</span></span><br><span class="line">  InjectShell(shell=<span class="string">'ipython'</span>),</span><br><span class="line">  <span class="comment"># estimate time until completion</span></span><br><span class="line">  EstimatedTimeLeft()</span><br><span class="line">] + [    <span class="comment"># these callbacks are enabled by default already, though you can customize them</span></span><br><span class="line">  <span class="comment"># maintain those moving average summaries defined in the model (e.g. training loss, training error)</span></span><br><span class="line">  MovingAverageSummary(),</span><br><span class="line">  <span class="comment"># draw a progress bar</span></span><br><span class="line">  ProgressBar(),</span><br><span class="line">  <span class="comment"># run `tf.summary.merge_all` every epoch and log to monitors</span></span><br><span class="line">  MergeAllSummaries(),</span><br><span class="line">  <span class="comment"># run ops in GraphKeys.UPDATE_OPS collection along with training, if any</span></span><br><span class="line">  RunUpdateOps(),</span><br><span class="line">],</span><br><span class="line">monitors=[        <span class="comment"># monitors are a special kind of callbacks. these are also enabled by default</span></span><br><span class="line">  <span class="comment"># write everything to tensorboard</span></span><br><span class="line">  TFEventWriter(),</span><br><span class="line">  <span class="comment"># write all scalar data to a json file, for easy parsing</span></span><br><span class="line">  JSONWriter(),</span><br><span class="line">  <span class="comment"># print all scalar data every epoch (can be configured differently)</span></span><br><span class="line">  ScalarPrinter(),</span><br><span class="line">]</span><br></pre></td></tr></table></figure><blockquote><p>请注意，回调涵盖了训练的每个细节，从图操作到进度条。这意味着我们可以根据自己的喜好自定义训练的各个部分，例如，在进度条上显示不同的内容，以不同训练阶段评估部分结果等。</p></blockquote><p>这些功能并不总是必要的，但是想想如果你将这些逻辑与循环一起写入，主循环看起来有多混乱。如果您在需要时只用一条线即可启用这些功能，那么将变得多么简单。</p><h3 id="Save-and-Load-models"><a href="#Save-and-Load-models" class="headerlink" title="Save and Load models"></a>Save and Load models</h3><h4 id="Work-with-TF-Checkpoint"><a href="#Work-with-TF-Checkpoint" class="headerlink" title="Work with TF Checkpoint"></a>Work with TF Checkpoint</h4><p><code>ModelSaver</code>回调以TensorFlow检查点(checkpoint)格式将模型保存到<code>logger.get_logger_dir()</code>定义的目录中。TF检查点通常包含<code>.data-xxxxx</code>文件和<code>.index</code>文件。两者都是必要的。<br><code>tf.train.NewCheckpointReader</code>是解析TensorFlow检查点的最佳工具。我们有两个示例脚本来演示它的用法，但请阅读TF文档以获取详细信息。</p><ul><li><code>scripts/ls-checkpoint.py</code>演示如何在检查点打印所有变量及其形状。</li><li><code>scripts/dump-model-params.py</code>可用于删除检查点中不必要的变量。它需要一个<code>metagraph</code>文件（它也保存在ModelSaver中），只保存模型在推理时需要的变量。它可以将模型转储到以npz格式保存的<code>var-name：value</code>字典中。</li></ul><h4 id="Load-a-Model"><a href="#Load-a-Model" class="headerlink" title="Load a Model"></a>Load a Model</h4><p>模型加载（在训练或测试中）是通过<code>session_init</code>接口。 目前有两种方法可以恢复会话：<code>session_init = SaverRestore(…)</code>来恢复TF检查点，或<code>session_init = DictRestore(…)</code>来恢复字典。<code>get_model_loader</code>帮助决定使用文件名中哪一个。要加载多个模型，请使用<code>ChainInit</code>。<br>变量还原完全基于当前图中变量与session_init初始值设定项中的变量之间的<strong>名称匹配</strong>。仅出现在一侧的变量将被打印为警告。</p><h4 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h4><p>迁移学习是很简单的。如果你想加载一些模型，只需使用相同的变量名称即可。如果你想重新训练一些层，只需重新命名相关层即可。</p><h3 id="Summary-and-Logging"><a href="#Summary-and-Logging" class="headerlink" title="Summary and Logging"></a>Summary and Logging</h3><blockquote><p>在训练期间，除迭代以外的所有内容均通过回调执行。本教程将解释如何在回调中处理汇总和日志记录，以及如何定制它们。默认的日志记录行为应该足够用于正常的用例，所以你可以跳过本教程。</p></blockquote><h4 id="TensorFlow-Summaries"><a href="#TensorFlow-Summaries" class="headerlink" title="TensorFlow Summaries"></a>TensorFlow Summaries</h4><p>这是TensorFlow如何最终被记录/保存/打印的总结：</p><ul><li><strong>What to Log</strong>：当在构造图代码中调用<code>tf.summary.xxx</code>时，TensorFlow会将一个操作添加到<code>tf.GraphKeys.SUMMARIES</code>集合（默认情况下）。</li><li><strong>When to Log</strong>：<code>MergeAllSummaries</code>回调位于默认回调中。它每个周期（默认情况下）在<code>SUMMARIES</code>集合（默认情况下）运行，并将结果写入监视器。</li><li><strong>Where to Log</strong>：默认情况下启用多个监视器。<ul><li><code>TFEventWriter</code>通过<code>tensorboard</code>将东西写入所使用的事件中。</li><li><code>ScalarPrinter</code>打印终端中的所有标量。</li><li><code>JSONWriter</code>将标量保存到JSON文件。</li></ul></li></ul><p>所有的 “what, when, where”可以在图或者回调(callbacks)/监视器(monitors)设置中自定义。<br>由于<code>TF summaries</code>在默认情况下不会频繁被评估（每个epoch），如果内容是依赖于数据的，那么这些值可能具有很高的方差。 为了解决这个问题，你可以：</p><ul><li>更改“What to Log”：更频繁地记录日志，但请注意，记录某些summaries可能很昂贵。可能需要使用单独的集合进行频繁日志记录。</li><li>更改“When to Log”：可以在标量张量上调用<code>tfutils.summary.add_moving_summary</code>，它将总结这些标量的移动平均值，而不是它们的即时值。移动平均由<code>MovingAverageSummary</code>回调（默认启用）维护。</li></ul><h4 id="Other-Logging-Data"><a href="#Other-Logging-Data" class="headerlink" title="Other Logging Data"></a>Other Logging Data</h4><p>除了TensorFlow summaries外，回调还可以在训练开始后的任何时候通过<code>self.trainer.monitors.put_xxx</code>将其他数据写入监视器后端。只要支持数据类型，数据将被分派到并记录到同一个地方。<br>因此，<strong>tensorboard不仅会显示图中的信息，还会显示我们自定义的数据</strong>。例如，验证集的准确度通常需要在TensorFlow图之外计算。通过一个统一的监视器后端，这个数字也会显示在tensorboard上。</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="Inference-During-Training"><a href="#Inference-During-Training" class="headerlink" title="Inference During Training"></a>Inference During Training</h4><p>训练期间有两种方法可以进行推理。</p><ul><li>最简单的方法是编写回调函数，并使用<code>self.trainer.get_predictor()</code>在推理模式下获取可调用的函数。</li><li>如果推理过程中遵循以下范例：“每个输入获取一些张量，并汇总结果”。可以使用<code>InferenceRunner</code>接口和一些<code>Inferencer</code>。这将进一步支持预取和数据并行推断。</li></ul><p>在这两种方法中，tower函数都会被再次调用，使用<code>TowerContext.is_training == False</code>构建不同的图。</p><h4 id="Inference-After-Training"><a href="#Inference-After-Training" class="headerlink" title="Inference After Training"></a>Inference After Training</h4><p>Tensorpack<strong>不关心训练后的操作</strong>。它将模型保存为标准检查点格式，以及metagraph protobuf文件。它们足以用于TensorFlow进行任何部署方法。但是您需要阅读TF文档并自行完成。</p><h3 id="FAQs"><a href="#FAQs" class="headerlink" title="FAQs"></a>FAQs</h3>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> Tensorpack </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Tensorpack </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow神经网络之DCGAN</title>
      <link href="/2018/03/15/DeepLearning/TensorFlow/15Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BDCGAN/"/>
      <url>/2018/03/15/DeepLearning/TensorFlow/15Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BDCGAN/</url>
      <content type="html"><![CDATA[<h3 id="DCGAN简介"><a href="#DCGAN简介" class="headerlink" title="DCGAN简介"></a>DCGAN简介</h3><p>DCGAN在GAN的基础上优化了网络结构，加入了 <code>conv</code>，<code>batch_norm</code> 等层，使得网络更容易训练，网络结构如下：<br><img src="http://xukeqiniu.xukeai.cn/67d781d61c77b7d1985f57ecc932a1e3.png" alt="DCGAN网络结构图"><br>注意：本图只是示例，与下面实际网络参数不对应。</p><a id="more"></a><h3 id="Tensorflow实现DCGAN"><a href="#Tensorflow实现DCGAN" class="headerlink" title="Tensorflow实现DCGAN"></a>Tensorflow实现DCGAN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, absolute_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练参数</span></span><br><span class="line">num_steps = <span class="number">10000</span> <span class="comment"># 总迭代次数</span></span><br><span class="line">batch_size = <span class="number">128</span> <span class="comment"># 批量大小</span></span><br><span class="line">lr_generator = <span class="number">0.002</span> <span class="comment"># 生成器学习率</span></span><br><span class="line">lr_discriminator = <span class="number">0.002</span> <span class="comment"># 判别器学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络参数</span></span><br><span class="line">image_dim = <span class="number">784</span> <span class="comment"># 28*28 pixels * 1 channel</span></span><br><span class="line">noise_dim = <span class="number">100</span> <span class="comment"># Noise data points</span></span><br></pre></td></tr></table></figure><h4 id="构建DCGAN网络"><a href="#构建DCGAN网络" class="headerlink" title="构建DCGAN网络"></a>构建DCGAN网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建网络</span></span><br><span class="line"><span class="comment"># 网络输入</span></span><br><span class="line">noise_input = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, noise_dim])  <span class="comment"># 生成器输入 噪声 batch*100，none后面被赋值batch</span></span><br><span class="line">real_image_input = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]) <span class="comment"># 判别器输入 真实图像 batch*28*28*1</span></span><br><span class="line"><span class="comment"># A boolean to indicate batch normalization if it is training or inference time</span></span><br><span class="line"><span class="comment"># 判断是否在训练</span></span><br><span class="line">is_training = tf.placeholder(tf.bool)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义激活函数 LeakyReLU，在判别器网络中用</span></span><br><span class="line"><span class="comment"># LeakyReLU 是 ReLU 的变种 [^1]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leakyrelu</span><span class="params">(x, alpha=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * (<span class="number">1</span> + alpha) * x + <span class="number">0.5</span> * (<span class="number">1</span> - alpha) * abs(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义生成器网络</span></span><br><span class="line"><span class="comment"># 输入：噪声  输出：图像</span></span><br><span class="line"><span class="comment"># 训练时，才使用batch_normalization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(x, reuse=False)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'Generator'</span>, reuse=reuse):</span><br><span class="line">        <span class="comment"># 第一层为全连接层，含神经元个数为7*7*128，输入是噪声batch*100</span></span><br><span class="line">        x = tf.layers.dense(x, units=<span class="number">7</span> * <span class="number">7</span> * <span class="number">128</span>)</span><br><span class="line">        <span class="comment"># tf.layers.batch_normalization() 的第二个参数axis表示在哪一个维度做normalize，通常数据排布顺序为(batch, height, width, channels)，固默认为-1</span></span><br><span class="line">        <span class="comment"># 全连接层channel=1，所以是对所有数据做normalize</span></span><br><span class="line">        x = tf.layers.batch_normalization(x, training=is_training)</span><br><span class="line">        <span class="comment"># 激活函数 rule</span></span><br><span class="line">        x = tf.nn.relu(x)</span><br><span class="line">        <span class="comment"># Reshape为4维: (batch, height, width, channels)，这里是 (batch, 7, 7, 128)</span></span><br><span class="line">        x = tf.reshape(x, shape=[<span class="number">-1</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">128</span>])</span><br><span class="line">        <span class="comment"># 反卷积层1</span></span><br><span class="line">        <span class="comment"># 卷积核大小5*5*128，64个，步长2（tf.layers.conv2d_transpose函数前几个参数为input，filters(输出feature map通道数)，kernel_size, strides，padding）</span></span><br><span class="line">        <span class="comment"># 输入x shape：(batch，7，7，128)， 输出image shape: (batch, 14, 14, 64)</span></span><br><span class="line">        x = tf.layers.conv2d_transpose(x, <span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">        <span class="comment"># batch normalization，在channel维度上做normalize</span></span><br><span class="line">        x = tf.layers.batch_normalization(x, training=is_training)</span><br><span class="line">        <span class="comment"># 激活函数 relu</span></span><br><span class="line">        x = tf.nn.relu(x)</span><br><span class="line">        <span class="comment"># 反卷积层2</span></span><br><span class="line">        <span class="comment"># 卷积核大小5*5*128，1个，步长2</span></span><br><span class="line">        <span class="comment"># 输入x shape：(batch，14，14，64)， 输出image shape: (batch, 28, 28, 1)</span></span><br><span class="line">        x = tf.layers.conv2d_transpose(x, <span class="number">1</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">        <span class="comment"># 激活函数 tanh</span></span><br><span class="line">        <span class="comment"># Apply tanh for better stability - clip values to [-1, 1].</span></span><br><span class="line">        x = tf.nn.tanh(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义判别器网络</span></span><br><span class="line"><span class="comment"># 输入：图像, 输出: 预测结果（Real/Fake Image）</span></span><br><span class="line"><span class="comment"># 同样训练时，才使用batch_normalization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(x, reuse=False)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'Discriminator'</span>, reuse=reuse):</span><br><span class="line">        <span class="comment"># 卷积层1，输入x，卷积核大小5x5，64个，步长2</span></span><br><span class="line">        x = tf.layers.conv2d(x, <span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">        x = tf.layers.batch_normalization(x, training=is_training)</span><br><span class="line">        <span class="comment"># 激活函数 leakyrelu</span></span><br><span class="line">        x = leakyrelu(x)</span><br><span class="line">        <span class="comment"># 卷积层2，输入第一个卷积层的输出，卷积核大小5x5，128个，步长2</span></span><br><span class="line">        x = tf.layers.conv2d(x, <span class="number">128</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">        x = tf.layers.batch_normalization(x, training=is_training)</span><br><span class="line">        <span class="comment"># 激活函数 leakyrelu</span></span><br><span class="line">        x = leakyrelu(x)</span><br><span class="line">        <span class="comment"># 展平</span></span><br><span class="line">        x = tf.reshape(x, shape=[<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">128</span>])</span><br><span class="line">        <span class="comment"># 全连接层，含1024个神经元</span></span><br><span class="line">        x = tf.layers.dense(x, <span class="number">1024</span>)</span><br><span class="line">        x = tf.layers.batch_normalization(x, training=is_training)</span><br><span class="line">        <span class="comment"># 激活函数 leakyrelu</span></span><br><span class="line">        x = leakyrelu(x)</span><br><span class="line">        <span class="comment"># 输出2个类别: Real and Fake images</span></span><br><span class="line">        x = tf.layers.dense(x, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建生成器</span></span><br><span class="line">gen_sample = generator(noise_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建两个判别器（一个是真实图像输入，一个是生成图像）</span></span><br><span class="line">disc_real = discriminator(real_image_input)</span><br><span class="line">disc_fake = discriminator(gen_sample, reuse=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the stacked generator/discriminator</span></span><br><span class="line"><span class="comment"># 用于计算生成器的损失</span></span><br><span class="line">stacked_gan = discriminator(gen_sample, reuse=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数，交叉熵</span></span><br><span class="line"><span class="comment"># 真实图像，标签1</span></span><br><span class="line">disc_loss_real = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    logits=disc_real, labels=tf.ones([batch_size], dtype=tf.int32)))</span><br><span class="line"><span class="comment"># 生成图像，标签0</span></span><br><span class="line">disc_loss_fake = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    logits=disc_fake, labels=tf.zeros([batch_size], dtype=tf.int32)))</span><br><span class="line"><span class="comment"># 判别器损失函数是两者之和</span></span><br><span class="line">disc_loss = disc_loss_real + disc_loss_fake</span><br><span class="line"><span class="comment"># 生成器损失函数 (生成器试图骗过判别器，因此这里标签是1)</span></span><br><span class="line">gen_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    logits=stacked_gan, labels=tf.ones([batch_size], dtype=tf.int32)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建优化器（采用Adam方法）</span></span><br><span class="line">optimizer_gen = tf.train.AdamOptimizer(learning_rate=lr_generator, beta1=<span class="number">0.5</span>, beta2=<span class="number">0.999</span>)</span><br><span class="line">optimizer_disc = tf.train.AdamOptimizer(learning_rate=lr_discriminator, beta1=<span class="number">0.5</span>, beta2=<span class="number">0.999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training Variables for each optimizer</span></span><br><span class="line"><span class="comment"># By default in TensorFlow, all variables are updated by each optimizer, so we</span></span><br><span class="line"><span class="comment"># need to precise for each one of them the specific variables to update.</span></span><br><span class="line"><span class="comment"># 生成网络的变量</span></span><br><span class="line">gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="string">'Generator'</span>) <span class="comment"># tf.get_collection：从一个结合中取出全部变量，是一个列表</span></span><br><span class="line"><span class="comment"># 判别器网络的变量</span></span><br><span class="line">disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="string">'Discriminator'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练操作</span></span><br><span class="line"><span class="comment"># TensorFlow UPDATE_OPS collection holds all batch norm operation to update the moving mean/stddev</span></span><br><span class="line">gen_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=<span class="string">'Generator'</span>)</span><br><span class="line"><span class="comment"># `control_dependencies` ensure that the `gen_update_ops` will be run before the `minimize` op (backprop)</span></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies(gen_update_ops):</span><br><span class="line">    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)</span><br><span class="line">disc_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=<span class="string">'Discriminator'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies(disc_update_ops):</span><br><span class="line">    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 变量全局初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start Training</span></span><br><span class="line"><span class="comment"># Start a new TF session</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the initializer</span></span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_steps+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prepare Input Data</span></span><br><span class="line">    <span class="comment"># Get the next batch of MNIST data (only images are needed, not labels)</span></span><br><span class="line">    batch_x, _ = mnist.train.next_batch(batch_size)</span><br><span class="line">    batch_x = np.reshape(batch_x, newshape=[<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># Rescale to [-1, 1], the input range of the discriminator</span></span><br><span class="line">    batch_x = batch_x * <span class="number">2.</span> - <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Discriminator Training</span></span><br><span class="line">    <span class="comment"># Generate noise to feed to the generator</span></span><br><span class="line">    z = np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[batch_size, noise_dim])</span><br><span class="line">    _, dl = sess.run([train_disc, disc_loss], feed_dict=&#123;real_image_input: batch_x, noise_input: z, is_training:<span class="keyword">True</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Generator Training</span></span><br><span class="line">    <span class="comment"># Generate noise to feed to the generator</span></span><br><span class="line">    z = np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[batch_size, noise_dim])</span><br><span class="line">    _, gl = sess.run([train_gen, gen_loss], feed_dict=&#123;noise_input: z, is_training:<span class="keyword">True</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">'Step %i: Generator Loss: %f, Discriminator Loss: %f'</span> % (i, gl, dl))</span><br></pre></td></tr></table></figure><hr><pre><code>Step 1: Generator Loss: 4.064141, Discriminator Loss: 1.679586Step 500: Generator Loss: 1.472707, Discriminator Loss: 0.974612Step 1000: Generator Loss: 1.918907, Discriminator Loss: 0.964812Step 1500: Generator Loss: 2.567637, Discriminator Loss: 0.717904Step 2000: Generator Loss: 2.398796, Discriminator Loss: 0.512406Step 2500: Generator Loss: 3.057401, Discriminator Loss: 1.235215Step 3000: Generator Loss: 2.620444, Discriminator Loss: 0.539795Step 3500: Generator Loss: 3.193395, Discriminator Loss: 0.265896Step 4000: Generator Loss: 5.071162, Discriminator Loss: 0.409445Step 4500: Generator Loss: 5.213869, Discriminator Loss: 0.203033Step 5000: Generator Loss: 6.087250, Discriminator Loss: 0.350634Step 5500: Generator Loss: 5.467363, Discriminator Loss: 0.424895Step 6000: Generator Loss: 4.910432, Discriminator Loss: 0.196554Step 6500: Generator Loss: 3.230242, Discriminator Loss: 0.268745Step 7000: Generator Loss: 4.777361, Discriminator Loss: 0.676658Step 7500: Generator Loss: 4.165446, Discriminator Loss: 0.150221Step 8000: Generator Loss: 5.681596, Discriminator Loss: 0.108955Step 8500: Generator Loss: 6.023059, Discriminator Loss: 0.114312Step 9000: Generator Loss: 4.660669, Discriminator Loss: 0.182506Step 9500: Generator Loss: 4.492438, Discriminator Loss: 0.411817Step 10000: Generator Loss: 5.906080, Discriminator Loss: 0.088082</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Testing</span></span><br><span class="line"><span class="comment"># Generate images from noise, using the generator network.</span></span><br><span class="line">n = <span class="number">6</span></span><br><span class="line">canvas = np.empty((<span class="number">28</span> * n, <span class="number">28</span> * n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    <span class="comment"># Noise input.</span></span><br><span class="line">    z = np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[n, noise_dim])</span><br><span class="line">    <span class="comment"># Generate image from noise.</span></span><br><span class="line">    g = sess.run(gen_sample, feed_dict=&#123;noise_input: z, is_training:<span class="keyword">False</span>&#125;)</span><br><span class="line">    <span class="comment"># Rescale values to the original [0, 1] (from tanh -&gt; [-1, 1])</span></span><br><span class="line">    g = (g + <span class="number">1.</span>) / <span class="number">2.</span></span><br><span class="line">    <span class="comment"># Reverse colours for better display</span></span><br><span class="line">    g = <span class="number">-1</span> * (g - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># Draw the generated digits</span></span><br><span class="line">        canvas[i * <span class="number">28</span>:(i + <span class="number">1</span>) * <span class="number">28</span>, j * <span class="number">28</span>:(j + <span class="number">1</span>) * <span class="number">28</span>] = g[j].reshape([<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n, n))</span><br><span class="line">plt.imshow(canvas, origin=<span class="string">"upper"</span>, cmap=<span class="string">"gray"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/c614ca83379dc0e98f9d49ff17e80894.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/u013146742/article/details/51986575" target="_blank" rel="noopener">RELU 激活函数及其他相关的函数</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow神经网络之GAN</title>
      <link href="/2018/03/14/DeepLearning/TensorFlow/14Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BGAN/"/>
      <url>/2018/03/14/DeepLearning/TensorFlow/14Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BGAN/</url>
      <content type="html"><![CDATA[<h3 id="生成对抗网络简介"><a href="#生成对抗网络简介" class="headerlink" title="生成对抗网络简介"></a>生成对抗网络简介</h3><p>生成对抗网络（<code>GAN</code>）启发自博弈论中的二人零和博弈（two-player game），<strong>类似于周伯通的绝学——“左右互搏”</strong>。<code>GAN</code> 模型中的两位博弈方分别由生成式模型（<code>generative model</code>）和判别式模型（<code>discriminative model</code>）充当。生成模型 <code>G</code> 捕捉样本数据的分布，用服从某一分布（均匀分布，高斯分布等）的噪声 <code>z</code> 生成一个类似真实训练数据的样本，追求效果是越像真实样本越好；判别模型 <code>D</code> 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，<code>D</code> 输出大概率，否则，<code>D</code> 输出小概率。可以做如下类比：生成网络 <code>G</code> 好比假币制造团伙，专门制造假币，判别网络 <code>D</code> 好比警察，专门检测使用的货币是真币还是假币，<code>G</code> 的目标是想方设法生成和真币一样的货币，使得 <code>D</code> 判别不出来，<code>D</code> 的目标是想方设法检测出来 <code>G</code> 生成的假币。随着训练时间的增加，判别模型与生成模型的能力都相应的提升！</p><p>具体生成网络的示意图如下所示：<br><img src="http://xukeqiniu.xukeai.cn/bbafdb123a7be9bb43f06bdee819dd86.png" alt="生成对抗网络结构示意图"></p><a id="more"></a><h3 id="Tensorflow生成对抗网络实现"><a href="#Tensorflow生成对抗网络实现" class="headerlink" title="Tensorflow生成对抗网络实现"></a>Tensorflow生成对抗网络实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, absolute_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入mnist数据集</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training Params</span></span><br><span class="line">num_steps = <span class="number">70000</span> <span class="comment">#总迭代次数</span></span><br><span class="line">batch_size = <span class="number">128</span>  <span class="comment"># 批量大小</span></span><br><span class="line">learning_rate = <span class="number">0.0002</span> <span class="comment">#学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Params</span></span><br><span class="line">image_dim = <span class="number">784</span> <span class="comment"># 28*28 pixels，生成器的输出层节点数，也是判别器的输入</span></span><br><span class="line">gen_hidden_dim = <span class="number">256</span> <span class="comment"># 生成器隐藏层节点数</span></span><br><span class="line">disc_hidden_dim = <span class="number">256</span> <span class="comment"># 判别器隐藏层节点数</span></span><br><span class="line">noise_dim = <span class="number">100</span> <span class="comment"># Noise data points 生成器输入节点数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Xavier 初始化方式（更适合有ReLU的网络训练）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glorot_init</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.random_normal(shape=shape, stddev=<span class="number">1.</span> / tf.sqrt(shape[<span class="number">0</span>] / <span class="number">2.</span>))</span><br></pre></td></tr></table></figure><p>Xavier 初始化方式方差：</p><p><img src="http://xukeqiniu.xukeai.cn/e6d9383d4eb0fac2f7d1183a592deceb.png" alt=""></p><p>这里的参数是标准差。</p><h4 id="设置每一层的权重与偏置"><a href="#设置每一层的权重与偏置" class="headerlink" title="设置每一层的权重与偏置"></a>设置每一层的权重与偏置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置每一层的权重（Xavier初始化）与偏置（初始化为零）</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'gen_hidden1'</span>: tf.Variable(glorot_init([noise_dim, gen_hidden_dim])),<span class="comment">#（100 - 256）</span></span><br><span class="line">    <span class="string">'gen_out'</span>: tf.Variable(glorot_init([gen_hidden_dim, image_dim])), <span class="comment">#（256 - 784）</span></span><br><span class="line">    <span class="string">'disc_hidden1'</span>: tf.Variable(glorot_init([image_dim, disc_hidden_dim])),<span class="comment">#（784 - 256）</span></span><br><span class="line">    <span class="string">'disc_out'</span>: tf.Variable(glorot_init([disc_hidden_dim, <span class="number">1</span>])),<span class="comment">#（256 - 1）</span></span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'gen_hidden1'</span>: tf.Variable(tf.zeros([gen_hidden_dim])),</span><br><span class="line">    <span class="string">'gen_out'</span>: tf.Variable(tf.zeros([image_dim])),</span><br><span class="line">    <span class="string">'disc_hidden1'</span>: tf.Variable(tf.zeros([disc_hidden_dim])),</span><br><span class="line">    <span class="string">'disc_out'</span>: tf.Variable(tf.zeros([<span class="number">1</span>])),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="定义生成对抗网络"><a href="#定义生成对抗网络" class="headerlink" title="定义生成对抗网络"></a>定义生成对抗网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义生成器函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 输入x是1x100的矩阵，weights['gen_hidden1']是100x256的矩阵，矩阵相乘结果是1x256的矩阵，生成器隐藏层含256个节点</span></span><br><span class="line">    hidden_layer = tf.matmul(x, weights[<span class="string">'gen_hidden1'</span>])</span><br><span class="line">    <span class="comment"># biases['gen_hidden1']是1x256的矩阵，生成器隐藏层含256个节点</span></span><br><span class="line">    hidden_layer = tf.add(hidden_layer, biases[<span class="string">'gen_hidden1'</span>])</span><br><span class="line">    <span class="comment"># 激活函数 relu</span></span><br><span class="line">    hidden_layer = tf.nn.relu(hidden_layer)</span><br><span class="line">    <span class="comment"># hidden_layer是1x256的矩阵，weights['gen_out']是256x784的矩阵，矩阵相乘结果是1x784的矩阵，生成器输出层含784个节点</span></span><br><span class="line">    out_layer = tf.matmul(hidden_layer, weights[<span class="string">'gen_out'</span>])</span><br><span class="line">    <span class="comment"># biases['gen_out']是1x784的矩阵，生成器输出层含784个节点</span></span><br><span class="line">    out_layer = tf.add(out_layer, biases[<span class="string">'gen_out'</span>])</span><br><span class="line">    <span class="comment"># 激活函数 sigmoid</span></span><br><span class="line">    out_layer = tf.nn.sigmoid(out_layer)</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义判别器函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 输入x是生成器生成的1x784的矩阵（生成的图片），weights['disc_hidden1']是784x256的矩阵，矩阵相乘结果是1x256的矩阵，判别器隐藏层含256个节点</span></span><br><span class="line">    hidden_layer = tf.matmul(x, weights[<span class="string">'disc_hidden1'</span>])</span><br><span class="line">    <span class="comment"># biases['disc_hidden1']是1x256的矩阵，生成器隐藏层含256个节点</span></span><br><span class="line">    hidden_layer = tf.add(hidden_layer, biases[<span class="string">'disc_hidden1'</span>])</span><br><span class="line">    <span class="comment"># 激活函数 relu</span></span><br><span class="line">    hidden_layer = tf.nn.relu(hidden_layer)</span><br><span class="line">    <span class="comment"># hidden_layer是1x256的矩阵，weights['disc_out']是256x1的矩阵，矩阵相乘结果是一个数，判别器输出层含1个节点</span></span><br><span class="line">    out_layer = tf.matmul(hidden_layer, weights[<span class="string">'disc_out'</span>])</span><br><span class="line">    <span class="comment"># biases['disc_out']是一个数，判别器输出层含1个节点</span></span><br><span class="line">    out_layer = tf.add(out_layer, biases[<span class="string">'disc_out'</span>])</span><br><span class="line">    <span class="comment"># 激活函数 sigmoid</span></span><br><span class="line">    out_layer = tf.nn.sigmoid(out_layer)</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建网络</span></span><br><span class="line"><span class="comment"># 网络输入</span></span><br><span class="line">gen_input = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, noise_dim], name=<span class="string">'input_noise'</span>) <span class="comment"># 生成器 输入噪点 batch*100，none是一个空值，后面赋值batch_size</span></span><br><span class="line">disc_input = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, image_dim], name=<span class="string">'disc_input'</span>) <span class="comment"># 判别器 输入真实图像 batch*784</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建生成器（generator）</span></span><br><span class="line">gen_sample = generator(gen_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建两个判别器（一个是真实图像输入，一个是生成图像）</span></span><br><span class="line">disc_real = discriminator(disc_input) <span class="comment"># 真实图像</span></span><br><span class="line">disc_fake = discriminator(gen_sample) <span class="comment"># 通过生成器生成的图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line"><span class="comment"># 关于GAN的理论推导，可参见 [^1]</span></span><br><span class="line">gen_loss = -tf.reduce_mean(tf.log(disc_fake)) <span class="comment"># 生成器损失函数</span></span><br><span class="line">disc_loss = -tf.reduce_mean(tf.log(disc_real) + tf.log(<span class="number">1.</span> - disc_fake)) <span class="comment"># 判别器损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建优化器（采用Adam方法），可参见 [^2]</span></span><br><span class="line">optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training Variables for each optimizer</span></span><br><span class="line"><span class="comment"># By default in TensorFlow, all variables are updated by each optimizer, so we</span></span><br><span class="line"><span class="comment"># need to precise for each one of them the specific variables to update.</span></span><br><span class="line"><span class="comment"># 生成网络的变量</span></span><br><span class="line">gen_vars = [weights[<span class="string">'gen_hidden1'</span>], weights[<span class="string">'gen_out'</span>],</span><br><span class="line">            biases[<span class="string">'gen_hidden1'</span>], biases[<span class="string">'gen_out'</span>]]</span><br><span class="line"><span class="comment"># 判别网络的变量</span></span><br><span class="line">disc_vars = [weights[<span class="string">'disc_hidden1'</span>], weights[<span class="string">'disc_out'</span>],</span><br><span class="line">            biases[<span class="string">'disc_hidden1'</span>], biases[<span class="string">'disc_out'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练操作</span></span><br><span class="line">train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)</span><br><span class="line">train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 变量全局初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><p>GAN的网络结构类似于多层感知机：</p><p><img src="http://xukeqiniu.xukeai.cn/0e39fd772bfa170145f493cacadab129.png" alt=""></p><h4 id="训练生成对抗网络"><a href="#训练生成对抗网络" class="headerlink" title="训练生成对抗网络"></a>训练生成对抗网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="comment"># 创建一个会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_steps+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 准备数据</span></span><br><span class="line">    <span class="comment"># 拿到下一批次的 MNIST 数据 (仅需要图像, 不需要标签)</span></span><br><span class="line">    batch_x, _ = mnist.train.next_batch(batch_size) <span class="comment"># 判别器输入 真实图像，batch_*784</span></span><br><span class="line">    <span class="comment"># 给生成器生成噪点数据</span></span><br><span class="line">    z = np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[batch_size, noise_dim]) <span class="comment"># 生成器输入 噪声，batch*100</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    feed_dict = &#123;disc_input: batch_x, gen_input: z&#125; <span class="comment">#给placeholder填入值</span></span><br><span class="line">    _, _, gl, dl = sess.run([train_gen, train_disc, gen_loss, disc_loss],</span><br><span class="line">                            feed_dict=feed_dict)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">'Step %i: Generator Loss: %f, Discriminator Loss: %f'</span> % (i, gl, dl))</span><br></pre></td></tr></table></figure><hr><pre><code>Step 1: Generator Loss: 0.223592, Discriminator Loss: 2.090910Step 2000: Generator Loss: 4.678916, Discriminator Loss: 0.041115Step 4000: Generator Loss: 3.605874, Discriminator Loss: 0.068698Step 6000: Generator Loss: 3.845584, Discriminator Loss: 0.190420Step 8000: Generator Loss: 4.470613, Discriminator Loss: 0.117488Step 10000: Generator Loss: 3.813103, Discriminator Loss: 0.146255Step 12000: Generator Loss: 2.991248, Discriminator Loss: 0.392258Step 14000: Generator Loss: 3.769275, Discriminator Loss: 0.153639Step 16000: Generator Loss: 4.366917, Discriminator Loss: 0.206618Step 18000: Generator Loss: 4.052875, Discriminator Loss: 0.225112Step 20000: Generator Loss: 3.574747, Discriminator Loss: 0.362798Step 22000: Generator Loss: 3.760236, Discriminator Loss: 0.188211Step 24000: Generator Loss: 3.055995, Discriminator Loss: 0.354645Step 26000: Generator Loss: 3.619049, Discriminator Loss: 0.211489Step 28000: Generator Loss: 3.523777, Discriminator Loss: 0.273607Step 30000: Generator Loss: 3.889854, Discriminator Loss: 0.286803Step 32000: Generator Loss: 3.106094, Discriminator Loss: 0.298111Step 34000: Generator Loss: 3.548391, Discriminator Loss: 0.343262Step 36000: Generator Loss: 3.081174, Discriminator Loss: 0.332788Step 38000: Generator Loss: 2.946176, Discriminator Loss: 0.335102Step 40000: Generator Loss: 3.078653, Discriminator Loss: 0.465524Step 42000: Generator Loss: 2.601799, Discriminator Loss: 0.409574Step 44000: Generator Loss: 3.168177, Discriminator Loss: 0.325075Step 46000: Generator Loss: 2.601811, Discriminator Loss: 0.428143Step 48000: Generator Loss: 2.853810, Discriminator Loss: 0.403768Step 50000: Generator Loss: 2.690175, Discriminator Loss: 0.483180Step 52000: Generator Loss: 3.278867, Discriminator Loss: 0.375016Step 54000: Generator Loss: 2.869437, Discriminator Loss: 0.477840Step 56000: Generator Loss: 2.561056, Discriminator Loss: 0.449300Step 58000: Generator Loss: 2.814199, Discriminator Loss: 0.484522Step 60000: Generator Loss: 2.469474, Discriminator Loss: 0.428359Step 62000: Generator Loss: 2.721684, Discriminator Loss: 0.494090Step 64000: Generator Loss: 2.491284, Discriminator Loss: 0.654795Step 66000: Generator Loss: 2.725388, Discriminator Loss: 0.423149Step 68000: Generator Loss: 2.758215, Discriminator Loss: 0.513224Step 70000: Generator Loss: 3.072056, Discriminator Loss: 0.481437</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="comment"># 通过训练出的生成网络输入噪点，生成图像</span></span><br><span class="line">n = <span class="number">6</span></span><br><span class="line">canvas = np.empty((<span class="number">28</span> * n, <span class="number">28</span> * n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    <span class="comment"># 噪点输入</span></span><br><span class="line">    z = np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[n, noise_dim])</span><br><span class="line">    <span class="comment"># 生成图像</span></span><br><span class="line">    g = sess.run(gen_sample, feed_dict=&#123;gen_input: z&#125;)</span><br><span class="line">    <span class="comment"># 颜色反转便于显示</span></span><br><span class="line">    g = <span class="number">-1</span> * (g - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># 绘制生成的手写体数字</span></span><br><span class="line">        canvas[i * <span class="number">28</span>:(i + <span class="number">1</span>) * <span class="number">28</span>, j * <span class="number">28</span>:(j + <span class="number">1</span>) * <span class="number">28</span>] = g[j].reshape([<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n, n))</span><br><span class="line">plt.imshow(canvas, origin=<span class="string">"upper"</span>, cmap=<span class="string">"gray"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/853bf244485839cf44f495ab5afc1f91.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://www.jiqizhixin.com/articles/2017-10-1-1" target="_blank" rel="noopener">机器之心GitHub项目：GAN完整理论推导与实现，Perfect！</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）</a></p><p>[3] <a href="https://www.cnblogs.com/Charles-Wan/p/6238033.html" target="_blank" rel="noopener">不要怂，就是GAN (生成式对抗网络) （一）： GAN 简介</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow神经网络之LSTM</title>
      <link href="/2018/03/13/DeepLearning/TensorFlow/13Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BLSTM/"/>
      <url>/2018/03/13/DeepLearning/TensorFlow/13Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BLSTM/</url>
      <content type="html"><![CDATA[<h3 id="LSTM-简介"><a href="#LSTM-简介" class="headerlink" title="LSTM 简介"></a>LSTM 简介</h3><h4 id="公式-LSTM"><a href="#公式-LSTM" class="headerlink" title="公式 LSTM"></a>公式 LSTM</h4><p><code>LSTM</code>作为门控循环神经网络因此我们从门控单元切入理解。主要包括：</p><ul><li>输入门：It</li><li>遗忘门：Ft</li><li>输出门：Ot</li><li>候选细胞：~Ct</li><li>细胞：Ct</li><li>隐含状态：Ht</li></ul><p>假设隐含状态长度为h，数据Xt是一个样本数为n、特征向量维度为x的批量数据，其计算如下所示（W和b表示权重和偏置）：</p><p><img src="http://xukeqiniu.xukeai.cn/44981d27690d8e9d4c4878f34409dfe0.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/bd180c0fdffc0557b48e84d99b28fe56.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/a57ff99931e6bd62068d381e593a14a7.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/6a0256f97e28c3b099a63391cc97aae7.png" alt=""></p><p>最后的输出其实只有两个，一个是输出，一个是状态，输出就是Ht，而状态为(Ct,Ht)，其他都是中间计算过程。[^2]</p><p><img src="http://xukeqiniu.xukeai.cn/2244cf361736f4ddd46ccbcec6baa459.png" alt=""></p><a id="more"></a><h4 id="图示-LSTM"><a href="#图示-LSTM" class="headerlink" title="图示 LSTM"></a>图示 LSTM</h4><p><img src="http://xukeqiniu.xukeai.cn/2572534e9bc1dff727d3485cf8f958e4.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/83b5c59a84db028eddf567d68b5344bb.png" alt=""></p><ul><li>遗忘门</li></ul><p><img src="http://xukeqiniu.xukeai.cn/36ae3aec84e5e94a0a30a7534840af22.png" alt=""></p><ul><li>输入门</li></ul><p><img src="http://xukeqiniu.xukeai.cn/1d3b7ab425df98f2f564bb951c9c2878.png" alt=""></p><ul><li>当前状态</li></ul><p><img src="http://xukeqiniu.xukeai.cn/d9a47f577060d86970701838d18779e7.png" alt=""></p><ul><li>输出层</li></ul><p><img src="http://xukeqiniu.xukeai.cn/199537faf0d0cbbd0771a4e9013d180a.png" alt=""></p><h4 id="Tensorflow-LSTM"><a href="#Tensorflow-LSTM" class="headerlink" title="Tensorflow LSTM"></a>Tensorflow LSTM</h4><p>tensorflow 提供了LSTM 实现的一个 basic 版本，不包含 LSTM 的一些高级扩展，同时也提供了一个标准接口，其中包含了 LSTM 的扩展。分别为：tf.nn.rnn_cell.BasicLSTMCell()，tf.nn.rnn_cell.LSTMCell()，我们这里实现一个基本版本。[^1]</p><h3 id="Tensorflow-实现-LSTM"><a href="#Tensorflow-实现-LSTM" class="headerlink" title="Tensorflow 实现 LSTM"></a>Tensorflow 实现 LSTM</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> rnn</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练参数</span></span><br><span class="line">learning_rate = <span class="number">0.001</span> <span class="comment"># 学习率</span></span><br><span class="line">training_steps = <span class="number">10000</span> <span class="comment"># 总迭代次数</span></span><br><span class="line">batch_size = <span class="number">128</span> <span class="comment"># 批量大小</span></span><br><span class="line">display_step = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络参数</span></span><br><span class="line">num_input = <span class="number">28</span> <span class="comment"># MNIST数据集图片: 28*28</span></span><br><span class="line">timesteps = <span class="number">28</span> <span class="comment"># timesteps</span></span><br><span class="line">num_hidden = <span class="number">128</span> <span class="comment"># 隐藏层神经元数</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># MNIST 数据集类别数 (0-9 digits)</span></span><br></pre></td></tr></table></figure><h4 id="构建-LSTM-网络"><a href="#构建-LSTM-网络" class="headerlink" title="构建 LSTM 网络"></a>构建 LSTM 网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义输入</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, timesteps, num_input])</span><br><span class="line">Y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, num_classes])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义权重和偏置</span></span><br><span class="line"><span class="comment"># weights矩阵[128, 10]</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([num_hidden, num_classes]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([num_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义LSTM网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LSTM</span><span class="params">(x, weights, biases)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prepare data shape to match `rnn` function requirements</span></span><br><span class="line">    <span class="comment"># 输入数据x的shape: (batch_size, timesteps, n_input)</span></span><br><span class="line">    <span class="comment"># 需要的shape: 按 timesteps 切片，得到 timesteps 个 (batch_size, n_input)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对x进行切分</span></span><br><span class="line">    <span class="comment"># tf.unstack(value,num=None,axis=0,name='unstack')</span></span><br><span class="line">    <span class="comment"># value：要进行分割的tensor</span></span><br><span class="line">    <span class="comment"># axis：整数，打算进行切分的维度</span></span><br><span class="line">    <span class="comment"># num：整数，axis（打算切分）维度的长度</span></span><br><span class="line">    x = tf.unstack(x, timesteps, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个lstm cell，即上面图示LSTM中的A</span></span><br><span class="line">    <span class="comment"># n_hidden表示神经元的个数，forget_bias就是LSTM们的忘记系数，如果等于1，就是不会忘记任何信息。如果等于0，就都忘记。</span></span><br><span class="line">    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到 lstm cell 输出</span></span><br><span class="line">    <span class="comment"># 输出output和states</span></span><br><span class="line">    <span class="comment"># outputs是一个长度为T的列表，通过outputs[-1]取出最后的输出</span></span><br><span class="line">    <span class="comment"># state是最后的状态</span></span><br><span class="line">    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 线性激活</span></span><br><span class="line">    <span class="comment"># 矩阵乘法</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(outputs[<span class="number">-1</span>], weights[<span class="string">'out'</span>]) + biases[<span class="string">'out'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">logits = LSTM(X, weights, biases)</span><br><span class="line">prediction = tf.nn.softmax(logits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">    logits=logits, labels=Y))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">train_op = optimizer.minimize(loss_op)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评估(with test logits, for dropout to be disabled)</span></span><br><span class="line">correct_pred = tf.equal(tf.argmax(prediction, <span class="number">1</span>), tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化全局变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h4 id="训练-测试"><a href="#训练-测试" class="headerlink" title="训练+测试"></a>训练+测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run the initializer</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, training_steps+<span class="number">1</span>):</span><br><span class="line">        batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">        <span class="comment"># Reshape data to get 28 seq of 28 elements</span></span><br><span class="line">        batch_x = batch_x.reshape((batch_size, timesteps, num_input))</span><br><span class="line">        <span class="comment"># Run optimization op (backprop)</span></span><br><span class="line">        sess.run(train_op, feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">        <span class="keyword">if</span> step % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># Calculate batch loss and accuracy</span></span><br><span class="line">            loss, acc = sess.run([loss_op, accuracy], feed_dict=&#123;X: batch_x,</span><br><span class="line">                                                                 Y: batch_y&#125;)</span><br><span class="line">            print(<span class="string">"Step "</span> + str(step) + <span class="string">", Minibatch Loss= "</span> + \</span><br><span class="line">                  <span class="string">"&#123;:.4f&#125;"</span>.format(loss) + <span class="string">", Training Accuracy= "</span> + \</span><br><span class="line">                  <span class="string">"&#123;:.3f&#125;"</span>.format(acc))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate accuracy for 128 mnist test images</span></span><br><span class="line">    test_len = <span class="number">128</span></span><br><span class="line">    test_data = mnist.test.images[:test_len].reshape((<span class="number">-1</span>, timesteps, num_input))</span><br><span class="line">    test_label = mnist.test.labels[:test_len]</span><br><span class="line">    print(<span class="string">"Testing Accuracy:"</span>, \</span><br><span class="line">        sess.run(accuracy, feed_dict=&#123;X: test_data, Y: test_label&#125;))</span><br></pre></td></tr></table></figure><hr><pre><code>Step 1, Minibatch Loss= 2.8645, Training Accuracy= 0.062Step 200, Minibatch Loss= 2.1180, Training Accuracy= 0.227Step 400, Minibatch Loss= 1.9726, Training Accuracy= 0.344Step 600, Minibatch Loss= 1.7784, Training Accuracy= 0.445Step 800, Minibatch Loss= 1.5500, Training Accuracy= 0.547Step 1000, Minibatch Loss= 1.5882, Training Accuracy= 0.453Step 1200, Minibatch Loss= 1.5326, Training Accuracy= 0.555Step 1400, Minibatch Loss= 1.3682, Training Accuracy= 0.570Step 1600, Minibatch Loss= 1.3374, Training Accuracy= 0.594Step 1800, Minibatch Loss= 1.1551, Training Accuracy= 0.648Step 2000, Minibatch Loss= 1.2116, Training Accuracy= 0.633Step 2200, Minibatch Loss= 1.1292, Training Accuracy= 0.609Step 2400, Minibatch Loss= 1.0862, Training Accuracy= 0.680Step 2600, Minibatch Loss= 1.0501, Training Accuracy= 0.672Step 2800, Minibatch Loss= 1.0487, Training Accuracy= 0.688Step 3000, Minibatch Loss= 1.0223, Training Accuracy= 0.727Step 3200, Minibatch Loss= 1.0418, Training Accuracy= 0.695Step 3400, Minibatch Loss= 0.8273, Training Accuracy= 0.719Step 3600, Minibatch Loss= 0.9088, Training Accuracy= 0.727Step 3800, Minibatch Loss= 0.9243, Training Accuracy= 0.750Step 4000, Minibatch Loss= 0.8085, Training Accuracy= 0.703Step 4200, Minibatch Loss= 0.8466, Training Accuracy= 0.711Step 4400, Minibatch Loss= 0.8973, Training Accuracy= 0.734Step 4600, Minibatch Loss= 0.7647, Training Accuracy= 0.750Step 4800, Minibatch Loss= 0.9088, Training Accuracy= 0.742Step 5000, Minibatch Loss= 0.7906, Training Accuracy= 0.742Step 5200, Minibatch Loss= 0.7275, Training Accuracy= 0.781Step 5400, Minibatch Loss= 0.7488, Training Accuracy= 0.789Step 5600, Minibatch Loss= 0.7517, Training Accuracy= 0.758Step 5800, Minibatch Loss= 0.7778, Training Accuracy= 0.797Step 6000, Minibatch Loss= 0.6736, Training Accuracy= 0.742Step 6200, Minibatch Loss= 0.6552, Training Accuracy= 0.773Step 6400, Minibatch Loss= 0.5746, Training Accuracy= 0.828Step 6600, Minibatch Loss= 0.8102, Training Accuracy= 0.727Step 6800, Minibatch Loss= 0.6669, Training Accuracy= 0.773Step 7000, Minibatch Loss= 0.6524, Training Accuracy= 0.766Step 7200, Minibatch Loss= 0.6481, Training Accuracy= 0.805Step 7400, Minibatch Loss= 0.5743, Training Accuracy= 0.828Step 7600, Minibatch Loss= 0.6983, Training Accuracy= 0.773Step 7800, Minibatch Loss= 0.5552, Training Accuracy= 0.828Step 8000, Minibatch Loss= 0.5728, Training Accuracy= 0.820Step 8200, Minibatch Loss= 0.5587, Training Accuracy= 0.789Step 8400, Minibatch Loss= 0.5205, Training Accuracy= 0.836Step 8600, Minibatch Loss= 0.4266, Training Accuracy= 0.906Step 8800, Minibatch Loss= 0.7197, Training Accuracy= 0.812Step 9000, Minibatch Loss= 0.4216, Training Accuracy= 0.852Step 9200, Minibatch Loss= 0.4448, Training Accuracy= 0.844Step 9400, Minibatch Loss= 0.3577, Training Accuracy= 0.891Step 9600, Minibatch Loss= 0.4034, Training Accuracy= 0.883Step 9800, Minibatch Loss= 0.4747, Training Accuracy= 0.828Step 10000, Minibatch Loss= 0.5763, Training Accuracy= 0.805Optimization Finished!Testing Accuracy: 0.875</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/u012436149/article/details/52887091" target="_blank" rel="noopener">tensorflow学习笔记（六）：LSTM 与 GRU</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/28919765" target="_blank" rel="noopener">学会区分 RNN 的 output 和 state</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorBoard高级篇</title>
      <link href="/2018/03/12/DeepLearning/TensorFlow/12TensorBoard%E9%AB%98%E7%BA%A7%E7%AF%87/"/>
      <url>/2018/03/12/DeepLearning/TensorFlow/12TensorBoard%E9%AB%98%E7%BA%A7%E7%AF%87/</url>
      <content type="html"><![CDATA[<h3 id="多层感知机的Tensorboard可视化"><a href="#多层感知机的Tensorboard可视化" class="headerlink" title="多层感知机的Tensorboard可视化"></a>多层感知机的Tensorboard可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><a id="more"></a><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line">logs_path = <span class="string">'./log/example/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span> <span class="comment"># 1st layer number of features</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span> <span class="comment"># 2nd layer number of features</span></span><br><span class="line">n_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">n_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line"><span class="comment"># mnist data image of shape 28*28=784</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">'InputData'</span>)</span><br><span class="line"><span class="comment"># 0-9 digits recognition =&gt; 10 classes</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>], name=<span class="string">'LabelData'</span>)</span><br></pre></td></tr></table></figure><h4 id="创建多层感知机函数"><a href="#创建多层感知机函数" class="headerlink" title="创建多层感知机函数"></a>创建多层感知机函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multilayer_perceptron</span><span class="params">(x, weights, biases)</span>:</span></span><br><span class="line">    <span class="comment"># Hidden layer with RELU activation</span></span><br><span class="line">    layer_1 = tf.add(tf.matmul(x, weights[<span class="string">'w1'</span>]), biases[<span class="string">'b1'</span>])</span><br><span class="line">    layer_1 = tf.nn.relu(layer_1)</span><br><span class="line">    <span class="comment"># Create a summary to visualize the first layer ReLU activation</span></span><br><span class="line">    tf.summary.histogram(<span class="string">"relu1"</span>, layer_1)</span><br><span class="line">    <span class="comment"># Hidden layer with RELU activation</span></span><br><span class="line">    layer_2 = tf.add(tf.matmul(layer_1, weights[<span class="string">'w2'</span>]), biases[<span class="string">'b2'</span>])</span><br><span class="line">    layer_2 = tf.nn.relu(layer_2)</span><br><span class="line">    <span class="comment"># Create another summary to visualize the second layer ReLU activation</span></span><br><span class="line">    tf.summary.histogram(<span class="string">"relu2"</span>, layer_2)</span><br><span class="line">    <span class="comment"># Output layer</span></span><br><span class="line">    out_layer = tf.add(tf.matmul(layer_2, weights[<span class="string">'w3'</span>]), biases[<span class="string">'b3'</span>])</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Store layers weight &amp; bias</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'w1'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1]), name=<span class="string">'W1'</span>),</span><br><span class="line">    <span class="string">'w2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name=<span class="string">'W2'</span>),</span><br><span class="line">    <span class="string">'w3'</span>: tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name=<span class="string">'W3'</span>)</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'b1'</span>: tf.Variable(tf.random_normal([n_hidden_1]), name=<span class="string">'b1'</span>),</span><br><span class="line">    <span class="string">'b2'</span>: tf.Variable(tf.random_normal([n_hidden_2]), name=<span class="string">'b2'</span>),</span><br><span class="line">    <span class="string">'b3'</span>: tf.Variable(tf.random_normal([n_classes]), name=<span class="string">'b3'</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="创建模型和操作（模型-损失函数-优化-准确率）"><a href="#创建模型和操作（模型-损失函数-优化-准确率）" class="headerlink" title="创建模型和操作（模型+损失函数+优化+准确率）"></a>创建模型和操作（模型+损失函数+优化+准确率）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Encapsulating all ops into scopes, making Tensorboard's Graph</span></span><br><span class="line"><span class="comment"># Visualization more convenient</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Model'</span>):</span><br><span class="line">    <span class="comment"># Build model</span></span><br><span class="line">    pred = multilayer_perceptron(x, weights, biases)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Loss'</span>):</span><br><span class="line">    <span class="comment"># Softmax Cross entropy (cost function)</span></span><br><span class="line">    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'SGD'</span>):</span><br><span class="line">    <span class="comment"># Gradient Descent</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line">    <span class="comment"># Op to calculate every variable gradient</span></span><br><span class="line">    grads = tf.gradients(loss, tf.trainable_variables())</span><br><span class="line">    grads = list(zip(grads, tf.trainable_variables()))</span><br><span class="line">    <span class="comment"># Op to update all variables according to their gradient</span></span><br><span class="line">    apply_grads = optimizer.apply_gradients(grads_and_vars=grads)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Accuracy'</span>):</span><br><span class="line">    <span class="comment"># Accuracy</span></span><br><span class="line">    acc = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    acc = tf.reduce_mean(tf.cast(acc, tf.float32))</span><br></pre></td></tr></table></figure><h4 id="初始化并合并操作"><a href="#初始化并合并操作" class="headerlink" title="初始化并合并操作"></a>初始化并合并操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a summary to monitor cost tensor</span></span><br><span class="line">tf.summary.scalar(<span class="string">"loss"</span>, loss)</span><br><span class="line"><span class="comment"># Create a summary to monitor accuracy tensor</span></span><br><span class="line">tf.summary.scalar(<span class="string">"accuracy"</span>, acc)</span><br><span class="line"><span class="comment"># Create summaries to visualize weights</span></span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> tf.trainable_variables():</span><br><span class="line">    tf.summary.histogram(var.name, var)</span><br><span class="line"><span class="comment"># Summarize all gradients</span></span><br><span class="line"><span class="keyword">for</span> grad, var <span class="keyword">in</span> grads:</span><br><span class="line">    tf.summary.histogram(var.name + <span class="string">'/gradient'</span>, grad)</span><br><span class="line"><span class="comment"># Merge all summaries into a single op</span></span><br><span class="line">merged_summary_op = tf.summary.merge_all()</span><br></pre></td></tr></table></figure><hr><pre><code>INFO:tensorflow:Summary name W1:0 is illegal; using W1_0 instead.INFO:tensorflow:Summary name W2:0 is illegal; using W2_0 instead.INFO:tensorflow:Summary name W3:0 is illegal; using W3_0 instead.INFO:tensorflow:Summary name b1:0 is illegal; using b1_0 instead.INFO:tensorflow:Summary name b2:0 is illegal; using b2_0 instead.INFO:tensorflow:Summary name b3:0 is illegal; using b3_0 instead.INFO:tensorflow:Summary name W1:0/gradient is illegal; using W1_0/gradient instead.INFO:tensorflow:Summary name W2:0/gradient is illegal; using W2_0/gradient instead.INFO:tensorflow:Summary name W3:0/gradient is illegal; using W3_0/gradient instead.INFO:tensorflow:Summary name b1:0/gradient is illegal; using b1_0/gradient instead.INFO:tensorflow:Summary name b2:0/gradient is illegal; using b2_0/gradient instead.INFO:tensorflow:Summary name b3:0/gradient is illegal; using b3_0/gradient instead.</code></pre><h4 id="训练并保存log"><a href="#训练并保存log" class="headerlink" title="训练并保存log"></a>训练并保存log</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run the initializer</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># op to write logs to Tensorboard</span></span><br><span class="line">    summary_writer = tf.summary.FileWriter(logs_path,</span><br><span class="line">                                            graph=tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop), cost op (to get loss value)</span></span><br><span class="line">            <span class="comment"># and summary nodes</span></span><br><span class="line">            _, c, summary = sess.run([apply_grads, loss, merged_summary_op],</span><br><span class="line">                                     feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># Write logs at every iteration</span></span><br><span class="line">            summary_writer.add_summary(summary, epoch * total_batch + i)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    <span class="comment"># Calculate accuracy</span></span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, acc.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Run the command line:\n"</span> \</span><br><span class="line">          <span class="string">"--&gt; tensorboard --logdir=./log "</span> \</span><br><span class="line">          <span class="string">"\nThen open http://0.0.0.0:6006/ into your web browser"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Epoch: 0001 cost= 82.491150440Epoch: 0002 cost= 11.219711702Epoch: 0003 cost= 6.885841494Epoch: 0004 cost= 4.898687713Epoch: 0005 cost= 3.742709111Epoch: 0006 cost= 2.969850923Epoch: 0007 cost= 2.429568350Epoch: 0008 cost= 2.024799560Epoch: 0009 cost= 1.742192560Epoch: 0010 cost= 1.494883727Epoch: 0011 cost= 1.313867836Epoch: 0012 cost= 1.153405372Epoch: 0013 cost= 1.022956383Epoch: 0014 cost= 0.917282970Epoch: 0015 cost= 0.831443023Epoch: 0016 cost= 0.739466778Epoch: 0017 cost= 0.660427638Epoch: 0018 cost= 0.606233582Epoch: 0019 cost= 0.547995506Epoch: 0020 cost= 0.506534999Epoch: 0021 cost= 0.462353780Epoch: 0022 cost= 0.424939641Epoch: 0023 cost= 0.399291764Epoch: 0024 cost= 0.364750651Epoch: 0025 cost= 0.334185596Optimization Finished!Accuracy: 0.9215Run the command line:--&gt; tensorboard --logdir=./logThen open http://0.0.0.0:6006/ into your web browser</code></pre><h4 id="损失和准确率折线图"><a href="#损失和准确率折线图" class="headerlink" title="损失和准确率折线图"></a>损失和准确率折线图</h4><p><img src="http://xukeqiniu.xukeai.cn/d9addb5e916784c4dc58a5fd3b7d19b2.png" alt="Loss and Accuracy Visualization"></p><h4 id="计算图模型的可视化"><a href="#计算图模型的可视化" class="headerlink" title="计算图模型的可视化"></a>计算图模型的可视化</h4><p><img src="http://xukeqiniu.xukeai.cn/bb45bab3de233fb2ff3611af6cacb160.png" alt="Computation Graph——Model and SGD"><br><img src="http://xukeqiniu.xukeai.cn/b79932f2eb49429dbcd7af11abdc8c1c.png" alt="Computation Graph——Loss and Accuracy "></p><h4 id="权重及其梯度直方图"><a href="#权重及其梯度直方图" class="headerlink" title="权重及其梯度直方图"></a>权重及其梯度直方图</h4><p><img src="http://xukeqiniu.xukeai.cn/58f1c3f448e14df1632d8482b77c4701.png" alt="Weights and Gradients Visualization"></p><h4 id="偏置及其梯度直方图"><a href="#偏置及其梯度直方图" class="headerlink" title="偏置及其梯度直方图"></a>偏置及其梯度直方图</h4><p><img src="http://xukeqiniu.xukeai.cn/9ee31a697512aaa3b8803e17d70eaa1d.png" alt="bias and Gradients Visualization"></p><h4 id="FeatureMap-直方图"><a href="#FeatureMap-直方图" class="headerlink" title="FeatureMap 直方图"></a>FeatureMap 直方图</h4><p><img src="http://xukeqiniu.xukeai.cn/255b33e56d2ab12b375d529f52fcef74.png" alt="FeatureMap Visualization"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[TensorBoard: 图表可视化]<a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html" target="_blank" rel="noopener">http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorBoard基础篇</title>
      <link href="/2018/03/11/DeepLearning/TensorFlow/11TensorBoard%E5%9F%BA%E7%A1%80%E7%AF%87/"/>
      <url>/2018/03/11/DeepLearning/TensorFlow/11TensorBoard%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
      <content type="html"><![CDATA[<h3 id="TensorBoard简介"><a href="#TensorBoard简介" class="headerlink" title="TensorBoard简介"></a>TensorBoard简介</h3><p>TensorBoard是Tensorflow自带的一个强大的可视化工具，也是一个web应用程序套件。在众多机器学习库中，Tensorflow是目前唯一自带可视化工具的库，这也是Tensorflow的一个优点。学会使用TensorBoard，将可以帮助我们构建复杂模型。</p><p>这里需要理解“可视化”的意义。“可视化”也叫做数据可视化。是关于数据之视觉表现形式的研究。这种数据的视觉表现形式被定义为一种以某种概要形式抽提出来的信息，包括相应信息单位的各种属性和变量。例如我们需要可视化算法运行的错误率，那么我们可以取算法每次训练的错误率，绘制成折线图或曲线图，来表达训练过程中错误率的变化。可视化的方法有很多种。但无论哪一种，均是对数据进行摘要(summary)与处理。</p><p>通常使用TensorBoard有三个步骤：</p><ul><li>首先需要在需要可视化的相关部位添加可视化代码，即创建摘要、添加摘要；</li><li>其次运行代码，可以生成了一个或多个事件文件(event files)；</li><li>最后启动TensorBoard的Web服务器。</li></ul><p>完成以上三个步骤，就可以在浏览器中可视化结果，Web服务器将会分析这个事件文件中的内容，并在浏览器中将结果绘制出来。</p><p>如果我们已经拥有了一个事件文件，也可以直接利用TensorBoard查看这个事件文件中的摘要。<br>TensorBoard视图如下所示：<br><img src="http://xukeqiniu.xukeai.cn/graph_vis_animation.gif" alt="TensorBoard示意图"></p><a id="more"></a><h3 id="Logistic回归的Tensorboard可视化"><a href="#Logistic回归的Tensorboard可视化" class="headerlink" title="Logistic回归的Tensorboard可视化"></a>Logistic回归的Tensorboard可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MINST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_epoch = <span class="number">1</span></span><br><span class="line">logs_path = <span class="string">'./log/example/'</span> <span class="comment"># log存放位置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line"><span class="comment"># mnist data image of shape 28*28=784</span></span><br><span class="line"><span class="comment">#（name=''将在Tensorboard中显示）</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">'InputData'</span>) <span class="comment">#输入数据（InputData）</span></span><br><span class="line"><span class="comment"># 0-9 digits recognition =&gt; 10 classes</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>], name=<span class="string">'LabelData'</span>) <span class="comment"># 输出标签（LabelData）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set model weights</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]), name=<span class="string">'Weights'</span>) <span class="comment">#权重（Weights）</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]), name=<span class="string">'Bias'</span>) <span class="comment">#偏置（Bias）</span></span><br></pre></td></tr></table></figure><h4 id="构建模型和操作（模型-损失函数-优化-准确率）"><a href="#构建模型和操作（模型-损失函数-优化-准确率）" class="headerlink" title="构建模型和操作（模型+损失函数+优化+准确率）"></a>构建模型和操作（模型+损失函数+优化+准确率）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Construct model and encapsulating all ops into scopes, making</span></span><br><span class="line"><span class="comment"># Tensorboard's Graph visualization more convenient</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Model'</span>):</span><br><span class="line">    <span class="comment"># Model</span></span><br><span class="line">    pred = tf.nn.softmax(tf.matmul(x, W) + b) <span class="comment"># Softmax</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Loss'</span>):</span><br><span class="line">    <span class="comment"># Minimize error using cross entropy</span></span><br><span class="line">    cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'SGD'</span>):</span><br><span class="line">    <span class="comment"># Gradient Descent</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Accuracy'</span>):</span><br><span class="line">    <span class="comment"># Accuracy</span></span><br><span class="line">    acc = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    acc = tf.reduce_mean(tf.cast(acc, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the variables</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a summary to monitor cost tensor</span></span><br><span class="line">tf.summary.scalar(<span class="string">"loss"</span>, cost)</span><br><span class="line"><span class="comment"># Create a summary to monitor accuracy tensor</span></span><br><span class="line">tf.summary.scalar(<span class="string">"accuracy"</span>, acc)</span><br><span class="line"><span class="comment"># Merge all summaries into a single op</span></span><br><span class="line">merged_summary_op = tf.summary.merge_all()</span><br></pre></td></tr></table></figure><h4 id="训练并保存log"><a href="#训练并保存log" class="headerlink" title="训练并保存log"></a>训练并保存log</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start Training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># op to write logs to Tensorboard</span></span><br><span class="line">    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop), cost op (to get loss value)</span></span><br><span class="line">            <span class="comment"># and summary nodes</span></span><br><span class="line">            _, c, summary = sess.run([optimizer, cost, merged_summary_op],</span><br><span class="line">                                     feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># Write logs at every iteration</span></span><br><span class="line">            summary_writer.add_summary(summary, epoch * total_batch + i)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_epoch == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    <span class="comment"># Calculate accuracy</span></span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, acc.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Run the command line:\n"</span> \</span><br><span class="line">          <span class="string">"--&gt; tensorboard --logdir=./log"</span> \</span><br><span class="line">          <span class="string">"\nThen open http://0.0.0.0:6006/ into your web browser"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Epoch: 0001 cost= 1.183717763Epoch: 0002 cost= 0.665147323Epoch: 0003 cost= 0.552818966Epoch: 0004 cost= 0.498699070Epoch: 0005 cost= 0.465521080Epoch: 0006 cost= 0.442596199Epoch: 0007 cost= 0.425560050Epoch: 0008 cost= 0.412205354Epoch: 0009 cost= 0.401337254Epoch: 0010 cost= 0.392412475Epoch: 0011 cost= 0.384738669Epoch: 0012 cost= 0.378180920Epoch: 0013 cost= 0.372407395Epoch: 0014 cost= 0.367316018Epoch: 0015 cost= 0.362715464Epoch: 0016 cost= 0.358595766Epoch: 0017 cost= 0.354887394Epoch: 0018 cost= 0.351458600Epoch: 0019 cost= 0.348339875Epoch: 0020 cost= 0.345448156Epoch: 0021 cost= 0.342770365Epoch: 0022 cost= 0.340232303Epoch: 0023 cost= 0.337901928Epoch: 0024 cost= 0.335753958Epoch: 0025 cost= 0.333657109Optimization Finished!Accuracy: 0.9136Run the command line:--&gt; tensorboard --logdir=./logThen open http://0.0.0.0:6006/ into your web browser</code></pre><h4 id="损失和准确率的可视化"><a href="#损失和准确率的可视化" class="headerlink" title="损失和准确率的可视化"></a>损失和准确率的可视化</h4><p><img src="http://xukeqiniu.xukeai.cn/dfd03c5ec422bd903841637b67f97910.png" alt="Loss and Accuracy Visualization"></p><h4 id="计算图模型的基本单元"><a href="#计算图模型的基本单元" class="headerlink" title="计算图模型的基本单元"></a>计算图模型的基本单元</h4><p><img src="http://xukeqiniu.xukeai.cn/0d397a70c516c7fe3f7ceeec0511793f.png" alt=""></p><h4 id="计算图模型的可视化"><a href="#计算图模型的可视化" class="headerlink" title="计算图模型的可视化"></a>计算图模型的可视化</h4><p><img src="http://xukeqiniu.xukeai.cn/45577b0533ec74f923dd92bd4d09519b.png" alt="Graph Visualization"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[TensorBoard: 图表可视化]<a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html" target="_blank" rel="noopener">http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow模型的保存与读取</title>
      <link href="/2018/03/10/DeepLearning/TensorFlow/10Tensorflow%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/"/>
      <url>/2018/03/10/DeepLearning/TensorFlow/10Tensorflow%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>首先，我们从一个直观的例子，讲解如何实现Tensorflow模型参数的保存以及保存后模型的读取。<br>然后，我们在之前多层感知机的基础上进行模型的参数保存，以及参数的读取。该项技术可以用于Tensorflow分段训练模型以及对经典模型进行<code>fine tuning</code>（微调）</p><a id="more"></a><h3 id="Tensorflow-模型的保存与读取（直观）"><a href="#Tensorflow-模型的保存与读取（直观）" class="headerlink" title="Tensorflow 模型的保存与读取（直观）"></a>Tensorflow 模型的保存与读取（直观）</h3><h4 id="模型参数存储"><a href="#模型参数存储" class="headerlink" title="模型参数存储"></a>模型参数存储</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成v1与v2变量</span></span><br><span class="line">v1 = tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">2</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line"><span class="comment"># 全局初始化</span></span><br><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># 调用Saver方法（重要）</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"V1:"</span>,sess.run(v1))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"V2:"</span>,sess.run(v2))</span><br><span class="line">    <span class="comment"># 存储Session工作空间</span></span><br><span class="line">    saver_path = saver.save(sess, <span class="string">"./save/model.ckpt"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Model saved in file: "</span>, saver_path)</span><br></pre></td></tr></table></figure><pre><code>V1: [[1.2366687 0.4373563]]V2: [[-0.9465265  -0.7147392  -2.421146  ] [-0.48401725  0.40536404  0.21300188]]Model saved in file:  ./save/model.ckpt</code></pre><p>模型存储的文件格式如下图所示：<br><img src="http://xukeqiniu.xukeai.cn/db94d565e96ee5ba244853e52ca1b675.png" alt="模型存储文件"></p><h4 id="模型参数读取"><a href="#模型参数读取" class="headerlink" title="模型参数读取"></a>模型参数读取</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">v1 = tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">2</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"./save/model.ckpt"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"V1:"</span>,sess.run(v1))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"V2:"</span>,sess.run(v2))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Model restored"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>INFO:tensorflow:Restoring parameters from ./save/model.ckptV1: [[1.2366687 0.4373563]]V2: [[-0.9465265  -0.7147392  -2.421146  ] [-0.48401725  0.40536404  0.21300188]]Model restored</code></pre><h3 id="Tensorflow-模型的保存与读取（多层感知机）"><a href="#Tensorflow-模型的保存与读取（多层感知机）" class="headerlink" title="Tensorflow 模型的保存与读取（多层感知机）"></a>Tensorflow 模型的保存与读取（多层感知机）</h3><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import MINST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="创建多层感知机模型"><a href="#创建多层感知机模型" class="headerlink" title="创建多层感知机模型"></a>创建多层感知机模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练参数设置</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line">model_path = <span class="string">"./save/model.ckpt"</span> <span class="comment">#模型存储路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络参数设置</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span> <span class="comment"># 1st layer number of features</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span> <span class="comment"># 2nd layer number of features</span></span><br><span class="line">n_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">n_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph input</span></span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_input])</span><br><span class="line">y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_classes])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multilayer_perceptron</span><span class="params">(x, weights, biases)</span>:</span></span><br><span class="line">    <span class="comment"># Hidden layer with RELU activation</span></span><br><span class="line">    layer_1 = tf.add(tf.matmul(x, weights[<span class="string">'h1'</span>]), biases[<span class="string">'b1'</span>])</span><br><span class="line">    layer_1 = tf.nn.relu(layer_1)</span><br><span class="line">    <span class="comment"># Hidden layer with RELU activation</span></span><br><span class="line">    layer_2 = tf.add(tf.matmul(layer_1, weights[<span class="string">'h2'</span>]), biases[<span class="string">'b2'</span>])</span><br><span class="line">    layer_2 = tf.nn.relu(layer_2)</span><br><span class="line">    <span class="comment"># Output layer with linear activation</span></span><br><span class="line">    out_layer = tf.matmul(layer_2, weights[<span class="string">'out'</span>]) + biases[<span class="string">'out'</span>]</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Store layers weight &amp; bias</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'h1'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1])),</span><br><span class="line">    <span class="string">'h2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_hidden_2, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'b1'</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    <span class="string">'b2'</span>: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct model</span></span><br><span class="line">pred = multilayer_perceptron(x, weights, biases)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss and optimizer</span></span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the variables</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h4 id="调用Saver方法"><a href="#调用Saver方法" class="headerlink" title="调用Saver方法"></a>调用Saver方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 'Saver' 操作用于保存与读取所有的变量</span></span><br><span class="line">saver = tf.train.Saver()</span><br></pre></td></tr></table></figure><h4 id="第一次训练（训练完成保存参数）"><a href="#第一次训练（训练完成保存参数）" class="headerlink" title="第一次训练（训练完成保存参数）"></a>第一次训练（训练完成保存参数）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Running first session</span></span><br><span class="line">print(<span class="string">"Starting 1st session..."</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Initialize variables</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle（迭代三次）</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_x,</span><br><span class="line">                                                          y: batch_y&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, \</span><br><span class="line">                <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">    print(<span class="string">"First Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Calculate accuracy</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型参数到硬盘上</span></span><br><span class="line">    save_path = saver.save(sess, model_path)</span><br><span class="line">    print(<span class="string">"Model saved in file: %s"</span> % save_path)</span><br></pre></td></tr></table></figure><hr><pre><code>Starting 1st session...Epoch: 0001 cost= 172.468734065Epoch: 0002 cost= 43.036823805Epoch: 0003 cost= 26.978232009First Optimization Finished!Accuracy: 0.9084Model saved in file: ./save/model.ckpt</code></pre><h4 id="第二次训练（加载第一次参数）"><a href="#第二次训练（加载第一次参数）" class="headerlink" title="第二次训练（加载第一次参数）"></a>第二次训练（加载第一次参数）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Running a new session</span></span><br><span class="line">print(<span class="string">"Starting 2nd session..."</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Initialize variables</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Restore model weights from previously saved model</span></span><br><span class="line">    load_path = saver.restore(sess, model_path)</span><br><span class="line">    print(<span class="string">"Model restored from file: %s"</span> % save_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Resume training</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">7</span>):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_x,</span><br><span class="line">                                                          y: batch_y&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, \</span><br><span class="line">                <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">    print(<span class="string">"Second Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Calculate accuracy</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy.eval(</span><br><span class="line">        &#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure><hr><pre><code>Starting 2nd session...INFO:tensorflow:Restoring parameters from ./save/model.ckptModel restored from file: ./save/model.ckptEpoch: 0001 cost= 18.712020244Epoch: 0002 cost= 13.624892972Epoch: 0003 cost= 10.156988694Epoch: 0004 cost= 7.652410518Epoch: 0005 cost= 5.658380691Epoch: 0006 cost= 4.276506317Epoch: 0007 cost= 3.249772967Second Optimization Finished!Accuracy: 0.9381</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">TensorFlow-Examples</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow神经网络之卷积神经网络</title>
      <link href="/2018/03/09/DeepLearning/TensorFlow/09Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2018/03/09/DeepLearning/TensorFlow/09Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<h3 id="Tensorflow卷积神经网络实现"><a href="#Tensorflow卷积神经网络实现" class="headerlink" title="Tensorflow卷积神经网络实现"></a>Tensorflow卷积神经网络实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, absolute_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><a id="more"></a><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">num_steps = <span class="number">2000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">num_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br><span class="line">dropout = <span class="number">0.25</span> <span class="comment"># Dropout, probability to drop a unit</span></span><br></pre></td></tr></table></figure><h4 id="定义CNN模型"><a href="#定义CNN模型" class="headerlink" title="定义CNN模型"></a>定义CNN模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the neural network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_net</span><span class="params">(x_dict, n_classes, dropout, reuse, is_training)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define a scope for reusing the variables</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'ConvNet'</span>, reuse=reuse):</span><br><span class="line">        <span class="comment"># TF Estimator input is a dict, in case of multiple inputs</span></span><br><span class="line">        x = x_dict[<span class="string">'images'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># MNIST data input is a 1-D vector of 784 features (28*28 pixels)</span></span><br><span class="line">        <span class="comment"># Reshape to match picture format [Height x Width x Channel]</span></span><br><span class="line">        <span class="comment"># Tensor input become 4-D: [Batch Size, Height, Width, Channel]</span></span><br><span class="line">        x = tf.reshape(x, shape=[<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convolution Layer with 32 filters and a kernel size of 5</span></span><br><span class="line">        conv1 = tf.layers.conv2d(x, <span class="number">32</span>, <span class="number">5</span>, activation=tf.nn.relu)</span><br><span class="line">        <span class="comment"># Max Pooling (down-sampling) with strides of 2 and kernel size of 2</span></span><br><span class="line">        conv1 = tf.layers.max_pooling2d(conv1, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convolution Layer with 64 filters and a kernel size of 3</span></span><br><span class="line">        conv2 = tf.layers.conv2d(conv1, <span class="number">64</span>, <span class="number">3</span>, activation=tf.nn.relu)</span><br><span class="line">        <span class="comment"># Max Pooling (down-sampling) with strides of 2 and kernel size of 2</span></span><br><span class="line">        conv2 = tf.layers.max_pooling2d(conv2, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Flatten the data to a 1-D vector for the fully connected layer</span></span><br><span class="line">        fc1 = tf.contrib.layers.flatten(conv2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Fully connected layer (in tf contrib folder for now)</span></span><br><span class="line">        fc1 = tf.layers.dense(fc1, <span class="number">1024</span>)</span><br><span class="line">        <span class="comment"># Apply Dropout (if is_training is False, dropout is not applied)</span></span><br><span class="line">        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Output layer, class prediction</span></span><br><span class="line">        out = tf.layers.dense(fc1, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><blockquote><p><strong>补充：tf.nn，tf.layers， tf.contrib模块区别</strong> [^1]<br>tf.nn，tf.layers， tf.contrib模块有很多功能是重复的，尤其是卷积操作，在使用的时候，我们可以根据需要现在不同的模块。但有些时候可以一起混用。<br>下面是对三个模块的简述：</p><ul><li>tf.nn ：提供神经网络相关操作的支持，包括卷积操作（conv）、池化操作（pooling）、归一化、loss、分类操作、embedding、RNN、Evaluation。</li><li>tf.layers：主要提供的高层的神经网络，主要和卷积相关的，个人感觉是对tf.nn的进一步封装，tf.nn会更底层一些。</li><li>tf.contrib：tf.contrib.layers提供够将计算图中的  网络层、正则化、摘要操作、是构建计算图的高级操作，但是tf.contrib包含不稳定和实验代码，有可能以后API会改变。<br>以上三个模块的封装程度是逐个递进的。</li></ul><p><strong>补充：TensorFlow layers模块</strong>  [^2]</p><h5 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h5><p>Convolution 有多个方法，如 conv1d()、conv2d()、conv3d()，分别代表一维、二维、三维卷积，另外还有 conv2d_transpose()、conv3d_transpose()，分别代表二维和三维反卷积，还有 separable_conv2d() 方法代表二维深度可分离卷积。它们定义在 tensorflow/python/layers/convolutional.py 中，其用法都是类似的，在这里以 conv2d() 方法为例进行说明。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt; conv2d(</span><br><span class="line">&gt;     inputs,</span><br><span class="line">&gt;     filters,</span><br><span class="line">&gt;     kernel_size,</span><br><span class="line">&gt;     strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">&gt;     padding=<span class="string">'valid'</span>,</span><br><span class="line">&gt;     data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">&gt;     dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">&gt;     activation=<span class="keyword">None</span>,</span><br><span class="line">&gt;     use_bias=<span class="keyword">True</span>,</span><br><span class="line">&gt;     kernel_initializer=<span class="keyword">None</span>,</span><br><span class="line">&gt;     bias_initializer=tf.zeros_initializer(),</span><br><span class="line">&gt;     kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;     bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;     activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;     kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">&gt;     bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">&gt;     trainable=<span class="keyword">True</span>,</span><br><span class="line">&gt;     name=<span class="keyword">None</span>,</span><br><span class="line">&gt;     reuse=<span class="keyword">None</span></span><br><span class="line">&gt; )</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p><p>参数说明如下：</p><ul><li>inputs：必需，即需要进行操作的输入数据。</li><li>filters：必需，是一个数字，代表了输出通道的个数，即 output_channels。</li><li>kernel_size：必需，卷积核大小，必须是一个数字（高和宽都是此数字）或者长度为 2 的列表（分别代表高、宽）。</li><li>strides：可选，默认为 (1, 1)，卷积步长，必须是一个数字（高和宽都是此数字）或者长度为 2 的列表（分别代表高、宽）。</li><li>padding：可选，默认为 valid，padding 的模式，有 valid 和 same 两种，大小写不区分。</li><li>data_format：可选，默认 channels_last，分为 channels_last 和 channels_first 两种模式，代表了输入数据的维度类型，如果是 channels_last，那么输入数据的 shape 为 (batch, height, width, channels)，如果是 channels_first，那么输入数据的 shape 为 (batch, channels, height, width)。</li><li>dilation_rate：可选，默认为 (1, 1)，卷积的扩张率，如当扩张率为 2 时，卷积核内部就会有边距，3×3 的卷积核就会变成 5×5。</li><li>activation：可选，默认为 None，如果为 None 则是线性激活。</li><li>use_bias：可选，默认为 True，是否使用偏置。</li><li>kernel_initializer：可选，默认为 None，即权重的初始化方法，如果为 None，则使用默认的 Xavier 初始化方法。</li><li>bias_initializer：可选，默认为零值初始化，即偏置的初始化方法。</li><li>kernel_regularizer：可选，默认为 None，施加在权重上的正则项。</li><li>bias_regularizer：可选，默认为 None，施加在偏置上的正则项。</li><li>activity_regularizer：可选，默认为 None，施加在输出上的正则项。</li><li>kernel_constraint，可选，默认为 None，施加在权重上的约束项。</li><li>bias_constraint，可选，默认为 None，施加在偏置上的约束项。</li><li>trainable：可选，默认为 True，布尔类型，如果为 True，则将变量添加到 GraphKeys.TRAINABLE_VARIABLES 中。</li><li>name：可选，默认为 None，卷积层的名称。</li><li>reuse：可选，默认为 None，布尔类型，如果为 True，那么如果 name 相同时，会重复利用。</li><li>返回值： 卷积后的 Tensor。</li></ul><p><strong>注意，这里只需要给出输入数据，输出通道数，卷积核大小即可。</strong></p><h5 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h5><p>layers 模块提供了多个池化方法，这几个池化方法都是类似的，包括 max_pooling1d()、max_pooling2d()、max_pooling3d()、average_pooling1d()、average_pooling2d()、average_pooling3d()，分别代表一维二维三维最大和平均池化方法，它们都定义在 tensorflow/python/layers/pooling.py 中，这里以 &gt; max_pooling2d() 方法为例进行介绍。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; max_pooling2d(</span><br><span class="line">&gt;     inputs,</span><br><span class="line">&gt;     pool_size,</span><br><span class="line">&gt;     strides,</span><br><span class="line">&gt;     padding=<span class="string">'valid'</span>,</span><br><span class="line">&gt;     data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">&gt;     name=<span class="keyword">None</span></span><br><span class="line">&gt; )</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p><p>参数说明如下：</p><ul><li>inputs: 必需，即需要池化的输入对象，必须是 4 维的。</li><li>pool_size：必需，池化窗口大小，必须是一个数字（高和宽都是此数字）或者长度为 2 的列表（分别代表高、宽）。</li><li>strides：必需，池化步长，必须是一个数字（高和宽都是此数字）或者长度为 2 的列表（分别代表高、宽）。</li><li>padding：可选，默认 valid，padding 的方法，valid 或者 same，大小写不区分。</li><li>data_format：可选，默认 channels_last，分为 channels_last 和 channels_first 两种模式，代表了输入数据的维度类型，如果是 channels_last，那么输入数据的 shape 为 (batch, height, width, channels)，如果是 channels_first，那么输入数据的 shape 为 (batch, channels, height, width)。</li><li>name：可选，默认 None，池化层的名称。</li><li>返回值： 经过池化处理后的 Tensor。</li></ul><h5 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h5><p>dropout 是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃，可以用来防止过拟合，layers 模块中提供了 dropout() 方法来实现这一操作，定义在 tensorflow/python/layers/core.py。下面我们来说明一下它的用法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; dropout(</span><br><span class="line">&gt;     inputs,</span><br><span class="line">&gt;     rate=<span class="number">0.5</span>,</span><br><span class="line">&gt;     noise_shape=<span class="keyword">None</span>,</span><br><span class="line">&gt;     seed=<span class="keyword">None</span>,</span><br><span class="line">&gt;     training=<span class="keyword">False</span>,</span><br><span class="line">&gt;     name=<span class="keyword">None</span></span><br><span class="line">&gt; )</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p><p>参数说明如下：</p><ul><li>inputs：必须，即输入数据。</li><li>rate：可选，默认为 0.5，即 dropout rate，如设置为 0.1，则意味着会丢弃 10% 的神经元。</li><li>noise_shape：可选，默认为 None，int32 类型的一维 Tensor，它代表了 dropout mask 的 shape，dropout mask 会与 inputs 相乘对 inputs 做转换，例如 inputs 的 shape 为 (batch_size, timesteps, features)，但我们想要 droput mask 在所有 timesteps 都是相同的，我们可以设置 noise_shape=[batch_size, 1, features]。</li><li>seed：可选，默认为 None，即产生随机熟的种子值。</li><li>training：可选，默认为 False，布尔类型，即代表了是否标志位 training 模式。</li><li>name：可选，默认为 None，dropout 层的名称。</li><li>返回： 经过 dropout 层之后的 Tensor。</li></ul></blockquote><h4 id="定义模型函数"><a href="#定义模型函数" class="headerlink" title="定义模型函数"></a>定义模型函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the model function (following TF Estimator Template)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the neural network</span></span><br><span class="line">    <span class="comment"># Because Dropout have different behavior at training and prediction time, we</span></span><br><span class="line">    <span class="comment"># need to create 2 distinct computation graphs that still share the same weights.</span></span><br><span class="line">    logits_train = conv_net(features, num_classes, dropout, reuse=<span class="keyword">False</span>, is_training=<span class="keyword">True</span>)</span><br><span class="line">    logits_test = conv_net(features, num_classes, dropout, reuse=<span class="keyword">True</span>, is_training=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predictions</span></span><br><span class="line">    pred_classes = tf.argmax(logits_test, axis=<span class="number">1</span>)</span><br><span class="line">    pred_probas = tf.nn.softmax(logits_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If prediction mode, early return</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, predictions=pred_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define loss and optimizer</span></span><br><span class="line">    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))</span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Evaluate the accuracy of the model</span></span><br><span class="line">    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TF Estimators requires to return a EstimatorSpec, that specify</span></span><br><span class="line">    <span class="comment"># the different ops for training, evaluating, ...</span></span><br><span class="line">    estim_specs = tf.estimator.EstimatorSpec(</span><br><span class="line">      mode=mode,</span><br><span class="line">      predictions=pred_classes,</span><br><span class="line">      loss=loss_op,</span><br><span class="line">      train_op=train_op,</span><br><span class="line">      eval_metric_ops=&#123;<span class="string">'accuracy'</span>: acc_op&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> estim_specs</span><br></pre></td></tr></table></figure><h4 id="创建评估器"><a href="#创建评估器" class="headerlink" title="创建评估器"></a>创建评估器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the Estimator</span></span><br><span class="line">model = tf.estimator.Estimator(model_fn)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Using default config.WARNING:tensorflow:Using temporary folder as model directory: C:\Users\xywang\AppData\Local\Temp\tmp8i1k3w75INFO:tensorflow:Using config: {&apos;_model_dir&apos;: &apos;C:\\Users\\xywang\\AppData\\Local\\Temp\\tmp8i1k3w75&apos;, &apos;_tf_random_seed&apos;: None, &apos;_save_summary_steps&apos;: 100, &apos;_save_checkpoints_steps&apos;: None, &apos;_save_checkpoints_secs&apos;: 600, &apos;_session_config&apos;: None, &apos;_keep_checkpoint_max&apos;: 5, &apos;_keep_checkpoint_every_n_hours&apos;: 10000, &apos;_log_step_count_steps&apos;: 100, &apos;_service&apos;: None, &apos;_cluster_spec&apos;: &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F84714B780&gt;, &apos;_task_type&apos;: &apos;worker&apos;, &apos;_task_id&apos;: 0, &apos;_global_id_in_cluster&apos;: 0, &apos;_master&apos;: &apos;&apos;, &apos;_evaluation_master&apos;: &apos;&apos;, &apos;_is_chief&apos;: True, &apos;_num_ps_replicas&apos;: 0, &apos;_num_worker_replicas&apos;: 1}</code></pre><h4 id="定义输入方法"><a href="#定义输入方法" class="headerlink" title="定义输入方法"></a>定义输入方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the input function for training</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: mnist.train.images&#125;, y=mnist.train.labels,</span><br><span class="line">    batch_size=batch_size, num_epochs=<span class="keyword">None</span>, shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the Model</span></span><br><span class="line">model.train(input_fn, steps=num_steps)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Create CheckpointSaverHook.INFO:tensorflow:Graph was finalized.INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.INFO:tensorflow:Saving checkpoints for 1 into C:\Users\xywang\AppData\Local\Temp\tmp8i1k3w75\model.ckpt.INFO:tensorflow:loss = 2.310159, step = 1INFO:tensorflow:global_step/sec: 7.94691INFO:tensorflow:loss = 0.15775274, step = 101 (12.585 sec)INFO:tensorflow:global_step/sec: 7.43979INFO:tensorflow:loss = 0.051440004, step = 201 (13.440 sec)INFO:tensorflow:global_step/sec: 8.26849INFO:tensorflow:loss = 0.07565387, step = 301 (12.095 sec)INFO:tensorflow:global_step/sec: 8.47324INFO:tensorflow:loss = 0.043410238, step = 401 (11.802 sec)INFO:tensorflow:global_step/sec: 7.94311INFO:tensorflow:loss = 0.048961755, step = 501 (12.590 sec)INFO:tensorflow:global_step/sec: 8.58757INFO:tensorflow:loss = 0.024859685, step = 601 (11.645 sec)INFO:tensorflow:global_step/sec: 8.39987INFO:tensorflow:loss = 0.07183821, step = 701 (11.904 sec)INFO:tensorflow:global_step/sec: 8.6733INFO:tensorflow:loss = 0.007703744, step = 801 (11.530 sec)INFO:tensorflow:global_step/sec: 8.25551INFO:tensorflow:loss = 0.02502199, step = 901 (12.113 sec)INFO:tensorflow:global_step/sec: 7.98054INFO:tensorflow:loss = 0.019118268, step = 1001 (12.563 sec)INFO:tensorflow:global_step/sec: 8.3921INFO:tensorflow:loss = 0.009793495, step = 1101 (11.884 sec)INFO:tensorflow:global_step/sec: 7.6179INFO:tensorflow:loss = 0.08203622, step = 1201 (13.127 sec)INFO:tensorflow:global_step/sec: 8.35142INFO:tensorflow:loss = 0.03721855, step = 1301 (11.975 sec)INFO:tensorflow:global_step/sec: 8.33818INFO:tensorflow:loss = 0.025231175, step = 1401 (11.992 sec)INFO:tensorflow:global_step/sec: 8.6748INFO:tensorflow:loss = 0.026730753, step = 1501 (11.528 sec)INFO:tensorflow:global_step/sec: 8.43105INFO:tensorflow:loss = 0.008975061, step = 1601 (11.862 sec)INFO:tensorflow:global_step/sec: 8.46893INFO:tensorflow:loss = 0.011308375, step = 1701 (11.807 sec)INFO:tensorflow:global_step/sec: 8.34723INFO:tensorflow:loss = 0.007505517, step = 1801 (11.980 sec)INFO:tensorflow:global_step/sec: 8.38929INFO:tensorflow:loss = 0.021354698, step = 1901 (11.920 sec)INFO:tensorflow:Saving checkpoints for 2000 into C:\Users\xywang\AppData\Local\Temp\tmp8i1k3w75\model.ckpt.INFO:tensorflow:Loss for final step: 0.011493968.tensorflow.python.estimator.estimator.Estimator at 0x1f84570c710</code></pre><h4 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate the Model</span></span><br><span class="line"><span class="comment"># Define the input function for evaluating</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: mnist.test.images&#125;, y=mnist.test.labels,</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># Use the Estimator 'evaluate' method</span></span><br><span class="line">model.evaluate(input_fn)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Starting evaluation at 2018-04-11-09:41:50INFO:tensorflow:Graph was finalized.INFO:tensorflow:Restoring parameters from C:\Users\xywang\AppData\Local\Temp\tmp8i1k3w75\model.ckpt-2000INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.INFO:tensorflow:Finished evaluation at 2018-04-11-09:41:53INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9868, global_step = 2000, loss = 0.043212146{&apos;accuracy&apos;: 0.9868, &apos;global_step&apos;: 2000, &apos;loss&apos;: 0.043212146}</code></pre><h4 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predict single images</span></span><br><span class="line">n_images = <span class="number">1</span></span><br><span class="line"><span class="comment"># Get images from test set</span></span><br><span class="line">test_images = mnist.test.images[:n_images]</span><br><span class="line"><span class="comment"># Prepare the input data</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: test_images&#125;, shuffle=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># Use the model to predict the images class</span></span><br><span class="line">preds = list(model.predict(input_fn))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_images):</span><br><span class="line">    plt.imshow(np.reshape(test_images[i], [<span class="number">28</span>, <span class="number">28</span>]), cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"Model prediction:"</span>, preds[i])</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Graph was finalized.INFO:tensorflow:Restoring parameters from C:\Users\xywang\AppData\Local\Temp\tmp8i1k3w75\model.ckpt-2000INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.</code></pre><p><img src="http://xukeqiniu.xukeai.cn/tf_7output_21_1.png" alt=""></p><pre><code>Model prediction: 7</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/u014365862/article/details/77833481" target="_blank" rel="noopener">tf API 研读1：tf.nn，tf.layers， tf.contrib概述</a></p><p>[2] <a href="https://cuiqingcai.com/5715.html" target="_blank" rel="noopener">TensorFlow layers模块用法</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow神经网络之多层感知机Eager API</title>
      <link href="/2018/03/08/DeepLearning/TensorFlow/08Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BAEager%20API/"/>
      <url>/2018/03/08/DeepLearning/TensorFlow/08Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BAEager%20API/</url>
      <content type="html"><![CDATA[<h3 id="Tensorflow多层感知机Eager-API"><a href="#Tensorflow多层感知机Eager-API" class="headerlink" title="Tensorflow多层感知机Eager API"></a>Tensorflow多层感知机Eager API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br></pre></td></tr></table></figure><h4 id="设置-Eager-API"><a href="#设置-Eager-API" class="headerlink" title="设置 Eager API"></a>设置 Eager API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set Eager API</span></span><br><span class="line">tfe.enable_eager_execution()</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="定义参数"><a href="#定义参数" class="headerlink" title="定义参数"></a>定义参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span> <span class="comment"># 1st layer number of neurons</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span> <span class="comment"># 2nd layer number of neurons</span></span><br><span class="line">num_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br></pre></td></tr></table></figure><h4 id="数据拆分成批"><a href="#数据拆分成批" class="headerlink" title="数据拆分成批"></a>数据拆分成批</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Using TF Dataset to split data into batches</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (mnist.train.images, mnist.train.labels)).batch(batch_size)</span><br><span class="line">dataset_iter = tfe.Iterator(dataset)</span><br></pre></td></tr></table></figure><h4 id="定义多层感知机模型"><a href="#定义多层感知机模型" class="headerlink" title="定义多层感知机模型"></a>定义多层感知机模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the neural network. To use eager API and tf.layers API together,</span></span><br><span class="line"><span class="comment"># we must instantiate a tfe.Network class as follow:</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span><span class="params">(tfe.Network)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Define each layer</span></span><br><span class="line">        super(NeuralNet, self).__init__()</span><br><span class="line">        <span class="comment"># Hidden fully connected layer with 256 neurons</span></span><br><span class="line">        self.layer1 = self.track_layer(</span><br><span class="line">            tf.layers.Dense(n_hidden_1, activation=tf.nn.relu))</span><br><span class="line">        <span class="comment"># Hidden fully connected layer with 256 neurons</span></span><br><span class="line">        self.layer2 = self.track_layer(</span><br><span class="line">            tf.layers.Dense(n_hidden_2, activation=tf.nn.relu))</span><br><span class="line">        <span class="comment"># Output fully connected layer with a neuron for each class</span></span><br><span class="line">        self.out_layer = self.track_layer(tf.layers.Dense(num_classes))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        <span class="keyword">return</span> self.out_layer(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">neural_net = NeuralNet()</span><br></pre></td></tr></table></figure><h4 id="定义损失函数-优化方法-准确率"><a href="#定义损失函数-优化方法-准确率" class="headerlink" title="定义损失函数+优化方法+准确率"></a>定义损失函数+优化方法+准确率</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cross-Entropy loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(inference_fn, inputs, labels)</span>:</span></span><br><span class="line">    <span class="comment"># Using sparse_softmax cross entropy</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=inference_fn(inputs), labels=labels))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(inference_fn, inputs, labels)</span>:</span></span><br><span class="line">    prediction = tf.nn.softmax(inference_fn(inputs))</span><br><span class="line">    correct_pred = tf.equal(tf.argmax(prediction, <span class="number">1</span>), labels)</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># SGD Optimizer</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute gradients</span></span><br><span class="line">grad = tfe.implicit_gradients(loss_fn)</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training</span></span><br><span class="line">average_loss = <span class="number">0.</span></span><br><span class="line">average_acc = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Iterate through the dataset</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        d = dataset_iter.next()</span><br><span class="line">    <span class="keyword">except</span> StopIteration:</span><br><span class="line">        <span class="comment"># Refill queue</span></span><br><span class="line">        dataset_iter = tfe.Iterator(dataset)</span><br><span class="line">        d = dataset_iter.next()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Images</span></span><br><span class="line">    x_batch = d[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Labels</span></span><br><span class="line">    y_batch = tf.cast(d[<span class="number">1</span>], dtype=tf.int64)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the batch loss</span></span><br><span class="line">    batch_loss = loss_fn(neural_net, x_batch, y_batch)</span><br><span class="line">    average_loss += batch_loss</span><br><span class="line">    <span class="comment"># Compute the batch accuracy</span></span><br><span class="line">    batch_accuracy = accuracy_fn(neural_net, x_batch, y_batch)</span><br><span class="line">    average_acc += batch_accuracy</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Display the initial cost, before optimizing</span></span><br><span class="line">        print(<span class="string">"Initial loss= &#123;:.9f&#125;"</span>.format(average_loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the variables following gradients info</span></span><br><span class="line">    optimizer.apply_gradients(grad(neural_net, x_batch, y_batch))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display info</span></span><br><span class="line">    <span class="keyword">if</span> (step + <span class="number">1</span>) % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">            average_loss /= display_step</span><br><span class="line">            average_acc /= display_step</span><br><span class="line">        print(<span class="string">"Step:"</span>, <span class="string">'%04d'</span> % (step + <span class="number">1</span>), <span class="string">" loss="</span>,</span><br><span class="line">              <span class="string">"&#123;:.9f&#125;"</span>.format(average_loss), <span class="string">" accuracy="</span>,</span><br><span class="line">              <span class="string">"&#123;:.4f&#125;"</span>.format(average_acc))</span><br><span class="line">        average_loss = <span class="number">0.</span></span><br><span class="line">        average_acc = <span class="number">0.</span></span><br></pre></td></tr></table></figure><hr><pre><code>Initial loss= 2.362281322Step: 0001  loss= 2.362281322  accuracy= 0.0391Step: 0100  loss= 0.583163500  accuracy= 0.8291Step: 0200  loss= 0.247603565  accuracy= 0.9281Step: 0300  loss= 0.214451462  accuracy= 0.9360Step: 0400  loss= 0.182251021  accuracy= 0.9452Step: 0500  loss= 0.139149994  accuracy= 0.9585Step: 0600  loss= 0.120601922  accuracy= 0.9649Step: 0700  loss= 0.114957660  accuracy= 0.9655Step: 0800  loss= 0.111238368  accuracy= 0.9660Step: 0900  loss= 0.085549861  accuracy= 0.9754Step: 1000  loss= 0.079464689  accuracy= 0.9752</code></pre><h4 id="测试评估"><a href="#测试评估" class="headerlink" title="测试评估"></a>测试评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate model on the test image set</span></span><br><span class="line">testX = mnist.test.images</span><br><span class="line">testY = mnist.test.labels</span><br><span class="line"></span><br><span class="line">test_acc = accuracy_fn(neural_net, testX, testY)</span><br><span class="line">print(<span class="string">"Testset Accuracy: &#123;:.4f&#125;"</span>.format(test_acc))</span><br></pre></td></tr></table></figure><hr><pre><code>Testset Accuracy: 0.9707</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">TensorFlow-Examples</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow神经网络之多层感知机</title>
      <link href="/2018/03/07/DeepLearning/TensorFlow/07Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
      <url>/2018/03/07/DeepLearning/TensorFlow/07Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
      <content type="html"><![CDATA[<h3 id="多层感知机简介"><a href="#多层感知机简介" class="headerlink" title="多层感知机简介"></a>多层感知机简介</h3><h5 id="多层感知机模型"><a href="#多层感知机模型" class="headerlink" title="多层感知机模型"></a>多层感知机模型</h5><p><img src="http://xukeqiniu.xukeai.cn/0e39fd772bfa170145f493cacadab129.png" alt=""></p><p>这里定义含有两个隐含层的模型，隐含层输出均为256个节点，输入784（MNIST数据集图片大小28*28），输出10。</p><a id="more"></a><h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><p>比较常用的是 ReLU：relu(x)=max(x,0)，本例中没有加激活函数。</p><h5 id="softmax（同前面的logistic回归）"><a href="#softmax（同前面的logistic回归）" class="headerlink" title="softmax（同前面的logistic回归）"></a>softmax（同前面的logistic回归）</h5><h5 id="损失函数：交叉熵"><a href="#损失函数：交叉熵" class="headerlink" title="损失函数：交叉熵"></a>损失函数：交叉熵</h5><h3 id="Tensorflow实现多层感知机"><a href="#Tensorflow实现多层感知机" class="headerlink" title="Tensorflow实现多层感知机"></a>Tensorflow实现多层感知机</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span> <span class="comment"># 1st layer number of neurons</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span> <span class="comment"># 2nd layer number of neurons</span></span><br><span class="line">num_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br></pre></td></tr></table></figure><h4 id="定义多层感知机模型"><a href="#定义多层感知机模型" class="headerlink" title="定义多层感知机模型"></a>定义多层感知机模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the neural network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neural_net</span><span class="params">(x_dict)</span>:</span></span><br><span class="line">    <span class="comment"># TF Estimator input is a dict, in case of multiple inputs</span></span><br><span class="line">    x = x_dict[<span class="string">'images'</span>]</span><br><span class="line">    <span class="comment"># Hidden fully connected layer with 256 neurons</span></span><br><span class="line">    layer_1 = tf.layers.dense(x, n_hidden_1)</span><br><span class="line">    <span class="comment"># Hidden fully connected layer with 256 neurons</span></span><br><span class="line">    layer_2 = tf.layers.dense(layer_1, n_hidden_2)</span><br><span class="line">    <span class="comment"># Output fully connected layer with a neuron for each class</span></span><br><span class="line">    out_layer = tf.layers.dense(layer_2, num_classes)</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br></pre></td></tr></table></figure><blockquote><p><strong>补充：dense</strong> [^2]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;dense(</span><br><span class="line">&gt;    inputs,</span><br><span class="line">&gt;    units,</span><br><span class="line">&gt;    activation=<span class="keyword">None</span>,</span><br><span class="line">&gt;    use_bias=<span class="keyword">True</span>,</span><br><span class="line">&gt;    kernel_initializer=<span class="keyword">None</span>,</span><br><span class="line">&gt;    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">&gt;    kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;    bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;    activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;    kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">&gt;    bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">&gt;    trainable=<span class="keyword">True</span>,</span><br><span class="line">&gt;    name=<span class="keyword">None</span>,</span><br><span class="line">&gt;    reuse=<span class="keyword">None</span></span><br><span class="line">&gt;)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p><p>参数说明如下：</p><ul><li>inputs：必需，即需要进行操作的输入数据。</li><li>units：必须，即神经元的数量。</li><li>activation：可选，默认为 None，如果为 None 则是线性激&gt;活。</li><li>use_bias：可选，默认为 True，是否使用偏置。</li><li>kernel_initializer：可选，默认为 None，即权重的初始化&gt;方法，如果为 None，则使用默认的 Xavier 初始化方法。</li><li>bias_initializer：可选，默认为零值初始化，即偏置的初始&gt;化方法。</li><li>kernel_regularizer：可选，默认为 None，施加在权重上的&gt;正则项。</li><li>bias_regularizer：可选，默认为 None，施加在偏置上的正&gt;则项。</li><li>activity_regularizer：可选，默认为 None，施加在输出上&gt;的正则项。</li><li>kernel_constraint，可选，默认为 None，施加在权重上的&gt;约束项。</li><li>bias_constraint，可选，默认为 None，施加在偏置上的约束&gt;项。</li><li>trainable：可选，默认为 True，布尔类型，如果为 True，&gt;则将变量添加到 GraphKeys.TRAINABLE_VARIABLES 中。</li><li>name：可选，默认为 None，卷积层的名称。</li><li>reuse：可选，默认为 None，布尔类型，如果为 True，那么&gt;如果 name 相同时，会重复利用。</li><li>返回值： 全连接网络处理后的 Tensor。</li></ul><p><strong>上面的代码中，第三个参数为空，所以这里采用线性激活</strong></p></blockquote><h4 id="定义模型函数"><a href="#定义模型函数" class="headerlink" title="定义模型函数"></a>定义模型函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the model function (following TF Estimator Template)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the neural network</span></span><br><span class="line">    logits = neural_net(features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predictions</span></span><br><span class="line">    pred_classes = tf.argmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">    pred_probas = tf.nn.softmax(logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If prediction mode, early return</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, predictions=pred_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define loss and optimizer</span></span><br><span class="line">    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Evaluate the accuracy of the model</span></span><br><span class="line">    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TF Estimators requires to return a EstimatorSpec, that specify</span></span><br><span class="line">    <span class="comment"># the different ops for training, evaluating, ...</span></span><br><span class="line">    estim_specs = tf.estimator.EstimatorSpec(</span><br><span class="line">      mode=mode,</span><br><span class="line">      predictions=pred_classes,</span><br><span class="line">      loss=loss_op,</span><br><span class="line">      train_op=train_op,</span><br><span class="line">      eval_metric_ops=&#123;<span class="string">'accuracy'</span>: acc_op&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> estim_specs</span><br></pre></td></tr></table></figure><h4 id="构建评估器"><a href="#构建评估器" class="headerlink" title="构建评估器"></a>构建评估器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the Estimator</span></span><br><span class="line">model = tf.estimator.Estimator(model_fn)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Using default config.WARNING:tensorflow:Using temporary folder as model directory: C:\Users\xywang\AppData\Local\Temp\tmp995gddibINFO:tensorflow:Using config: {&apos;_model_dir&apos;: &apos;C:\\Users\\xywang\\AppData\\Local\\Temp\\tmp995gddib&apos;, &apos;_tf_random_seed&apos;: None, &apos;_save_summary_steps&apos;: 100, &apos;_save_checkpoints_steps&apos;: None, &apos;_save_checkpoints_secs&apos;: 600, &apos;_session_config&apos;: None, &apos;_keep_checkpoint_max&apos;: 5, &apos;_keep_checkpoint_every_n_hours&apos;: 10000, &apos;_log_step_count_steps&apos;: 100, &apos;_service&apos;: None, &apos;_cluster_spec&apos;: &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x0000024BBEE7A1D0&gt;, &apos;_task_type&apos;: &apos;worker&apos;, &apos;_task_id&apos;: 0, &apos;_global_id_in_cluster&apos;: 0, &apos;_master&apos;: &apos;&apos;, &apos;_evaluation_master&apos;: &apos;&apos;, &apos;_is_chief&apos;: True, &apos;_num_ps_replicas&apos;: 0, &apos;_num_worker_replicas&apos;: 1}</code></pre><blockquote><p><strong>补充：tf.estimator</strong><br>tf.estimator就是已经预定义好的模型，自带train，evaluate，predict。<br>其编程范式为：</p><ul><li>定义算法模型，比如多层感知机，CNN；</li><li>定义模型函数（model_fn），包括构建graph，定义损失函数、优化器，估计准确率等，返回结果分训练和测试两种情况；</li><li><p>构建评估器；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; model = tf.estimator.Estimator(model_fn)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li><li><p>用 tf.estimator.inputs.numpy_input_fn 把 input_fn 传入 model，就可调用 model.train, model.evaluate, model.predict 了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; input_fn = tf.estimator.inputs.numpy_input_fn()</span><br><span class="line">&gt; model.train(input_fn)</span><br><span class="line">&gt; model.evaluate(input_fn)</span><br><span class="line">&gt; model.predict(input_fn)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li></ul><p>Estimator 是一种更高层次的封装，它把一些基本算法的算法&gt;模型和模型函数预定义好，你只需要传入参数即可。</p><h5 id="input-fn-1"><a href="#input-fn-1" class="headerlink" title="input_fn [^1]"></a>input_fn [^1]</h5><p>一般来讲，input_fn方法做两件事：</p><p>1.数据预处理，如洗脏数据，归整数据等。没有就空着。</p><p>2.返回feature_cols, labels。</p><ul><li>feature_cols：一个dict，key为feature名，value&gt;为feature值。</li><li>lables: 对应的分类标签。</li></ul><p>可以将多种对象转换为tensorflow对象，常见的为将Numpy&gt;转tensorflow对象。比如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt; <span class="comment">#numpy input_fn.</span></span><br><span class="line">&gt; x_data =[&#123;<span class="string">"feature1"</span>: <span class="number">2</span>, <span class="string">"features2"</span>:<span class="number">6</span>&#125;,</span><br><span class="line">&gt;          &#123;<span class="string">"feature1"</span>: <span class="number">1</span>, <span class="string">"features2"</span>:<span class="number">5</span>&#125;,</span><br><span class="line">&gt;          &#123;<span class="string">"feature1"</span>: <span class="number">4</span>, <span class="string">"features2"</span>:<span class="number">8</span>&#125;]</span><br><span class="line">&gt; y_data = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">&gt; my_input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">&gt;     x=&#123;<span class="string">"x"</span>: np.array(x_data)&#125;,</span><br><span class="line">&gt;     y=np.array(y_data),</span><br><span class="line">&gt;     shuffle = <span class="keyword">True</span>)</span><br><span class="line">&gt; <span class="comment">#得到的是一个名为my_input_fn的function对象。</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p></blockquote><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the input function for training</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: mnist.train.images&#125;, y=mnist.train.labels,</span><br><span class="line">    batch_size=batch_size, num_epochs=<span class="keyword">None</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Train the Model</span></span><br><span class="line">model.train(input_fn, steps=num_steps)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Create CheckpointSaverHook.INFO:tensorflow:Graph was finalized.INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.INFO:tensorflow:Saving checkpoints for 1 into C:\Users\xywang\AppData\Local\Temp\tmp995gddib\model.ckpt.INFO:tensorflow:loss = 2.4631722, step = 1INFO:tensorflow:global_step/sec: 248.501INFO:tensorflow:loss = 0.42414927, step = 101 (0.388 sec)INFO:tensorflow:global_step/sec: 301.862INFO:tensorflow:loss = 0.48539022, step = 201 (0.331 sec)INFO:tensorflow:global_step/sec: 310.3INFO:tensorflow:loss = 0.2779913, step = 301 (0.323 sec)INFO:tensorflow:global_step/sec: 303.697INFO:tensorflow:loss = 0.20052063, step = 401 (0.329 sec)INFO:tensorflow:global_step/sec: 322.311INFO:tensorflow:loss = 0.5092098, step = 501 (0.309 sec)INFO:tensorflow:global_step/sec: 337.555INFO:tensorflow:loss = 0.28386787, step = 601 (0.297 sec)INFO:tensorflow:global_step/sec: 322.309INFO:tensorflow:loss = 0.36957514, step = 701 (0.309 sec)INFO:tensorflow:global_step/sec: 334.17INFO:tensorflow:loss = 0.28504127, step = 801 (0.300 sec)INFO:tensorflow:global_step/sec: 319.222INFO:tensorflow:loss = 0.37339848, step = 901 (0.312 sec)INFO:tensorflow:Saving checkpoints for 1000 into C:\Users\xywang\AppData\Local\Temp\tmp995gddib\model.ckpt.INFO:tensorflow:Loss for final step: 0.2043538.tensorflow.python.estimator.estimator.Estimator at 0x24bbee72160</code></pre><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate the Model</span></span><br><span class="line"><span class="comment"># Define the input function for evaluating</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: mnist.test.images&#125;, y=mnist.test.labels,</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># Use the Estimator 'evaluate' method</span></span><br><span class="line">model.evaluate(input_fn)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Starting evaluation at 2018-04-11-08:04:38INFO:tensorflow:Graph was finalized.INFO:tensorflow:Restoring parameters from C:\Users\xywang\AppData\Local\Temp\tmp995gddib\model.ckpt-1000INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.INFO:tensorflow:Finished evaluation at 2018-04-11-08:04:38INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9149, global_step = 1000, loss = 0.29386005{&apos;accuracy&apos;: 0.9149, &apos;global_step&apos;: 1000, &apos;loss&apos;: 0.29386005}</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predict single images</span></span><br><span class="line">n_images = <span class="number">1</span></span><br><span class="line"><span class="comment"># Get images from test set</span></span><br><span class="line">test_images = mnist.test.images[:n_images]</span><br><span class="line"><span class="comment"># Prepare the input data</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: test_images&#125;, shuffle=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># Use the model to predict the images class</span></span><br><span class="line">preds = list(model.predict(input_fn))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_images):</span><br><span class="line">    plt.imshow(np.reshape(test_images[i], [<span class="number">28</span>, <span class="number">28</span>]), cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"Model prediction:"</span>, preds[i])</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Graph was finalized.INFO:tensorflow:Restoring parameters from C:\Users\xywang\AppData\Local\Temp\tmp995gddib\model.ckpt-1000INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.</code></pre><p><img src="http://xukeqiniu.xukeai.cn/tf_output_21_1.png" alt=""></p><pre><code>Model prediction: 7</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/vagrantabc2017/article/details/77482891" target="_blank" rel="noopener">在tf.estimator中构建inpu_fn解读</a></p><p>[2] <a href="https://cuiqingcai.com/5715.html" target="_blank" rel="noopener">TensorFlow layers模块用法</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow基本模型之K-means</title>
      <link href="/2018/03/06/DeepLearning/TensorFlow/06Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8BK-means/"/>
      <url>/2018/03/06/DeepLearning/TensorFlow/06Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8BK-means/</url>
      <content type="html"><![CDATA[<h3 id="K-Means算法简介"><a href="#K-Means算法简介" class="headerlink" title="K-Means算法简介"></a>K-Means算法简介</h3><p><code>K-MEANS</code>算法是输入聚类个数<code>k</code>，以及包含 <code>n</code>个数据对象的数据库，输出满足方差最小标准<code>k</code>个聚类的一种算法。属于一种经典的无监督学习算法。<br>示意图如下所示：<br><img src="http://xukeqiniu.xukeai.cn/111fa6ed0ddea2c909efa2d1094e5035.png" alt="K-Means算法示意图"><br><code>k-means</code> 算法接受输入量 <code>k</code> ；然后将<code>n</code>个数据对象划分为 <code>k</code>个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。聚类相似度是利用各聚类中对象的均值所获得一个“中心对象”（引力中心）来进行计算的。</p><p>基本步骤：</p><p>（1） 从 n个数据对象任意选择 k 个对象作为初始聚类中心；</p><p>（2） 根据每个聚类对象的均值（中心对象），计算每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分；</p><p>（3） 重新计算每个（有变化）聚类的均值（中心对象）；</p><p>（4） 计算标准测度函数，当满足一定条件，如函数收敛时，则算法终止；如果条件不满足则回到步骤（2）。</p><a id="more"></a><h3 id="TensorFlow的K-Means实现"><a href="#TensorFlow的K-Means实现" class="headerlink" title="TensorFlow的K-Means实现"></a>TensorFlow的K-Means实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.factorization <span class="keyword">import</span> KMeans<span class="comment">#导入KMeans函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ignore all GPUs, tf random forest does not benefit from it.</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">""</span></span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">full_data_x = mnist.train.images</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">num_steps = <span class="number">50</span> <span class="comment"># Total steps to train</span></span><br><span class="line">batch_size = <span class="number">1024</span> <span class="comment"># The number of samples per batch</span></span><br><span class="line">k = <span class="number">25</span> <span class="comment"># The number of clusters</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># The 10 digits</span></span><br><span class="line">num_features = <span class="number">784</span> <span class="comment"># Each image is 28x28 pixels</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input images</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_features])</span><br><span class="line"><span class="comment"># Labels (for assigning a label to a centroid and testing)</span></span><br><span class="line">Y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_classes])</span><br><span class="line"></span><br><span class="line"><span class="comment"># K-Means Parameters</span></span><br><span class="line"><span class="comment"># 距离度量的方式采用余弦距离（余弦相似度）</span></span><br><span class="line">kmeans = KMeans(inputs=X, num_clusters=k, distance_metric=<span class="string">'cosine'</span>,</span><br><span class="line">                use_mini_batch=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h4 id="构建K-means图模型"><a href="#构建K-means图模型" class="headerlink" title="构建K-means图模型"></a>构建K-means图模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build KMeans graph</span></span><br><span class="line">(all_scores, cluster_idx, scores, cluster_centers_initialized,init_op,train_op) = kmeans.training_graph()</span><br><span class="line">cluster_idx = cluster_idx[<span class="number">0</span>] <span class="comment"># fix for cluster_idx being a tuple</span></span><br><span class="line">avg_distance = tf.reduce_mean(scores)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init_vars = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start TensorFlow session</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the initializer</span></span><br><span class="line">sess.run(init_vars, feed_dict=&#123;X: full_data_x&#125;)</span><br><span class="line">sess.run(init_op, feed_dict=&#123;X: full_data_x&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_steps + <span class="number">1</span>):</span><br><span class="line">    _, d, idx = sess.run([train_op, avg_distance, cluster_idx],</span><br><span class="line">                         feed_dict=&#123;X: full_data_x&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">"Step %i, Avg Distance: %f"</span> % (i, d))</span><br></pre></td></tr></table></figure><hr><pre><code>Step 1, Avg Distance: 0.341471Step 10, Avg Distance: 0.221609Step 20, Avg Distance: 0.220328Step 30, Avg Distance: 0.219776Step 40, Avg Distance: 0.219419Step 50, Avg Distance: 0.219154</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assign a label to each centroid</span></span><br><span class="line"><span class="comment"># Count total number of labels per centroid, using the label of each training</span></span><br><span class="line"><span class="comment"># sample to their closest centroid (given by 'idx')</span></span><br><span class="line">counts = np.zeros(shape=(k, num_classes))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(idx)):</span><br><span class="line">    counts[idx[i]] += mnist.train.labels[i]</span><br><span class="line"><span class="comment"># Assign the most frequent label to the centroid</span></span><br><span class="line">labels_map = [np.argmax(c) <span class="keyword">for</span> c <span class="keyword">in</span> counts]</span><br><span class="line">labels_map = tf.convert_to_tensor(labels_map)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation ops</span></span><br><span class="line"><span class="comment"># Lookup: centroid_id -&gt; label</span></span><br><span class="line">cluster_label = tf.nn.embedding_lookup(labels_map, cluster_idx)</span><br><span class="line"><span class="comment"># Compute accuracy</span></span><br><span class="line">correct_prediction = tf.equal(cluster_label, tf.cast(tf.argmax(Y, <span class="number">1</span>), tf.int32))</span><br><span class="line">accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Model</span></span><br><span class="line">test_x, test_y = mnist.test.images, mnist.test.labels</span><br><span class="line">print(<span class="string">"Test Accuracy:"</span>, sess.run(accuracy_op, feed_dict=&#123;X: test_x, Y: test_y&#125;))</span><br></pre></td></tr></table></figure><hr><pre><code>Test Accuracy: 0.7127</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://baike.baidu.com/item/K-MEANS算法/594631?fr=aladdin" target="_blank" rel="noopener">百度百科——K-MEANS算法</a></p><p><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">TensorFlow-Examples</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow基本模型之随机森林</title>
      <link href="/2018/03/05/DeepLearning/TensorFlow/05Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
      <url>/2018/03/05/DeepLearning/TensorFlow/05Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
      <content type="html"><![CDATA[<h3 id="随机森林简介"><a href="#随机森林简介" class="headerlink" title="随机森林简介"></a>随机森林简介</h3><p>随机森林是一种集成学习方法。训练时每个树分类器从样本集里面随机有放回的抽取一部分进行训练。预测时将要分类的样本带入一个个树分类器，然后以少数服从多数的原则，表决出这个样本的最终分类类型。[^4]</p><p>设有N个样本，M个变量(维度)个数，该算法具体流程如下：</p><ol><li>确定一个值m，它用来表示每个树分类器选取多少个变量；</li><li>从数据集中有放回的抽取 k 个样本集，用它们创建 k 个树分类器。另外还伴随生成了 k 个袋外数据，用来后面做检测。</li><li>输入待分类样本之后，每个树分类器都会对它进行分类，然后所有分类器按照少数服从多数原则，确定分类结果。</li></ol><p>重要参数：</p><ol><li>预选变量个数 (即框架流程中的m)；</li><li>随机森林中树的个数。</li></ol><a id="more"></a><h3 id="Tensorflow-随机森林"><a href="#Tensorflow-随机森林" class="headerlink" title="Tensorflow 随机森林"></a>Tensorflow 随机森林</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.ops <span class="keyword">import</span> resources</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.tensor_forest.python <span class="keyword">import</span> tensor_forest</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ignore all GPUs, tf random forest does not benefit from it.</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">""</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>补充：__futrure__</strong>[^1]<br>简单来说，Python 的每个新版本都会增加一些新的功能，或者对原来的功能作一些改动。有些改动是不兼容旧版本的。从 Python 2.7 到 Python 3 就有不兼容的一些改动，如果你想在 Python 2.7 中使用 Python 3 的新特性，那么你就需要从__future__模块导入。</p><ul><li>division<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span>/<span class="number">3</span> = <span class="number">3</span>  <span class="comment"># python2.7中，不导入__future__</span></span><br><span class="line"><span class="number">10</span>/<span class="number">3</span> = <span class="number">3.3333333333333335</span>  <span class="comment"># python2.7中，导入__future__</span></span><br></pre></td></tr></table></figure></li></ul><p>很容易看出来，2.7中默认的整数除法是结果向下取整，而导入了<strong>future</strong>之后除法就是真正的除法了。这也是python2和python3的一个重要区别。</p><ul><li>absolute_import<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python2.7中，在默认情况下，导入模块是相对导入的（relative import），比如说</span></span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> .json <span class="keyword">import</span> json_dump</span><br></pre></td></tr></table></figure></li></ul><p>这些以’.’点导入的是相对导入，而绝对导入（absolute import）则是指从系统路径sys.path最底层的模块导入。比如:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> sys</span><br></pre></td></tr></table></figure></p><ul><li>print_function<br>这个就是最经典的python2和python3的区别了，python2中print不需要括号，而在python3中则需要。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"Hello world"</span> <span class="comment"># python2.7</span></span><br><span class="line">print(<span class="string">"Hello world"</span>) <span class="comment"># python3</span></span><br></pre></td></tr></table></figure></li></ul></blockquote><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">num_steps = <span class="number">500</span> <span class="comment"># Total steps to train</span></span><br><span class="line">batch_size = <span class="number">1024</span> <span class="comment"># The number of samples per batch</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># The 10 digits</span></span><br><span class="line">num_features = <span class="number">784</span> <span class="comment"># Each image is 28x28 pixels</span></span><br><span class="line">num_trees = <span class="number">10</span></span><br><span class="line">max_nodes = <span class="number">1000</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>补充：Estimator API</strong><br>Estimator 跟 Dataset 都是 Tensorflow 中的高级API。<br>Estimator（评估器）是一种创建 TensorFlow 模型的高级方法，它包括了用于常见机器学习任务的预制模型，当然，你也可以使用它们来创建你的自定义模型。[^3]<br>contrib.tensor_forest 详细的实现了随机森林算法（Random Forests）评估器，并对外提供 high-level API。你只需传入 params 到构造器，params 使用 params.fill() 来填充，而不用传入所有的超参数，Tensor Forest 自己的 RandomForestGraphs 就能使用这些参数来构建整幅图。[^2]</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Input and Target data</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_features])</span><br><span class="line"><span class="comment"># For random forest, labels must be integers (the class id)</span></span><br><span class="line">Y = tf.placeholder(tf.int32, shape=[<span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Random Forest Parameters</span></span><br><span class="line">hparams = tensor_forest.ForestHParams(num_classes=num_classes,</span><br><span class="line">                                      num_features=num_features,</span><br><span class="line">                                      num_trees=num_trees,</span><br><span class="line">                                      max_nodes=max_nodes).fill()</span><br><span class="line"><span class="comment"># Build the Random Forest</span></span><br><span class="line">forest_graph = tensor_forest.RandomForestGraphs(hparams)</span><br></pre></td></tr></table></figure><p>INFO:tensorflow:Constructing forest with params =<br>INFO:tensorflow:{‘num_trees’: 10, ‘max_nodes’: 1000, ‘bagging_fraction’: 1.0, ‘feature_bagging_fraction’: 1.0, ‘num_splits_to_consider’: 28, ‘max_fertile_nodes’: 0, ‘split_after_samples’: 250, ‘valid_leaf_threshold’: 1, ‘dominate_method’: ‘bootstrap’, ‘dominate_fraction’: 0.99, ‘model_name’: ‘all_dense’, ‘split_finish_name’: ‘basic’, ‘split_pruning_name’: ‘none’, ‘collate_examples’: False, ‘checkpoint_stats’: False, ‘use_running_stats_method’: False, ‘initialize_average_splits’: False, ‘inference_tree_paths’: False, ‘param_file’: None, ‘split_name’: ‘less_or_equal’, ‘early_finish_check_every_samples’: 0, ‘prune_every_samples’: 0, ‘num_classes’: 10, ‘num_features’: 784, ‘bagged_num_features’: 784, ‘bagged_features’: None, ‘regression’: False, ‘num_outputs’: 1, ‘num_output_columns’: 11, ‘base_random_seed’: 0, ‘leaf_model_type’: 0, ‘stats_model_type’: 0, ‘finish_type’: 0, ‘pruning_type’: 0, ‘split_type’: 0}</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get training graph and loss</span></span><br><span class="line">train_op = forest_graph.training_graph(X, Y)</span><br><span class="line">loss_op = forest_graph.training_loss(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Measure the accuracy</span></span><br><span class="line">infer_op, _, _ = forest_graph.inference_graph(X)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(infer_op, <span class="number">1</span>), tf.cast(Y, tf.int64))</span><br><span class="line">accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value) and forest resources</span></span><br><span class="line">init_vars = tf.group(tf.global_variables_initializer(),</span><br><span class="line">    resources.initialize_resources(resources.shared_resources()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start TensorFlow session</span></span><br><span class="line">sess = tf.train.MonitoredSession()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the initializer</span></span><br><span class="line">sess.run(init_vars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_steps + <span class="number">1</span>):</span><br><span class="line">    <span class="comment"># Prepare Data</span></span><br><span class="line">    <span class="comment"># Get the next batch of MNIST data (only images are needed, not labels)</span></span><br><span class="line">    batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">    _, l = sess.run([train_op, loss_op], feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">1</span>:</span><br><span class="line">        acc = sess.run(accuracy_op, feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">        print(<span class="string">'Step %i, Loss: %f, Acc: %f'</span> % (i, l, acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Model</span></span><br><span class="line">test_x, test_y = mnist.test.images, mnist.test.labels</span><br><span class="line">print(<span class="string">"Test Accuracy:"</span>, sess.run(accuracy_op, feed_dict=&#123;X: test_x, Y: test_y&#125;))</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Graph was finalized.INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.Step 1, Loss: -1.000000, Acc: 0.411133Step 50, Loss: -254.800003, Acc: 0.892578Step 100, Loss: -538.799988, Acc: 0.915039Step 150, Loss: -826.599976, Acc: 0.922852Step 200, Loss: -1001.000000, Acc: 0.926758Step 250, Loss: -1001.000000, Acc: 0.919922Step 300, Loss: -1001.000000, Acc: 0.933594Step 350, Loss: -1001.000000, Acc: 0.916992Step 400, Loss: -1001.000000, Acc: 0.916992Step 450, Loss: -1001.000000, Acc: 0.927734Step 500, Loss: -1001.000000, Acc: 0.917969Test Accuracy: 0.9212</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/langb2014/article/details/53305246#t1" target="_blank" rel="noopener">Python __future__ 模块</a></p><p>[2] <a href="http://www.doc88.com/p-1834979585771.html" target="_blank" rel="noopener">【机器学习】在TensorFlow中构建自定义Estimator：深度解析TensorFlow组件Estimator</a></p><p>[3] <a href="http://www.sohu.com/a/191717118_390227" target="_blank" rel="noopener">TensorFlow 1.3的Datasets和Estimator知多少？谷歌大神来解答</a></p><p>[4] <a href="https://www.cnblogs.com/muchen/p/6883263.html" target="_blank" rel="noopener">穆晨：随机森林(Random Forest)</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow基本模型之最近邻</title>
      <link href="/2018/03/04/DeepLearning/TensorFlow/04TensorFlow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8B%E6%9C%80%E8%BF%91%E9%82%BB/"/>
      <url>/2018/03/04/DeepLearning/TensorFlow/04TensorFlow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8B%E6%9C%80%E8%BF%91%E9%82%BB/</url>
      <content type="html"><![CDATA[<h3 id="最近邻算法简介"><a href="#最近邻算法简介" class="headerlink" title="最近邻算法简介"></a>最近邻算法简介</h3><p>k近邻模型的核心就是使用一种距离度量，获得距离目标点最近的k个点，根据分类决策规则，决定目标点的分类。[2]</p><p>距离度量(L1范数)：<br><img src="http://xukeqiniu.xukeai.cn/c795dda5fdeb34d262274efb237e1b3b.png" alt=""></p><p>K值选择：这里k为10。</p><p>分类决策规则：k近邻的分类决策规则是最为常见的简单多数规则，也就是在最近的K个点中，哪个标签数目最多，就把目标点的标签归于哪一类。</p><a id="more"></a><h3 id="Tensorflow-最近邻"><a href="#Tensorflow-最近邻" class="headerlink" title="Tensorflow 最近邻"></a>Tensorflow 最近邻</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h4 id="导入-mnist数据集"><a href="#导入-mnist数据集" class="headerlink" title="导入 mnist数据集"></a>导入 mnist数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MINST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In this example, we limit mnist data</span></span><br><span class="line">Xtr, Ytr = mnist.train.next_batch(<span class="number">5000</span>) <span class="comment">#5000 for training (nn candidates)</span></span><br><span class="line">Xte, Yte = mnist.test.next_batch(<span class="number">10</span>) <span class="comment">#10 for testing</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line">xtr = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">xte = tf.placeholder(<span class="string">"float"</span>, [<span class="number">784</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Nearest Neighbor calculation using L1 Distance</span></span><br><span class="line"><span class="comment"># Calculate L1 Distance</span></span><br><span class="line">distance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Prediction: Get min distance index (Nearest neighbor)</span></span><br><span class="line">pred = tf.argmin(distance, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><blockquote><p><strong>补充：Tenosrflow中基本算术运算函数:</strong>[1]</p><ul><li>tf.add(x,y,name=None) # 求和运算</li><li>tf.subtract(x,y,name=None) # 减法运算</li><li>tf.multiply(x,y,name=None) #乘法运算</li><li>tf.div(x,y,name=None)    #除法运算</li><li>tf.mod(x,y,name=None) # 取模运算</li><li>tf.abs(x,name=None) ＃求绝对值</li><li>tf.negative(x,name=None) ＃取负运算（ｙ＝－ｘ）</li><li>tf.sign(x,name=None) ＃返回符合ｘ大于０，则返回１，小于０，则返回－１</li><li>tf.reciprocal(x,name=None)　＃取反运算</li><li>tf.square(x,name=None)　＃计算平方</li><li>tf.round(x,name=None)　＃舍入最接近的整数</li><li>tf.pow(x,y,name=None)　＃幂次方</li></ul></blockquote><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">accuracy = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over test data</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xte)):</span><br><span class="line">        <span class="comment"># Get nearest neighbor</span></span><br><span class="line">        <span class="comment"># 5000个样本点分别和10个测试点计算距离</span></span><br><span class="line">        nn_index = sess.run(pred, feed_dict=&#123;xtr: Xtr, xte: Xte[i, :]&#125;)</span><br><span class="line">        print(nn_index)</span><br><span class="line">        <span class="comment"># Get nearest neighbor class label and compare it to its true label</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Test"</span>, i, <span class="string">"Prediction:"</span>, np.argmax(Ytr[nn_index]), \</span><br><span class="line">            <span class="string">"True Class:"</span>, np.argmax(Yte[i]))</span><br><span class="line">        <span class="comment"># Calculate accuracy</span></span><br><span class="line">        <span class="keyword">if</span> np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):</span><br><span class="line">            accuracy += <span class="number">1.</span>/len(Xte)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Done!"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy)</span><br></pre></td></tr></table></figure><pre><code>190Test 0 Prediction: 9 True Class: 9475Test 1 Prediction: 5 True Class: 53152Test 2 Prediction: 7 True Class: 72413Test 3 Prediction: 2 True Class: 21088Test 4 Prediction: 2 True Class: 21427Test 5 Prediction: 2 True Class: 24743Test 6 Prediction: 7 True Class: 74826Test 7 Prediction: 6 True Class: 64099Test 8 Prediction: 5 True Class: 52421Test 9 Prediction: 5 True Class: 5Done!Accuracy: 0.9999999999999999</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/zSean/article/details/75097937" target="_blank" rel="noopener">Tenosrflow中基本算术运算函数</a></p><p>[2] <a href="https://blog.csdn.net/qq_35082030/article/details/60965320" target="_blank" rel="noopener">统计学习方法——K近邻模型</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow基本模型之Logistic回归</title>
      <link href="/2018/03/03/DeepLearning/TensorFlow/03Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8BLogistic%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/03/03/DeepLearning/TensorFlow/03Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8BLogistic%E5%9B%9E%E5%BD%92/</url>
      <content type="html"><![CDATA[<h3 id="Logistic-回归-简介"><a href="#Logistic-回归-简介" class="headerlink" title="Logistic 回归 简介"></a>Logistic 回归 简介</h3><h4 id="Logistic模型"><a href="#Logistic模型" class="headerlink" title="Logistic模型"></a>Logistic模型</h4><p><img src="http://xukeqiniu.xukeai.cn/1b2eecc284aec268c9f7cc1907376184.png" alt="Logistic模型"></p><p><img src="http://xukeqiniu.xukeai.cn/65de6987629cad90b435ca677a87b5a5.png" alt="Logistic模型图解"></p><a id="more"></a><h4 id="损失函数（交叉熵损失）"><a href="#损失函数（交叉熵损失）" class="headerlink" title="损失函数（交叉熵损失）"></a>损失函数（交叉熵损失）</h4><p><img src="http://xukeqiniu.xukeai.cn/46bfcfb36ca46ba0b0c62d2ef98d9886.png" alt="交叉熵"></p><h4 id="softmax多分类"><a href="#softmax多分类" class="headerlink" title="softmax多分类"></a>softmax多分类</h4><p><img src="http://xukeqiniu.xukeai.cn/7aa15bacea22f7e5f58bf50ae9faf7cb.png" alt="softmax"></p><h3 id="Tensorflow-Logistic回归"><a href="#Tensorflow-Logistic回归" class="headerlink" title="Tensorflow Logistic回归"></a>Tensorflow Logistic回归</h3><h4 id="导入-mnist数据集"><a href="#导入-mnist数据集" class="headerlink" title="导入 mnist数据集"></a>导入 mnist数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import MINST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br></pre></td></tr></table></figure><h4 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>]) <span class="comment"># mnist data image of shape 28*28=784  # tf.placeholder(dtype, shape=None, name=None)</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>]) <span class="comment"># 0-9 digits recognition =&gt; 10 classes</span></span><br><span class="line"><span class="comment"># Set model weights</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"><span class="comment"># Construct model</span></span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W) + b) <span class="comment"># Softmax</span></span><br></pre></td></tr></table></figure><h4 id="定义损失函数（交叉熵）"><a href="#定义损失函数（交叉熵）" class="headerlink" title="定义损失函数（交叉熵）"></a>定义损失函数（交叉熵）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Minimize error using cross entropy</span></span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><blockquote><p><strong> 补充: reduction_indices参数[1]</strong></p><p>在tensorflow的使用中，经常会使用tf.reduce_mean,tf.reduce_sum等函数，在函数中，有一个reduction_indices参数，表示函数的处理维度，直接上图，一目了然：<br><img src="http://xukeqiniu.xukeai.cn/0737761612df365b3afce08b9d619655.png" alt=""></p><p>tf.reduce_sum(x)  ==&gt; 如果不指定第二个参数，那么就在所有的元素求和</p><p>tf.reduce_sum(x, 0)  ==&gt; 指定第二个参数为0，则第一维的元素求和，即每一列求和</p><p>tf.reduce_sum(x, 1) ==&gt; 指定第二个参数为1，则第二维的元素求和，即每一行求和</p></blockquote><h4 id="设置优化器（SGD）"><a href="#设置优化器（SGD）" class="headerlink" title="设置优化器（SGD）"></a>设置优化器（SGD）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gradient Descent</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Fit training using batch data</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_xs,</span><br><span class="line">                                                          y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Calculate accuracy for 3000 examples</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) <span class="comment"># cast(x, dtype, name=None) 将x的数据格式转化成dtype.</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images[:<span class="number">3000</span>], y: mnist.test.labels[:<span class="number">3000</span>]&#125;))</span><br></pre></td></tr></table></figure><pre><code>Epoch: 0001 cost= 1.183862086Epoch: 0002 cost= 0.665185326Epoch: 0003 cost= 0.552937392Epoch: 0004 cost= 0.498404927Epoch: 0005 cost= 0.465865389Epoch: 0006 cost= 0.442440675Epoch: 0007 cost= 0.425578112Epoch: 0008 cost= 0.412035317Epoch: 0009 cost= 0.401478231Epoch: 0010 cost= 0.392347213Epoch: 0011 cost= 0.384493829Epoch: 0012 cost= 0.377989292Epoch: 0013 cost= 0.372704204Epoch: 0014 cost= 0.366971537Epoch: 0015 cost= 0.362937522Epoch: 0016 cost= 0.358783882Epoch: 0017 cost= 0.355023325Epoch: 0018 cost= 0.351152160Epoch: 0019 cost= 0.348280402Epoch: 0020 cost= 0.345466763Epoch: 0021 cost= 0.342640696Epoch: 0022 cost= 0.340194521Epoch: 0023 cost= 0.338306610Epoch: 0024 cost= 0.335532565Epoch: 0025 cost= 0.333705268Optimization Finished!Accuracy: 0.889</code></pre><blockquote><p><strong>补充 tf.argmax:[2]</strong></p><p>简单的说，tf.argmax就是返回最大的那个数值所在的下标。 </p><p>tf.argmax(array, 1)和tf.argmax(array, 0)的区别看下面的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])</span><br><span class="line">np.argmax(test, 0)　　　＃输出：array([3, 3, 1]</span><br><span class="line">np.argmax(test, 1)　　　＃输出：array([2, 2, 0, 0]</span><br></pre></td></tr></table></figure></p></blockquote><ul><li><p>axis = 0:<br>　　你就这么想，0是最大的范围，所有的数组都要进行比较，只是比较的是这些数组相同位置上的数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test[0] = array([1, 2, 3])</span><br><span class="line">test[1] = array([2, 3, 4])</span><br><span class="line">test[2] = array([5, 4, 3])</span><br><span class="line">test[3] = array([8, 7, 2])</span><br><span class="line"># output   :    [3, 3, 1]</span><br></pre></td></tr></table></figure></li><li><p>axis = 1:<br>　　等于1的时候，比较范围缩小了，只会比较每个数组内的数的大小，结果也会根据有几个数组，产生几个结果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test[0] = array([1, 2, 3])  #2</span><br><span class="line">test[1] = array([2, 3, 4])  #2</span><br><span class="line">test[2] = array([5, 4, 3])  #0</span><br><span class="line">test[3] = array([8, 7, 2])  #0</span><br></pre></td></tr></table></figure></li></ul><h3 id="Tensorflow-Eager-API-Logistic回归"><a href="#Tensorflow-Eager-API-Logistic回归" class="headerlink" title="Tensorflow Eager API Logistic回归"></a>Tensorflow Eager API Logistic回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br></pre></td></tr></table></figure><h4 id="设置-Eager-API"><a href="#设置-Eager-API" class="headerlink" title="设置 Eager API"></a>设置 Eager API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set Eager API</span></span><br><span class="line">tfe.enable_eager_execution()</span><br></pre></td></tr></table></figure><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置变量"><a href="#设置变量" class="headerlink" title="设置变量"></a>设置变量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">display_step = <span class="number">100</span></span><br></pre></td></tr></table></figure><h4 id="调用-Dataset-API-读取数据-3"><a href="#调用-Dataset-API-读取数据-3" class="headerlink" title="调用 Dataset API 读取数据[3]"></a>调用 Dataset API 读取数据[3]</h4><p>Dataset API是TensorFlow 1.3版本中引入的一个新的模块，主要服务于数据读取，构建输入数据的pipeline。</p><p>如果想要用到Eager模式，就必须要使用Dataset API来读取数据。</p><p>之前有用 placeholder 读取数据，tf.data.Dataset.from_tensor_slices 是另一种方式，其主要作用是切分传入 Tensor 的第一个维度，生成相应的 dataset。以下面的例子为例，是对 mnist.train.images 按batch_size 进行切分。</p><p>在Eager模式中，创建Iterator的方式是通过 tfe.Iterator(dataset) 的形式直接创建Iterator并迭代。迭代时可以直接取出值，不需要使用sess.run()。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iterator for the dataset</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (mnist.train.images, mnist.train.labels)).batch(batch_size)</span><br><span class="line">dataset_iter = tfe.Iterator(dataset)</span><br></pre></td></tr></table></figure><h4 id="定义模型（公式-损失函数-准确率计算）"><a href="#定义模型（公式-损失函数-准确率计算）" class="headerlink" title="定义模型（公式+损失函数+准确率计算）"></a>定义模型（公式+损失函数+准确率计算）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Variables</span></span><br><span class="line">W = tfe.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]), name=<span class="string">'weights'</span>)</span><br><span class="line">b = tfe.Variable(tf.zeros([<span class="number">10</span>]), name=<span class="string">'bias'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Logistic regression (Wx + b)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(inputs, W) + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cross-Entropy loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(inference_fn, inputs, labels)</span>:</span></span><br><span class="line">    <span class="comment"># Using sparse_softmax cross entropy</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=inference_fn(inputs), labels=labels))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(inference_fn, inputs, labels)</span>:</span></span><br><span class="line">    prediction = tf.nn.softmax(inference_fn(inputs))</span><br><span class="line">    correct_pred = tf.equal(tf.argmax(prediction, <span class="number">1</span>), labels)</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br></pre></td></tr></table></figure><blockquote><p><strong>补充:  tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None):</strong>[4]</p><ul><li>第一个参数logits：就是神经网络最后一层的输出，如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes</li><li>第二个参数labels：实际的标签，大小同上</li></ul><p>执行下面两步操作：<br><img src="http://xukeqiniu.xukeai.cn/4b8a342d4d19c2420e5458e1e1987900.png" alt=""></p><p>返回值是一个向量,对向量求 tf.reduce_mean，得到loss。</p></blockquote><h4 id="设置优化器（SGD）-1"><a href="#设置优化器（SGD）-1" class="headerlink" title="设置优化器（SGD）"></a>设置优化器（SGD）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGD Optimizer</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute gradients</span></span><br><span class="line">grad = tfe.implicit_gradients(loss_fn)</span><br></pre></td></tr></table></figure><h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training</span></span><br><span class="line">average_loss = <span class="number">0.</span></span><br><span class="line">average_acc = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Iterate through the dataset</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        d = dataset_iter.next()</span><br><span class="line">    <span class="keyword">except</span> StopIteration:  <span class="comment"># try...except，except用于处理异常</span></span><br><span class="line">        <span class="comment"># Refill queue</span></span><br><span class="line">        dataset_iter = tfe.Iterator(dataset)</span><br><span class="line">        d = dataset_iter.next()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Images</span></span><br><span class="line">    x_batch = d[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Labels</span></span><br><span class="line">    y_batch = tf.cast(d[<span class="number">1</span>], dtype=tf.int64)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the batch loss</span></span><br><span class="line">    batch_loss = loss_fn(logistic_regression, x_batch, y_batch)</span><br><span class="line">    average_loss += batch_loss</span><br><span class="line">    <span class="comment"># Compute the batch accuracy</span></span><br><span class="line">    batch_accuracy = accuracy_fn(logistic_regression, x_batch, y_batch)</span><br><span class="line">    average_acc += batch_accuracy</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Display the initial cost, before optimizing</span></span><br><span class="line">        print(<span class="string">"Initial loss= &#123;:.9f&#125;"</span>.format(average_loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the variables following gradients info</span></span><br><span class="line">    optimizer.apply_gradients(grad(logistic_regression, x_batch, y_batch))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display info</span></span><br><span class="line">    <span class="keyword">if</span> (step + <span class="number">1</span>) % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">            average_loss /= display_step</span><br><span class="line">            average_acc /= display_step</span><br><span class="line">        print(<span class="string">"Step:"</span>, <span class="string">'%04d'</span> % (step + <span class="number">1</span>), <span class="string">" loss="</span>,</span><br><span class="line">              <span class="string">"&#123;:.9f&#125;"</span>.format(average_loss), <span class="string">" accuracy="</span>,</span><br><span class="line">              <span class="string">"&#123;:.4f&#125;"</span>.format(average_acc))</span><br><span class="line">        average_loss = <span class="number">0.</span></span><br><span class="line">        average_acc = <span class="number">0.</span></span><br></pre></td></tr></table></figure><pre><code>Initial loss= 2.302585363Step: 0001  loss= 2.302585363  accuracy= 0.1172Step: 0100  loss= 0.952338576  accuracy= 0.7955Step: 0200  loss= 0.535867393  accuracy= 0.8712Step: 0300  loss= 0.485415280  accuracy= 0.8757Step: 0400  loss= 0.433947176  accuracy= 0.8843Step: 0500  loss= 0.381990731  accuracy= 0.8971Step: 0600  loss= 0.394154936  accuracy= 0.8947Step: 0700  loss= 0.391497582  accuracy= 0.8905Step: 0800  loss= 0.386373132  accuracy= 0.8945Step: 0900  loss= 0.332039326  accuracy= 0.9096Step: 1000  loss= 0.358993709  accuracy= 0.9002</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate model on the test image set</span></span><br><span class="line">testX = mnist.test.images</span><br><span class="line">testY = mnist.test.labels</span><br><span class="line"></span><br><span class="line">test_acc = accuracy_fn(logistic_regression, testX, testY)</span><br><span class="line">print(<span class="string">"Testset Accuracy: &#123;:.4f&#125;"</span>.format(test_acc))</span><br></pre></td></tr></table></figure><pre><code>Testset Accuracy: 0.9083</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://www.cnblogs.com/likethanlove/p/6547405.html" target="_blank" rel="noopener">tensorflow reduction_indices理解</a></p><p>[2] <a href="https://blog.csdn.net/qq575379110/article/details/70538051" target="_blank" rel="noopener">tf.argmax()以及axis解析</a></p><p>[3] <a href="https://blog.csdn.net/kwame211/article/details/78579035" target="_blank" rel="noopener">TensorFlow全新的数据读取方式：Dataset API入门教程</a></p><p>[4] <a href="https://blog.csdn.net/mao_xiao_feng/article/details/53382790" target="_blank" rel="noopener">【TensorFlow】tf.nn.softmax_cross_entropy_with_logits的用法</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow基本模型之线性回归</title>
      <link href="/2018/03/02/DeepLearning/TensorFlow/02Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
      <url>/2018/03/02/DeepLearning/TensorFlow/02Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
      <content type="html"><![CDATA[<h3 id="线性回归简述"><a href="#线性回归简述" class="headerlink" title="线性回归简述"></a>线性回归简述</h3><p>在这里，我们仅仅讨论单变量的线型回归模型。不对回归算法进行过多的展开。重点放在<code>Tensorflow</code>的学习上。<br>下图展示的分别是：单变量线性回归模型的公式；学习的参数；损失函数（采用的均方误差）；目标函数的优化求解（SGD）。</p><p><img src="http://xukeqiniu.xukeai.cn/e6adf19554f7aa930416fa65b1c5ff2b.png" alt=""></p><a id="more"></a><h3 id="Tensorflow-线性回归"><a href="#Tensorflow-线性回归" class="headerlink" title="Tensorflow 线性回归"></a>Tensorflow 线性回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">rng = np.random</span><br></pre></td></tr></table></figure><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数设置</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">1000</span></span><br><span class="line">display_step = <span class="number">50</span></span><br></pre></td></tr></table></figure><h4 id="生成训练数据"><a href="#生成训练数据" class="headerlink" title="生成训练数据"></a>生成训练数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line">train_X = np.asarray([<span class="number">3.3</span>,<span class="number">4.4</span>,<span class="number">5.5</span>,<span class="number">6.71</span>,<span class="number">6.93</span>,<span class="number">4.168</span>,<span class="number">9.779</span>,<span class="number">6.182</span>,<span class="number">7.59</span>,<span class="number">2.167</span>,</span><br><span class="line">                         <span class="number">7.042</span>,<span class="number">10.791</span>,<span class="number">5.313</span>,<span class="number">7.997</span>,<span class="number">5.654</span>,<span class="number">9.27</span>,<span class="number">3.1</span>])</span><br><span class="line">train_Y = np.asarray([<span class="number">1.7</span>,<span class="number">2.76</span>,<span class="number">2.09</span>,<span class="number">3.19</span>,<span class="number">1.694</span>,<span class="number">1.573</span>,<span class="number">3.366</span>,<span class="number">2.596</span>,<span class="number">2.53</span>,<span class="number">1.221</span>,</span><br><span class="line">                         <span class="number">2.827</span>,<span class="number">3.465</span>,<span class="number">1.65</span>,<span class="number">2.904</span>,<span class="number">2.42</span>,<span class="number">2.94</span>,<span class="number">1.3</span>])</span><br><span class="line">n_samples = train_X.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="构造线型回归模型"><a href="#构造线型回归模型" class="headerlink" title="构造线型回归模型"></a>构造线型回归模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf 图的输入</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">Y = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line"><span class="comment"># 设置模型的权重与偏置</span></span><br><span class="line">W = tf.Variable(rng.randn(), name=<span class="string">"weight"</span>)</span><br><span class="line">b = tf.Variable(rng.randn(), name=<span class="string">"bias"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个线性模型</span></span><br><span class="line">pred = tf.add(tf.multiply(X, W), b)</span><br></pre></td></tr></table></figure><h4 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数设置为均方误差</span></span><br><span class="line">cost = tf.reduce_sum(tf.pow(pred-Y, <span class="number">2</span>))/(<span class="number">2</span>*n_samples)</span><br></pre></td></tr></table></figure><h4 id="定义优化方法"><a href="#定义优化方法" class="headerlink" title="定义优化方法"></a>定义优化方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化变量(i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fit all training data</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> zip(train_X, train_Y):</span><br><span class="line">            sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#现实每50轮迭代的结果</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            c = sess.run(cost, feed_dict=&#123;X: train_X, Y:train_Y&#125;)</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(c), \</span><br><span class="line">                <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>, sess.run(b))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">    training_cost = sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Training cost="</span>, training_cost, <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>, sess.run(b), <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#绘图</span></span><br><span class="line">    plt.plot(train_X, train_Y, <span class="string">'ro'</span>, label=<span class="string">'Original data'</span>)</span><br><span class="line">    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=<span class="string">'Fitted line'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><hr><pre><code>Epoch: 0050 cost= 0.160369754 W= 0.41108337 b= -0.36027926Epoch: 0100 cost= 0.150733337 W= 0.40147883 b= -0.2911848Epoch: 0150 cost= 0.142209828 W= 0.39244553 b= -0.22619964Epoch: 0200 cost= 0.134670869 W= 0.38394934 b= -0.16507955Epoch: 0250 cost= 0.128002644 W= 0.37595856 b= -0.10759445Epoch: 0300 cost= 0.122104712 W= 0.36844307 b= -0.05352829Epoch: 0350 cost= 0.116888084 W= 0.3613746 b= -0.0026777028Epoch: 0400 cost= 0.112274118 W= 0.35472643 b= 0.04514854Epoch: 0450 cost= 0.108193211 W= 0.34847358 b= 0.09013041Epoch: 0500 cost= 0.104583815 W= 0.34259278 b= 0.13243689Epoch: 0550 cost= 0.101391472 W= 0.33706158 b= 0.17222734Epoch: 0600 cost= 0.098568030 W= 0.33185956 b= 0.20965117Epoch: 0650 cost= 0.096070863 W= 0.32696673 b= 0.2448493Epoch: 0700 cost= 0.093862340 W= 0.32236505 b= 0.2779539Epoch: 0750 cost= 0.091909051 W= 0.3180369 b= 0.3090903Epoch: 0800 cost= 0.090181611 W= 0.31396636 b= 0.33837357Epoch: 0850 cost= 0.088653855 W= 0.31013775 b= 0.365916Epoch: 0900 cost= 0.087302707 W= 0.3065368 b= 0.39182106Epoch: 0950 cost= 0.086107843 W= 0.30315018 b= 0.41618422Epoch: 1000 cost= 0.085051164 W= 0.29996493 b= 0.43909845Optimization Finished!Training cost= 0.085051164 W= 0.29996493 b= 0.43909845</code></pre><p><img src="http://xukeqiniu.xukeai.cn/2b84f0eb5acb81fa3b14cf10bfdd0fdf.png" alt="拟合曲线"></p><h3 id="Tensorflow-线性回归（Eager-API）"><a href="#Tensorflow-线性回归（Eager-API）" class="headerlink" title="Tensorflow 线性回归（Eager API）"></a>Tensorflow 线性回归（Eager API）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br></pre></td></tr></table></figure><h4 id="设置Eager-API"><a href="#设置Eager-API" class="headerlink" title="设置Eager API"></a>设置Eager API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set Eager API</span></span><br><span class="line">tfe.enable_eager_execution()</span><br></pre></td></tr></table></figure><h4 id="生成训练数据-1"><a href="#生成训练数据-1" class="headerlink" title="生成训练数据"></a>生成训练数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training Data</span></span><br><span class="line">train_X = [<span class="number">3.3</span>, <span class="number">4.4</span>, <span class="number">5.5</span>, <span class="number">6.71</span>, <span class="number">6.93</span>, <span class="number">4.168</span>, <span class="number">9.779</span>, <span class="number">6.182</span>, <span class="number">7.59</span>, <span class="number">2.167</span>,</span><br><span class="line">           <span class="number">7.042</span>, <span class="number">10.791</span>, <span class="number">5.313</span>, <span class="number">7.997</span>, <span class="number">5.654</span>, <span class="number">9.27</span>, <span class="number">3.1</span>]</span><br><span class="line">train_Y = [<span class="number">1.7</span>, <span class="number">2.76</span>, <span class="number">2.09</span>, <span class="number">3.19</span>, <span class="number">1.694</span>, <span class="number">1.573</span>, <span class="number">3.366</span>, <span class="number">2.596</span>, <span class="number">2.53</span>, <span class="number">1.221</span>,</span><br><span class="line">           <span class="number">2.827</span>, <span class="number">3.465</span>, <span class="number">1.65</span>, <span class="number">2.904</span>, <span class="number">2.42</span>, <span class="number">2.94</span>, <span class="number">1.3</span>]</span><br><span class="line">n_samples = len(train_X)</span><br></pre></td></tr></table></figure><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">display_step = <span class="number">100</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br></pre></td></tr></table></figure><h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Weight and Bias</span></span><br><span class="line">W = tfe.Variable(np.random.randn())</span><br><span class="line">b = tfe.Variable(np.random.randn())</span><br></pre></td></tr></table></figure><h4 id="构建线性回归模型"><a href="#构建线性回归模型" class="headerlink" title="构建线性回归模型"></a>构建线性回归模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Linear regression (Wx + b)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> inputs * W + b</span><br></pre></td></tr></table></figure><h4 id="定义损失函数（均方误差）"><a href="#定义损失函数（均方误差）" class="headerlink" title="定义损失函数（均方误差）"></a>定义损失函数（均方误差）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mean square error</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_square_fn</span><span class="params">(model_fn, inputs, labels)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_sum(tf.pow(model_fn(inputs) - labels, <span class="number">2</span>)) / (<span class="number">2</span> * n_samples)</span><br></pre></td></tr></table></figure><h4 id="调用优化器（SGD）"><a href="#调用优化器（SGD）" class="headerlink" title="调用优化器（SGD）"></a>调用优化器（SGD）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGD Optimizer</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute gradients</span></span><br><span class="line">grad = tfe.implicit_gradients(mean_square_fn)</span><br></pre></td></tr></table></figure><h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initial cost, before optimizing</span></span><br><span class="line">print(<span class="string">"Initial cost= &#123;:.9f&#125;"</span>.format(</span><br><span class="line">    mean_square_fn(linear_regression, train_X, train_Y)),</span><br><span class="line">    <span class="string">"W="</span>, W.numpy(), <span class="string">"b="</span>, b.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line"></span><br><span class="line">    optimizer.apply_gradients(grad(linear_regression, train_X, train_Y))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (step + <span class="number">1</span>) % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (step + <span class="number">1</span>), <span class="string">"cost="</span>,</span><br><span class="line">              <span class="string">"&#123;:.9f&#125;"</span>.format(mean_square_fn(linear_regression, train_X, train_Y)),</span><br><span class="line">              <span class="string">"W="</span>, W.numpy(), <span class="string">"b="</span>, b.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Graphic display</span></span><br><span class="line">plt.plot(train_X, train_Y, <span class="string">'ro'</span>, label=<span class="string">'Original data'</span>)</span><br><span class="line">plt.plot(train_X, np.array(W * train_X + b), label=<span class="string">'Fitted line'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><pre><code>Initial cost= 34.570991516 W= -0.89701766 b= 0.09529222Epoch: 0001 cost= 10.462613106 W= -0.3445475 b= 0.17387688Epoch: 0100 cost= 0.090614140 W= 0.31795835 b= 0.32859808Epoch: 0200 cost= 0.087662779 W= 0.31037292 b= 0.38237545Epoch: 0300 cost= 0.085347883 W= 0.30365503 b= 0.4300023Epoch: 0400 cost= 0.083532237 W= 0.29770544 b= 0.472182Epoch: 0500 cost= 0.082108125 W= 0.29243633 b= 0.5095375Epoch: 0600 cost= 0.080991186 W= 0.28776988 b= 0.54262066Epoch: 0700 cost= 0.080115080 W= 0.28363708 b= 0.5719204Epoch: 0800 cost= 0.079427943 W= 0.27997696 b= 0.5978689Epoch: 0900 cost= 0.078888975 W= 0.27673545 b= 0.6208498Epoch: 1000 cost= 0.078466244 W= 0.27386472 b= 0.64120203</code></pre><p><img src="http://xukeqiniu.xukeai.cn/f0cd930820fa093c5b66e660710f6f77.png" alt="拟合曲线"></p><p><img src="http://xukeqiniu.xukeai.cn/1b2eecc284aec268c9f7cc1907376184.png" alt="Logistic模型"></p><p><img src="http://xukeqiniu.xukeai.cn/65de6987629cad90b435ca677a87b5a5.png" alt="Logistic模型图解"></p><p><img src="http://xukeqiniu.xukeai.cn/46bfcfb36ca46ba0b0c62d2ef98d9886.png" alt="交叉熵"></p><p><img src="http://xukeqiniu.xukeai.cn/7aa15bacea22f7e5f58bf50ae9faf7cb.png" alt="softmax"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">TensorFlow-Examples</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Tensorflow基础入门</title>
      <link href="/2018/03/01/DeepLearning/TensorFlow/01Tensorflow%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/"/>
      <url>/2018/03/01/DeepLearning/TensorFlow/01Tensorflow%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<h3 id="MNIST-数据集入门"><a href="#MNIST-数据集入门" class="headerlink" title="MNIST 数据集入门"></a>MNIST 数据集入门</h3><h4 id="MNIST-数据集简介"><a href="#MNIST-数据集简介" class="headerlink" title="MNIST 数据集简介"></a>MNIST 数据集简介</h4><p><img src="http://xukeqiniu.xukeai.cn/ed887cc7ed9a38b82885e9ed75d0d8f5.png" alt=""><br>数字手写体识别数据集，常用来作为Deep Learning入门的基础数据集。它有<code>60000</code>个训练样本集和<code>10000</code>个测试样本集，每个样本图像的宽高为 <code>28 * 28</code>。此数据集是以二进制存储的，不能直接以图像格式查看。</p><p>数据集大小：~12MB</p><p>下载地址：<a href="http://yann.lecun.com/exdb/mnist/index.html" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/index.html</a></p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br></pre></td></tr></table></figure><hr><pre><code>1.6.0</code></pre><h4 id="tensorflow加载MNIST数据集"><a href="#tensorflow加载MNIST数据集" class="headerlink" title="tensorflow加载MNIST数据集"></a>tensorflow加载MNIST数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"../../data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load data</span></span><br><span class="line">X_train = mnist.train.images</span><br><span class="line">Y_train = mnist.train.labels</span><br><span class="line">X_test = mnist.test.images</span><br><span class="line">Y_test = mnist.test.labels</span><br></pre></td></tr></table></figure><hr><pre><code>train-images-idx3-ubyte.gz:  training set images (9912422 bytes)train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)Extracting ../../data/train-images-idx3-ubyte.gzExtracting ../../data/train-labels-idx1-ubyte.gzExtracting ../../data/t10k-images-idx3-ubyte.gzExtracting ../../data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="查看并可视化MNIST数据集"><a href="#查看并可视化MNIST数据集" class="headerlink" title="查看并可视化MNIST数据集"></a>查看并可视化MNIST数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># Get the next 64 images array and labels</span></span><br><span class="line">batch_X, batch_Y = mnist.train.next_batch(<span class="number">64</span>)</span><br><span class="line">print(batch_X.astype,batch_X.shape) <span class="comment"># (64,28*28)</span></span><br><span class="line">print(batch_Y.astype,batch_Y.shape) <span class="comment"># (64,10[0-9哪一类])</span></span><br><span class="line">print(batch_Y[<span class="number">0</span>]) <span class="comment"># [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]</span></span><br><span class="line">plt.imshow(np.reshape(batch_X[<span class="number">0</span>], [<span class="number">28</span>, <span class="number">28</span>]), cmap=<span class="string">'gray'</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>&lt;built-in method astype of numpy.ndarray object at 0x0000016EEF769080&gt; (64, 784)&lt;built-in method astype of numpy.ndarray object at 0x0000016EEF772620&gt; (64, 10)[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]&lt;matplotlib.image.AxesImage at 0x16eef82a630&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/tf_01_output_8_2.png" alt="png"></p><h3 id="tensorflow入门（hello-world）"><a href="#tensorflow入门（hello-world）" class="headerlink" title="tensorflow入门（hello world）"></a>tensorflow入门（hello world）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TensorFlow 实现简单的 hello world</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个常量操作</span></span><br><span class="line"><span class="comment"># 这个常量操作会在默认的图中添加一个节点</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 构造函数返回的值表示常量op的输出。</span></span><br><span class="line">hello = tf.constant(<span class="string">'Hello, TensorFlow!'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 tf session</span></span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行图</span></span><br><span class="line">print(sess.run(hello))</span><br></pre></td></tr></table></figure><hr><pre><code>b&apos;Hello, TensorFlow!&apos;</code></pre><h3 id="tensorflow入门（基本操作）"><a href="#tensorflow入门（基本操作）" class="headerlink" title="tensorflow入门（基本操作）"></a>tensorflow入门（基本操作）</h3><h4 id="常量操作"><a href="#常量操作" class="headerlink" title="常量操作"></a>常量操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 常量基本操作</span></span><br><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动默认的图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"a: %i"</span> % sess.run(a), <span class="string">"b: %i"</span> % sess.run(b))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Addition with constants: %i"</span> % sess.run(a+b))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Multiplication with constants: %i"</span> % sess.run(a*b))</span><br></pre></td></tr></table></figure><hr><pre><code>a: 2 b: 3Addition with constants: 5Multiplication with constants: 6</code></pre><h4 id="变量操作"><a href="#变量操作" class="headerlink" title="变量操作"></a>变量操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 作为图输入变量的基本操作。</span></span><br><span class="line">a = tf.placeholder(tf.int16)</span><br><span class="line">b = tf.placeholder(tf.int16)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf 中定义的操作</span></span><br><span class="line">add = tf.add(a, b) <span class="comment">#加法操作</span></span><br><span class="line">mul = tf.multiply(a, b) <span class="comment">#乘法操作</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动默认的图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 使用变量输入运行每个操作。</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Addition with variables: %i"</span> % sess.run(add, feed_dict=&#123;a: <span class="number">2</span>, b: <span class="number">3</span>&#125;))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Multiplication with variables: %i"</span> % sess.run(mul, feed_dict=&#123;a: <span class="number">2</span>, b: <span class="number">3</span>&#125;))</span><br></pre></td></tr></table></figure><hr><pre><code>Addition with variables: 5Multiplication with variables: 6</code></pre><h4 id="矩阵操作"><a href="#矩阵操作" class="headerlink" title="矩阵操作"></a>矩阵操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个生成1x2矩阵的常数op。</span></span><br><span class="line">matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line"><span class="comment"># 创建另一个常数，生成一个2x1矩阵。</span></span><br><span class="line">matrix2 = tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf 中定义的操作</span></span><br><span class="line">product = tf.matmul(matrix1, matrix2) <span class="comment">#矩阵乘法操作</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(product)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure><hr><pre><code>[[ 12.]]</code></pre><h3 id="tensorflow入门（Eager-API）"><a href="#tensorflow入门（Eager-API）" class="headerlink" title="tensorflow入门（Eager API）"></a>tensorflow入门（Eager API）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置 Eager API</span></span><br><span class="line">print(<span class="string">"Setting Eager mode..."</span>)</span><br><span class="line">tfe.enable_eager_execution()</span><br></pre></td></tr></table></figure><hr><pre><code>Setting Eager mode...</code></pre><h4 id="Eager-API-常量操作"><a href="#Eager-API-常量操作" class="headerlink" title="Eager API 常量操作"></a>Eager API 常量操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义常量 tensors</span></span><br><span class="line">print(<span class="string">"Define constant tensors"</span>)</span><br><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">print(<span class="string">"a = %i"</span> % a)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br><span class="line">print(<span class="string">"b = %i"</span> % b)</span><br></pre></td></tr></table></figure><hr><pre><code>Define constant tensorsa = 2b = 3</code></pre><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行操作不需要 tf.Session</span></span><br><span class="line">print(<span class="string">"Running operations, without tf.Session"</span>)</span><br><span class="line">c = a + b</span><br><span class="line">print(<span class="string">"a + b = %i"</span> % c)</span><br><span class="line">d = a * b</span><br><span class="line">print(<span class="string">"a * b = %i"</span> % d)</span><br></pre></td></tr></table></figure><hr><pre><code>Running operations, without tf.Sessiona + b = 5a * b = 6</code></pre><h4 id="Eager-API-张量操作"><a href="#Eager-API-张量操作" class="headerlink" title="Eager API 张量操作"></a>Eager API 张量操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 与 Numpy完全兼容</span></span><br><span class="line">print(<span class="string">"Mixing operations with Tensors and Numpy Arrays"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define constant tensors</span></span><br><span class="line">a = tf.constant([[<span class="number">2.</span>, <span class="number">1.</span>],</span><br><span class="line">                 [<span class="number">1.</span>, <span class="number">0.</span>]], dtype=tf.float32)</span><br><span class="line">print(<span class="string">"Tensor:\n a = %s"</span> % a)</span><br><span class="line">b = np.array([[<span class="number">3.</span>, <span class="number">0.</span>],</span><br><span class="line">              [<span class="number">5.</span>, <span class="number">1.</span>]], dtype=np.float32)</span><br><span class="line">print(<span class="string">"NumpyArray:\n b = %s"</span> % b)</span><br></pre></td></tr></table></figure><hr><pre><code>Mixing operations with Tensors and Numpy ArraysTensor: a = tf.Tensor([[2. 1.] [1. 0.]], shape=(2, 2), dtype=float32)NumpyArray: b = [[3. 0.] [5. 1.]]</code></pre><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在不需要 tf.Session 的情况下运行该操作</span></span><br><span class="line">print(<span class="string">"Running operations, without tf.Session"</span>)</span><br><span class="line"></span><br><span class="line">c = a + b</span><br><span class="line">print(<span class="string">"a + b = %s"</span> % c)</span><br><span class="line"></span><br><span class="line">d = tf.matmul(a, b)</span><br><span class="line">print(<span class="string">"a * b = %s"</span> % d)</span><br></pre></td></tr></table></figure><hr><pre><code>Running operations, without tf.Sessiona + b = tf.Tensor([[5. 1.] [6. 1.]], shape=(2, 2), dtype=float32)a * b = tf.Tensor([[11.  1.] [ 3.  0.]], shape=(2, 2), dtype=float32)</code></pre><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 遍历张量</span></span><br><span class="line">print(<span class="string">"Iterate through Tensor 'a':"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(a.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(a.shape[<span class="number">1</span>]):</span><br><span class="line">        print(a[i][j])</span><br></pre></td></tr></table></figure><hr><pre><code>Iterate through Tensor &apos;a&apos;:tf.Tensor(2.0, shape=(), dtype=float32)tf.Tensor(1.0, shape=(), dtype=float32)tf.Tensor(1.0, shape=(), dtype=float32)tf.Tensor(0.0, shape=(), dtype=float32)</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">TensorFlow-Examples</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之反向传播算法</title>
      <link href="/2018/02/06/DeepLearning/caffe/solver/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/"/>
      <url>/2018/02/06/DeepLearning/caffe/solver/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<h3 id="前向传播与反向传播"><a href="#前向传播与反向传播" class="headerlink" title="前向传播与反向传播"></a>前向传播与反向传播</h3><ul><li>反向传播（back-propagation）是<strong>计算深度学习模型参数梯度的方法</strong>。总的来说，反向传播中会依据微积分中的链式法则，按照输出层、靠近输出层的隐含层、靠近输入层的隐含层和输入层的次序，依次<strong>计算并存储模型损失函数有关模型各层的中间变量和参数的梯度</strong>。反向传播回传误差（只在训练过程中实现）</li><li>反向传播对于各层中变量和参数的梯度计算可能会依赖各层变量和参数的当前值。对深度学习模型按照输入层、靠近输入层的隐含层、靠近输出层的隐含层和输出层的次序，<strong>依次计算并存储模型的中间变量</strong>叫做正向传播（forward-propagation）。前向传播求损失（训练与测试均需要）</li></ul><h3 id="反向传播公式推导"><a href="#反向传播公式推导" class="headerlink" title="反向传播公式推导"></a>反向传播公式推导</h3><p>在这里，我们先按照周志华《机器学习》的反向传播理解思路进行公式推导，对后面深入理解深度神经网络中的反向传播预热！<br><img src="http://xukeqiniu.xukeai.cn/399eb3c02fa3b4c350130156c7658294.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/bb4665c5f902894e9a8becc2ecde11db.png" alt=""></p><a id="more"></a><h3 id="深入理解反向传播"><a href="#深入理解反向传播" class="headerlink" title="深入理解反向传播"></a>深入理解反向传播</h3><h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>计算图的三要素：节点，连接线，操作</p><ul><li>节点：用于表示变量，变量可以是标量，矢量，向量，矩阵等</li><li>连接线：用于连通节点</li><li>操作：一个或多个变量的简单函数</li></ul><p>举例说明计算图的表示方式<br><img src="http://xukeqiniu.xukeai.cn/88e2db7ccac6aa788f50dd210f34d5df.png" alt=""></p><h4 id="链式法则与雅可比矩阵"><a href="#链式法则与雅可比矩阵" class="headerlink" title="链式法则与雅可比矩阵"></a>链式法则与雅可比矩阵</h4><ul><li>单项链式法则<br><img src="http://xukeqiniu.xukeai.cn/21d97a44a0190718d5b789af29cb3daa.png" alt=""></li><li>多项链式法则<br><img src="http://xukeqiniu.xukeai.cn/c81962c16c13eb129e6cc278cce94163.png" alt=""></li><li>雅可比矩阵<br><img src="http://xukeqiniu.xukeai.cn/608220d04dab0d8ec84707e30aeca7d0.png" alt=""></li><li>高维链式法则表示<br><img src="http://xukeqiniu.xukeai.cn/e5d82c5862acb8b1b9a192fe4d3bc145.png" alt=""></li></ul><h4 id="反向传播的优点"><a href="#反向传播的优点" class="headerlink" title="反向传播的优点"></a>反向传播的优点</h4><p>我们通过计算图结合链式法则举例说明反向传播在计算上的优势：<br><img src="http://xukeqiniu.xukeai.cn/d5db3e9154c95e2abd5cb6518a91c74b.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/be949eccfe5585a40de6a225a814bb2d.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/5dd91e4d699e2510fa7620de85dccaa5.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/2feecb3a122bdee266dbe1bb213d130b.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/7ef796877aed43c8b29b2940b4c7623b.png" alt=""></p><h4 id="反向传播与梯度下降的关系"><a href="#反向传播与梯度下降的关系" class="headerlink" title="反向传播与梯度下降的关系"></a>反向传播与梯度下降的关系</h4><p>有很多人会将反向传播与梯度下降算法混为一谈，但其实两者是可以分离的，我们独立出两个概念来讨论问题会使得学习的重点更加清晰！</p><ul><li><strong>反向传播仅仅是计算梯度的算法</strong></li><li><strong>梯度下降是使用该梯度进行学习的算法</strong></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>反向传播与梯度下降总结图示：<br><img src="http://xukeqiniu.xukeai.cn/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%9B%BE.gif" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>李宏毅机器学习课程<br>Deep Learning Book<br><a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="noopener">Principles of training multi-layer neural network using backpropagation</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> Solver </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之优化算法总结</title>
      <link href="/2018/02/05/DeepLearning/caffe/solver/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
      <url>/2018/02/05/DeepLearning/caffe/solver/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h3 id="常见优化算法总结"><a href="#常见优化算法总结" class="headerlink" title="常见优化算法总结"></a>常见优化算法总结</h3><p>前面我们介绍了卷积神经网络中主流的数据层，卷积层，全连接层，池化层，激活函数层，归一化层，dropout层,softmax层。分析每一层的配置及意义的目的主要是为了便于<strong>设计出适合自己的网络</strong>。然后<strong>根据自己的任务需要定义合适的损失函数</strong>。当搭建出自己的网络并确定网络的损失函数后，下一个关键问题便是<strong>训练网络</strong>，训练网络的前提需要<strong>确定优化算法</strong>。下面我们针对常见的深度学习优化算法进行梳理：</p><a id="more"></a><h4 id="SGD算法"><a href="#SGD算法" class="headerlink" title="SGD算法"></a>SGD算法</h4><p>介绍SGD算法之前，我们先通过一维与多维梯度下降理解其思想，然后介绍随机梯度下降，最后我们介绍工程中最常用的小批量随机梯度下降并给出对应的代码。</p><h5 id="一维梯度下降"><a href="#一维梯度下降" class="headerlink" title="一维梯度下降"></a>一维梯度下降</h5><p><img src="http://xukeqiniu.xukeai.cn/65dbdfa085340dcde0ba02da9d0f87f7.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/55d610dbc50ad72a7b827c86d5aa4354.png" alt=""></p><h5 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h5><p>上述梯度下降算法中的η（取正数）叫做学习率或步长。需要注意的是，学习率过大可能会造成x迈过（<code>overshoot</code>）最优解，甚至不断发散而无法收敛，如下图所示。<br><img src="http://xukeqiniu.xukeai.cn/5a78d56e946d762fd68e70c441c6988f.png" alt=""></p><p>TODO<br>学习率的另一种理解方式：</p><h5 id="多维梯度下降"><a href="#多维梯度下降" class="headerlink" title="多维梯度下降"></a>多维梯度下降</h5><p><img src="http://xukeqiniu.xukeai.cn/e3f0732575947860357302c62eecc57f.png" alt=""></p><h5 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h5><p><img src="http://xukeqiniu.xukeai.cn/876f975c956ab16ba142fc51c9fc3607.png" alt=""></p><h5 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h5><p><img src="http://xukeqiniu.xukeai.cn/4b9ce4b4518b7372d8a563d265fa471d.png" alt=""></p><h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 小批量随机梯度下降。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(params, lr, batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - lr * param.grad / batch_size</span><br></pre></td></tr></table></figure><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>简而言之，<strong>批量梯度下降针对的是整个数据集，随机梯度下降针对的是当前样本点，而小批量随机梯度下降针对的是一个<code>batch</code>样本数据</strong>。梯度下降<strong>计算量大</strong>，因为受到过多数据的牵制，<strong>收敛曲线对数据不敏感，不容易跳出陷入的局部极小点</strong>。随机梯度下降<strong>计算量小</strong>，但是只是针对当前一个样本数据，目光狭隘，<strong>受数据影响过大导致收敛曲线震荡过于严重</strong>。小批量梯度下降则是前面两者的中庸。<strong>计算量取决于<code>batch</code>的大小</strong>，对<strong>批量的数据有一定的敏感性</strong>，收敛具有一定的<strong>震荡性</strong>，使得算法在搜索的过程中具有<strong>跳出当前局部极小点的潜力</strong>。这样，对于搜索到更好的局部极小点有帮助。</p><h4 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h4><h5 id="梯度下降的问题"><a href="#梯度下降的问题" class="headerlink" title="梯度下降的问题"></a>梯度下降的问题</h5><p><img src="http://xukeqiniu.xukeai.cn/95640be7b1e4e7e475a3d7a8667c05fb.png" alt=""><br>上图中，红色三角形代表参数x的初始值。带箭头的线段表示每次迭代时参数的更新。由于目标函数在竖直方向（x2轴方向）上比在水平方向（x1轴方向）弯曲得更厉害，梯度下降迭代参数时会使参数在竖直方向比在水平方向移动更猛烈。因此，我们需要一个较小的学习率从而避免参数在竖直方向上<code>overshoot</code>。这就造成了上图中<strong>参数向最优解移动速度的缓慢</strong>。</p><h5 id="动量法思想"><a href="#动量法思想" class="headerlink" title="动量法思想"></a>动量法思想</h5><p><img src="http://xukeqiniu.xukeai.cn/f5009b2981de20beb31a1bb1ffbfbfa2.png" alt=""><br>动量法的核心思想是：<strong>每次参数的迭代更新都吸收一部分上次更新的余势</strong>。使得迭代的速度可以明显的加快，如下图所示：<br><img src="http://xukeqiniu.xukeai.cn/26b6e957c5f418b7cadfea4cc4e5bfdb.png" alt=""></p><h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动量法。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(params, vs, lr, mom, batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> param, v <span class="keyword">in</span> zip(params, vs):</span><br><span class="line">        v[:] = mom * v + lr * param.grad / batch_size</span><br><span class="line">        param[:] -= v</span><br></pre></td></tr></table></figure><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><h5 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h5><p><img src="http://xukeqiniu.xukeai.cn/3cf30634a129d656760cf7e1f8b5febf.png" alt=""></p><h5 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h5><p>自动调整学习率，调整的思路是：在学习率的基础上除以一个<code>S</code>开方,而<code>S</code>取决于计算的小批量梯度<code>g</code>,按元素平方后累加。这种自动更新学习率的方式，<strong>对低频出现的参数进行大的更新，而对高频出现的参数进行小的更新</strong>。因此，该方法适用于<strong>稀疏</strong>的数据。</p><h5 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Adagrad算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad</span><span class="params">(params, sqrs, lr, batch_size)</span>:</span></span><br><span class="line">    eps_stable = <span class="number">1e-7</span></span><br><span class="line">    <span class="keyword">for</span> param, sqr <span class="keyword">in</span> zip(params, sqrs):</span><br><span class="line">        g = param.grad / batch_size</span><br><span class="line">        sqr[:] += nd.square(g)</span><br><span class="line">        div = lr * g / nd.sqrt(sqr + eps_stable)</span><br><span class="line">        param[:] -= div</span><br></pre></td></tr></table></figure><h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><h5 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h5><p><img src="http://xukeqiniu.xukeai.cn/3eee01c093511742569249b171db46e6.png" alt=""></p><h5 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h5><p><code>RMSProp</code>为了解决<strong>Adagrad学习率不断单调下降的问题</strong><br>实现的方式非常简单，<code>RMSProp</code>只在<code>Adagrad</code>的基础上修改了变量<code>S</code>的更新方法：<strong>把累加改成了指数加权移动平均</strong>。因此，<strong>每个元素的学习率在迭代过程中既可能降低又可能升高</strong>。</p><h5 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># RMSProp</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(params, sqrs, lr, gamma, batch_size)</span>:</span></span><br><span class="line">    eps_stable = <span class="number">1e-8</span></span><br><span class="line">    <span class="keyword">for</span> param, sqr <span class="keyword">in</span> zip(params, sqrs):</span><br><span class="line">        g = param.grad / batch_size</span><br><span class="line">        sqr[:] = gamma * sqr + (<span class="number">1.</span> - gamma) * nd.square(g)</span><br><span class="line">        div = lr * g / nd.sqrt(sqr + eps_stable)</span><br><span class="line">        param[:] -= div</span><br></pre></td></tr></table></figure><h4 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h4><h5 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h5><p><img src="http://xukeqiniu.xukeai.cn/8bbc777e93a7ddb577dc6c9c77913f7e.png" alt=""></p><h5 id="思想-2"><a href="#思想-2" class="headerlink" title="思想"></a>思想</h5><p><strong>为解决Adagrad学习率不断单调下降的问题</strong>与<code>RMSProp</code>相同采用指数加权移动平均代替累加。在此基础上，<code>AdaDelta</code><strong>舍弃了学习率参数，完全由输入数据进行决定</strong>！</p><h5 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Adadalta</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adadelta</span><span class="params">(params, sqrs, deltas, rho, batch_size)</span>:</span></span><br><span class="line">    eps_stable = <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">for</span> param, sqr, delta <span class="keyword">in</span> zip(params, sqrs, deltas):</span><br><span class="line">        g = param.grad / batch_size</span><br><span class="line">        sqr[:] = rho * sqr + (<span class="number">1.</span> - rho) * nd.square(g)</span><br><span class="line">        cur_delta = nd.sqrt(delta + eps_stable) / nd.sqrt(sqr + eps_stable) * g</span><br><span class="line">        delta[:] = rho * delta + (<span class="number">1.</span> - rho) * cur_delta * cur_delta</span><br><span class="line">        param[:] -= cur_delta</span><br></pre></td></tr></table></figure><h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><h5 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h5><p><img src="http://xukeqiniu.xukeai.cn/b4a213eb1cc51b3376a2066bb7fb0dfc.png" alt=""></p><h5 id="思想-3"><a href="#思想-3" class="headerlink" title="思想"></a>思想</h5><p>结合了<code>RMSProp</code>与<code>动量法</code>的思想。<strong>同时利用了更新梯度的一阶矩和二阶矩</strong>，并紧接着做了<strong>偏差修正</strong>使得<code>估计无偏</code>。</p><h5 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Adam</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(params, vs, sqrs, lr, batch_size, t)</span>:</span></span><br><span class="line">    beta1 = <span class="number">0.9</span></span><br><span class="line">    beta2 = <span class="number">0.999</span></span><br><span class="line">    eps_stable = <span class="number">1e-8</span></span><br><span class="line">    <span class="keyword">for</span> param, v, sqr <span class="keyword">in</span> zip(params, vs, sqrs):</span><br><span class="line">        g = param.grad / batch_size</span><br><span class="line">        v[:] = beta1 * v + (<span class="number">1.</span> - beta1) * g</span><br><span class="line">        sqr[:] = beta2 * sqr + (<span class="number">1.</span> - beta2) * nd.square(g)</span><br><span class="line">        v_bias_corr = v / (<span class="number">1.</span> - beta1 ** t)</span><br><span class="line">        sqr_bias_corr = sqr / (<span class="number">1.</span> - beta2 ** t)</span><br><span class="line">        div = lr * v_bias_corr / (nd.sqrt(sqr_bias_corr) + eps_stable)</span><br><span class="line">        param[:] = param - div</span><br></pre></td></tr></table></figure><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p><img src="http://xukeqiniu.xukeai.cn/e2279fbf1b9e6bd53dc5093c156179b4.png" alt=""><br>需要注意的是：我们梳理优化算法的过程中发现，除了SGD外，还有很多高级的优化算法，他们可以自学习学习率，对我们调整参数来说提供了一定的便利性。但是为什么深度学习调参的过程中还是大量使用SGD算法呢？我总结的原因是SGD算法虽然简单但是灵活可以调整的超参数较多，除了算法本身，我们还有一些手动调整学习率的方法。因此<strong>针对不同的问题，采用手调的SGD往往结果要比一些高级的优化算法效果还要好</strong>，原因是SGD通过手调参数可以做到不同问题，不同分析应对，因此，我们还是要将SGD算法做深入的了解，并在实战中学习调整参数的方法与经验！</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>An overview of gradient descent optimization algorithms<br><a href="https://blog.slinuxer.com/2016/09/sgd-comparison" target="_blank" rel="noopener">SGD算法比较</a><br><a href="http://zh.gluon.ai/chapter_optimization/index.html" target="_blank" rel="noopener">动手学习深度学习——优化算法</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> Solver </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之Solver文件配置</title>
      <link href="/2018/02/04/DeepLearning/caffe/solver/solver/"/>
      <url>/2018/02/04/DeepLearning/caffe/solver/solver/</url>
      <content type="html"><![CDATA[<h3 id="solver配置说明"><a href="#solver配置说明" class="headerlink" title="solver配置说明"></a>solver配置说明</h3><blockquote><p>前面我们介绍了卷积神经网络中主流的数据层，卷积层，全连接层，池化层，激活函数层，归一化层，dropout层,softmax层。分析每一层的配置及意义的目的主要是为了便于<strong>设计出适合自己的网络</strong>。然后<strong>根据自己的任务需要定义合适的损失函数</strong>。当搭建出自己的网络并确定网络的损失函数后，下一个关键问题便是<strong>训练网络</strong>，下面我们将重点集中到训练网络的过程。</p></blockquote><a id="more"></a><p><code>caffe</code>中关于训练网络中最为重要的一个文件便是<code>solver</code>配置文件。我们首先看一看<code>lenet</code>中定义的<code>solver</code>配置文件</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">net: <span class="string">"examples/mnist/lenet_train_test.prototxt"</span>  #网络位置</span><br><span class="line"></span><br><span class="line">test_iter: <span class="number">100</span> #设置测试迭代次数</span><br><span class="line">test_interval: <span class="number">500</span> #测试间隔。也就是每训练<span class="number">500</span>次，才进行一次测试。</span><br><span class="line">base_lr: <span class="number">0.01</span> #用于设置基础学习率</span><br><span class="line">momentum: <span class="number">0.9</span> #动量大小</span><br><span class="line">type: SGD     #SGD优化算法</span><br><span class="line">weight_decay: <span class="number">0.0005</span>  # 权重衰减项（正则化项），防止过拟合的一个参数λ</span><br><span class="line">lr_policy: <span class="string">"inv"</span> #学习率调整策略</span><br><span class="line">gamma: <span class="number">0.0001</span></span><br><span class="line">power: <span class="number">0.75</span></span><br><span class="line">display: <span class="number">100</span></span><br><span class="line">max_iter: <span class="number">20000</span>  # 最大迭代次数。这个数设置太小，会导致没有收敛，精确度很低。设置太大，会导致震荡，浪费时间。</span><br><span class="line">snapshot: <span class="number">5000</span> #设置快照</span><br><span class="line">snapshot_prefix: <span class="string">"examples/mnist/lenet"</span> #设置保存快照的位置</span><br><span class="line">solver_mode: CPU #选择训练模式</span><br></pre></td></tr></table></figure><h4 id="设置网络定义文件位置"><a href="#设置网络定义文件位置" class="headerlink" title="设置网络定义文件位置"></a>设置网络定义文件位置</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net: <span class="string">"examples/mnist/lenet_train_test.prototxt"</span></span><br></pre></td></tr></table></figure><p>设置网络定义文件位置，也可以分别设定<code>train</code>和<code>test</code>如下所示：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_net: <span class="string">"examples/hdf5_classification/logreg_auto_train.prototxt"</span></span><br><span class="line">test_net: <span class="string">"examples/hdf5_classification/logreg_auto_test.prototxt"</span></span><br></pre></td></tr></table></figure><h4 id="设置测试迭代次数"><a href="#设置测试迭代次数" class="headerlink" title="设置测试迭代次数"></a>设置测试迭代次数</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_iter: <span class="number">100</span></span><br></pre></td></tr></table></figure><p>这个要与<code>test layer</code>中的<code>batch_size</code>结合起来理解。<code>mnist</code>数据中测试样本总数为10000，一次性执行全部数据效率很低，因此我们将测试数据分成几个批次（<code>batch_num</code>）来执行，每个批次的数量就是<code>batch_size</code>。假设我们设置<code>batch_size</code>为100，则需要迭代100次才能将10000个数据全部执行完。因此<code>test_iter</code>设置为100。执行完一次全部数据，称之为一个<code>epoch</code>。<br>总结</p><ul><li><p>当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个<code>epoch</code>。一般情况下在迭代的过程中需要使用多次<code>epoch</code>防止模型欠拟合。</p></li><li><p>在不能将数据一次性通过神经网络的时候，就需要将数据集分成几个<code>batch</code>（Number of batches,简记为<code>batch_num</code>）。</p></li><li><p>一个 <code>batch</code> 中的样本总数(Batch Size,简记为<code>batch_size</code>)</p></li><li><p><code>Iteration</code>是 <code>batch</code> 需要完成一个 <code>epoch</code> 的次数。记住：在一个 <code>epoch</code> 中，<code>batch_num</code> 和<code>Iteration</code>是相等的。比如对于一个有 2000 个训练样本的数据集。将 2000 个样本分成4个大小为 500 的 <code>batch_size</code>，那么完成一个 <code>epoch</code> 需要 4 个 <code>iteration</code>,对应的<code>batch_num</code>也是4。</p></li></ul><h4 id="设置测试间隔"><a href="#设置测试间隔" class="headerlink" title="设置测试间隔"></a>设置测试间隔</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_interval: <span class="number">500</span></span><br></pre></td></tr></table></figure><p>测试间隔。也就是每训练500次，才进行一次测试。</p><h4 id="设置优化算法类型"><a href="#设置优化算法类型" class="headerlink" title="设置优化算法类型"></a>设置优化算法类型</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type: SGD</span><br></pre></td></tr></table></figure><p>常见的优化算法包括：SGD、Adagrad、RMSProp、AdaDelta、Adam等。组织结构如下图：<br><img src="http://xukeqiniu.xukeai.cn/e2279fbf1b9e6bd53dc5093c156179b4.png" alt=""><br>上图取自SGD算法比较</p><h4 id="设置动量系数"><a href="#设置动量系数" class="headerlink" title="设置动量系数"></a>设置动量系数</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">momentum ：<span class="number">0.9</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/46c223d32d8fe9e657692499fdadddab.png" alt=""><br>动量系数是设置r的大小</p><h4 id="设置权重衰减系数"><a href="#设置权重衰减系数" class="headerlink" title="设置权重衰减系数"></a>设置权重衰减系数</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight_decay: <span class="number">0.0005</span></span><br></pre></td></tr></table></figure><p>$ w_i = w_i - \eta \frac{\partial L}{\partial w_i} - \eta \lambda w_i$ , $\eta \lambda w_i$为正则化项，weight_decay为正则化项的参数$\lambda $的大小。</p><h4 id="设置显示周期"><a href="#设置显示周期" class="headerlink" title="设置显示周期"></a>设置显示周期</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display: <span class="number">100</span></span><br></pre></td></tr></table></figure><p>每训练100次，在屏幕上显示一次。如果设置为0，则不显示。</p><h4 id="设置最大迭代次数"><a href="#设置最大迭代次数" class="headerlink" title="设置最大迭代次数"></a>设置最大迭代次数</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">max_iter: <span class="number">20000</span></span><br></pre></td></tr></table></figure><p>最大迭代次数，2W次就停止了</p><h4 id="设置快照"><a href="#设置快照" class="headerlink" title="设置快照"></a>设置快照</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">snapshot: <span class="number">5000</span></span><br><span class="line">snapshot_prefix: <span class="string">"examples/mnist/lenet"</span></span><br></pre></td></tr></table></figure><p>快照。将训练出来的<code>model</code>和<code>solver</code>状态进行保存，<code>snapshot</code>用于设置训练多少次后进行保存，默认为0，不保存。<code>snapshot_prefix</code>设置保存路径。</p><p>还可以设置<code>snapshot_diff</code>，是否保存梯度值，默认为<code>false</code>,不保存。</p><p>也可以设置<code>snapshot_format</code>，保存的类型。有两种选择：<code>HDF5</code>和<code>BINARYPROTO</code>，默认为<code>BINARYPROTO</code></p><h4 id="设置运行模式"><a href="#设置运行模式" class="headerlink" title="设置运行模式"></a>设置运行模式</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">solver_mode: CPU</span><br></pre></td></tr></table></figure><p>设置运行模式。默认为GPU,如果你没有GPU,则需要改成CPU,否则会出错。</p><h4 id="设置学习率调整策略"><a href="#设置学习率调整策略" class="headerlink" title="设置学习率调整策略"></a>设置学习率调整策略</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">base_lr: <span class="number">0.01</span>    # base_lr用于设置基础学习率</span><br><span class="line">lr_policy: <span class="string">"inv"</span> # 学习率调整策略</span><br><span class="line">gamma: <span class="number">0.0001</span></span><br><span class="line">power: <span class="number">0.75</span></span><br></pre></td></tr></table></figure><p>学习率调整的策略还包括</p><ul><li>fixed:　　 保持base_lr不变.</li><li>step: 　　 如果设置为<code>step</code>,则还需要设置一个<code>stepsize</code>,  学习率为 $base_lr \times gamma ^ {(floor(\frac{iter} {stepsize}))}$,其中iter表示当前的迭代次数</li><li>exp:   　　 学习率为$base_lr \times gamma ^ {iter}$， iter为当前迭代次数</li><li>inv:　　     如果设置为<code>inv</code>,还需要设置一个<code>power</code>, 最终学习率为$base_lr \times (1 + gamma \times iter) ^ {(- power)}$</li><li>multistep: 如果设置为<code>multistep</code>,则还需要设置一个<code>stepvalue</code>。这个参数和<code>step</code>很相似，<code>step</code>是均匀等间隔变化，而multistep则是根据<code>stepvalue</code>值变化。</li><li>poly: 　　  学习率进行多项式误差, 最终学习率为 $ base_lr \times (1 - \frac{iter}{max_iter}) ^ {power} $</li><li>sigmoid:　学习率进行sigmod衰减，最终学习率为 $ base_lr \times ( \frac{1}{(1 + exp(-gamma \times (iter - stepsize)))})$</li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blog.slinuxer.com/2016/09/sgd-comparison" target="_blank" rel="noopener">SGD算法比较</a><br><a href="http://www.cnblogs.com/denny402/p/5074049.html" target="_blank" rel="noopener">Caffe学习系列(7)：solver及其配置</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> Solver </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之损失函数</title>
      <link href="/2018/02/03/DeepLearning/caffe/solver/caffe%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/"/>
      <url>/2018/02/03/DeepLearning/caffe/solver/caffe%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h3 id="【转】Caffe中的损失函数解析"><a href="#【转】Caffe中的损失函数解析" class="headerlink" title="【转】Caffe中的损失函数解析"></a>【转】Caffe中的损失函数解析</h3><h4 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h4><p>在有监督的机器学习中，需要有标签数据，与此同时，也需要有对应的损失函数（<code>Loss Function</code>）。<br>在<code>Caffe</code>中，目前已经实现了一些损失函数，包括最常见的<code>L2</code>损失函数，对比损失函数，信息增益损失函数等等。在这里做一个笔记，归纳总结<code>Caffe</code>中用到的不同的损失函数，以及分析它们各自适合的使用场景。</p><a id="more"></a><h4 id="欧式距离损失函数（Euclidean-Loss）"><a href="#欧式距离损失函数（Euclidean-Loss）" class="headerlink" title="欧式距离损失函数（Euclidean Loss）"></a>欧式距离损失函数（Euclidean Loss）</h4><ul><li>输入：<br>预测的值： $\hat{y} \in [-\infty,+\infty] $<br>其中，它们的形状为：$N \times C \times H \times W $<br>标签的值： $y \in [- \infty, + \infty]$<br>其中，它们的形状为：$N \times C \times H \times W$</li><li>输出：</li><li>损失函数：  $ loss =\frac{1}{2N}\sum \parallel pred - truth \parallel _{2}^{2} $</li><li>适合场景：<br>回归，特别适合其<strong>回归的值是实数值</strong>的时候。</li></ul><h4 id="对比损失函数（Contrastive-loss）"><a href="#对比损失函数（Contrastive-loss）" class="headerlink" title="对比损失函数（Contrastive loss）"></a>对比损失函数（Contrastive loss）</h4><ul><li>输入：<br> 形状：$ (N \times C \times 1 \times 1) $特征 $a \in [-\infty, +\infty]$<br> 形状：$ (N \times C \times 1 \times 1) $特征 $b \in [-\infty, +\infty]$<br> 形状：$ (N \times 1 \times 1 \times 1) $相似性 $y \in [0, 1] $</li><li>输出：<br>形状：$(1 \times 1 \times 1 \times 1) $</li><li>损失函数: $ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d + \left(1-y\right) \max \left(margin-d, 0\right) $<br>其中 $ d = \left| \left| a_n - b_n \right| \right|_2^2 $</li><li>适合场景：<br>可以用来<strong>训练<code>Siamese</code>网络</strong></li></ul><h4 id="铰链损失函数（Hinge-Loss）"><a href="#铰链损失函数（Hinge-Loss）" class="headerlink" title="铰链损失函数（Hinge Loss）"></a>铰链损失函数（Hinge Loss）</h4><ul><li><p>输入：<br> 形状：$(N \times C \times H \times W) $预测值 $t \in [-\infty, +\infty] $代表着预测  K= CHW 个类中的得分（注：CHW表示着    在网络设计中，不一定要把预测值进行向量化，只有其拉直后元素的个数相同即可。） . 在SVM中, t t 是 D 维特征$ X \in \mathcal{R}^{D \times N} $, 和学习到的超平面参数$W \in \mathcal{R}^{D \times K} $内积的结果 $X^T W X^T W$</p><p> 所以，一个网络如果仅仅只有全连接层 + 铰链损失函数，而没有其它的可学习的参数，那么它就等价于<code>SVM</code></p><p>标签值：$(N \times 1 \times 1 \times 1) $标签 l l , 是一个整数类型的数$l_n \in [0, 1, 2, …, K - 1]$ 其代表在 K  个类中的正确的标签。</p></li><li><p>输出：<br>形状：$(1 \times 1 \times 1 \times 1) $</p></li><li>损失函数:  $ E = \frac{1}{N} \sum\limits<em>{n=1}^N \sum\limits</em>{k=1}^K [\max(0, 1 - \delta{l<em>n = k} t</em>{nk})] ^ p$  $L^p$范数(默认是 p=1,是 L1 范数; L2 范数,正如在 L2-SVM中一样,也有实现),<br>其中 $\delta{\mathrm{条件}} = \left{ \begin{array}{lr} 1 &amp; \mbox{成立} \ -1 &amp; \mbox{不成立} \end{array} \right. $</li><li>应用场景：<br>在<strong>一对多的分类</strong>中应用,类似于SVM.</li></ul><h4 id="信息增益损失函数（InformationGain-Loss）"><a href="#信息增益损失函数（InformationGain-Loss）" class="headerlink" title="信息增益损失函数（InformationGain Loss）"></a>信息增益损失函数（InformationGain Loss）</h4><ul><li>输入：<br> 形状：$(N \times C \times H \times W) $预测值 $\hat{p} \in [0, 1] $内，<br> 表示这预测每一类的概率，共 $K = CHW$ 个类，<br> 每一个预测 概率$\hat{p}<em>n$ 的和为1:$K p ^ nk =1 \forall n \sum\limits</em>{k=1}^K \hat{p}_{nk} = 1 $.<br> 形状： $(N \times 1 \times 1 \times 1) $<br> 标签值： l, 是一个整数值，其范围是 $ l_n \in [0, 1, 2, …, K - 1]$表示着在 K 个类中的索引。<br> 形状：$(1 \times 1 \times K \times K) $ (可选)<br> 信息增益矩阵H作为第三个输入参数，.如果 $H=I$,则它等价于多项式逻辑损失函数</li><li>输出：<br>形状：$(1 \times 1 \times 1 \times 1)$</li><li>损失函数: $E = \frac{-1}{N} \sum\limits<em>{n=1}^N H</em>{l_n} \log(\hat{p}<em>n) = \frac{-1}{N} \sum\limits</em>{n=1}^N \sum\limits<em>{k=1}^{K} H</em>{l<em>n,k} \log(\hat{p}</em>{n,k}) $<br>其中 $H_{l_n}$ 表示 $l_n of H $.</li></ul><h4 id="多项式逻辑损失函数（Multinomial-Logistic-Loss）"><a href="#多项式逻辑损失函数（Multinomial-Logistic-Loss）" class="headerlink" title="多项式逻辑损失函数（Multinomial Logistic Loss）"></a>多项式逻辑损失函数（Multinomial Logistic Loss）</h4><ul><li>输入：<br> 形状：$(N \times C \times H \times W) $ 预测值 $\hat{p} \in [0, 1] $范围中，<br> 表示这预测的每一类的概率，共$K = CHW$ 个类.<br> 每一个预测概率$\hat{p}<em>n$ 的和为1:$K p ^ nk =1 \forall n \sum\limits</em>{k=1}^K \hat{p}_{nk} = 1$<br> 形状：$(N \times 1 \times 1 \times 1)$<br> 标签 l,是一个整数值，其范围是 $_n \in [0, 1, 2, …, K - 1]$表示着在 K 个类中的索引。</li><li>输出：</li><li>形状：$(1 \times 1 \times 1 \times 1)$</li><li>损失函数:  $E = \frac{-1}{N} \sum\limits<em>{n=1}^N \log(\hat{p}</em>{n,l_n}) $</li><li>应用场景：<br>在一对多的分类任务中使用，直接把预测的概率分布作为输入.</li></ul><h4 id="Sigmoid-交叉熵损失函数（Sigmoid-Cross-Entropy-Loss）"><a href="#Sigmoid-交叉熵损失函数（Sigmoid-Cross-Entropy-Loss）" class="headerlink" title="Sigmoid 交叉熵损失函数（Sigmoid Cross Entropy Loss）"></a>Sigmoid 交叉熵损失函数（Sigmoid Cross Entropy Loss）</h4><ul><li>输入：<br>形状: $(N \times C \times H \times W) $得分 $x \in [-\infty, +\infty]$,<br>这个层使用 sigmoid 函数$\sigma(.) $映射到概率分布$\hat{p}_n = \sigma(x_n) \in [0, 1]$<br>形状:$(N \times C \times H \times W) $标签$y \in [0, 1] $</li><li>输出：<br>形状：$(1 \times 1 \times 1 \times 1) $</li><li>损失函数:$E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $</li><li>应用场景：<br><strong>预测目标概率分布</strong></li></ul><h4 id="Softmax-损失函数-Softmax-With-Loss"><a href="#Softmax-损失函数-Softmax-With-Loss" class="headerlink" title="Softmax+损失函数(Softmax With Loss)"></a>Softmax+损失函数(Softmax With Loss)</h4><ul><li>输入：<br>形状：$(N \times C \times H \times W)$预测值$x \in [-\infty, +\infty]$代表预测每个类的得分。<br>共 $K=CHW$ 类.<br>这一层把得分通过softmax映射到概率分布$\hat{p}<em>{nk} = \exp(x</em>{nk}) / \left[\sum<em>{k’} \exp(x</em>{nk’})\right] $<br>形状：$(N \times 1 \times 1 \times 1)$<br>标签值是一个整数值，其范围是 $_n \in [0, 1, 2, …, K - 1] $表示着在 K 个类中的索引。</li><li>输出：<br>形状：$(1 \times 1 \times 1 \times 1)$</li><li>损失函数: $E = \frac{-1}{N} \sum\limits<em>{n=1}^N \log(\hat{p}</em>{n,l_n})$<br>其中 $ \hat{p}$为<code>softmax</code>输出的类概率。</li><li>应用场景：<br> 在<strong>一对多分类</strong>中应用。</li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>欧式距离损失函数：一般适用于回归问题，特别是回归的值是实数的时候。<br>对比损失函数：用来训练<code>siamese</code>网络时候。<br><code>Hinge loss</code>：在一对多的分类中应用，类似于<code>SVM</code>。<br>多项式逻辑损失函数：一般在一对多的分类任务中应用，直接把预测的概率分布作为输入。<br><code>sigmoid</code>交叉熵损失函数：预测目标概率分布。<br><code>softmax</code>+损失函数：在一对多分类中的应用。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>来源：Caffe中的损失函数解析 <a href="http://www.aichengxu.com/other/10039373.htm" target="_blank" rel="noopener">http://www.aichengxu.com/other/10039373.htm</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> Solver </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之Softmax层</title>
      <link href="/2018/02/02/DeepLearning/caffe/layer/Softmax/"/>
      <url>/2018/02/02/DeepLearning/caffe/layer/Softmax/</url>
      <content type="html"><![CDATA[<blockquote><p><img src="http://xukeqiniu.xukeai.cn/2490fe237f606073b367b7b03551461b.png" alt=""><br>从零开始，一步一步学习caffe的使用，期间贯穿深度学习和调参的相关知识！<br><a id="more"></a></p><h3 id="softmax-layer"><a href="#softmax-layer" class="headerlink" title="softmax layer"></a>softmax layer</h3></blockquote><h4 id="softmax-layer-输出似然值"><a href="#softmax-layer-输出似然值" class="headerlink" title="softmax layer: 输出似然值"></a>softmax layer: 输出似然值</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layers &#123;</span><br><span class="line">  bottom: <span class="string">"cls3_fc"</span></span><br><span class="line">  top: <span class="string">"prob"</span></span><br><span class="line">  name: <span class="string">"prob"</span></span><br><span class="line">  type: <span class="string">"softmax"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>公式如下所示：</p><p><img src="http://xukeqiniu.xukeai.cn/25c819785a0b06023dba5752b986f68c.png" alt=""></p><h4 id="softmax-loss-layer：输出loss值"><a href="#softmax-loss-layer：输出loss值" class="headerlink" title="softmax-loss layer：输出loss值"></a>softmax-loss layer：输出loss值</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"loss"</span></span><br><span class="line">  type: <span class="string">"SoftmaxWithLoss"</span></span><br><span class="line">  bottom: <span class="string">"ip1"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">  top: <span class="string">"loss"</span></span><br><span class="line">  loss_param&#123;</span><br><span class="line">    ignore_label：<span class="number">0</span></span><br><span class="line">    normalize: <span class="number">1</span></span><br><span class="line">    normalization: FULL</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>公式如下所示：</p><p><img src="http://xukeqiniu.xukeai.cn/e8d48e19c743740cfcc977331653e196.png" alt=""></p><p>loss_param 说明：</p><ul><li>ignore_label<br>int型变量，默认为空。<br>如果指定值，则label等于ignore_label的样本将不参与Loss计算，并且反向传播时梯度直接置0.</li><li>normalize<br>bool型变量，即Loss会除以参与计算的样本总数；否则Loss等于直接求和</li><li><p>normalization<br>enum型变量，默认为VALID，具体代表情况如下面的代码。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> NormalizationMode &#123;</span><br><span class="line">  <span class="comment">// Divide by the number of examples in the batch times spatial dimensions.</span></span><br><span class="line">  <span class="comment">// Outputs that receive the ignore label will NOT be ignored in computing the normalization factor.</span></span><br><span class="line">  FULL = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Divide by the total number of output locations that do not take the</span></span><br><span class="line">  <span class="comment">// ignore_label.  If ignore_label is not set, this behaves like FULL.</span></span><br><span class="line">  VALID = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Divide by the batch size.</span></span><br><span class="line">  BATCH_SIZE = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  NONE = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(1) 未设置normalization，但是设置了normalize:<br><code>normalize==1</code> : 归一化方式为VALID<br><code>normalize==0</code> : 归一化方式为BATCH_SIZE<br>(2)一旦设置normalization，归一化方式则由normalization决定，不再考虑normalize。</p></li></ul><h3 id="其他说明"><a href="#其他说明" class="headerlink" title="其他说明"></a>其他说明</h3><h4 id="softmax的上溢与下溢"><a href="#softmax的上溢与下溢" class="headerlink" title="softmax的上溢与下溢"></a>softmax的上溢与下溢</h4><p>对于<code>softmax</code>的计算公式来说，对于比较小的输入数据来说是没有什么问题的，但是针对指数函数的特点，对于较大或者较小的数据进行<code>softmax</code>计算会出现数据上溢与下溢的问题。计算机中浮点数的最大表示位数为$2^{64}$,如果超过此数会产生上溢<code>inf</code>,同样数据小于$2^{-64}$计算机在计算过程中会产生下溢<code>`-inf</code>。举个例子:</p><ul><li>对于[3,1,-3]，直接计算是可行的，我们可以得到(0.88,0.12,0)。</li><li>对于[1000,1000,1000]，我们会得到inf（<strong>上溢</strong>）；</li><li>对于[-1000,-999,-1000]，我们会得到-inf（<strong>下溢</strong>）。</li></ul><h4 id="softmax解决上溢与下溢的办法"><a href="#softmax解决上溢与下溢的办法" class="headerlink" title="softmax解决上溢与下溢的办法"></a>softmax解决上溢与下溢的办法</h4><p><img src="http://xukeqiniu.xukeai.cn/b826af98fc1a45a8eaa6c4be4f0957f9.png" alt=""><br>对任意a都成立，这意味着我们可以自由地调节指数函数的指数部分，一个典型的做法是取输入向量中的最大值：a=max{x1,x2…..xn}<br><strong>这可以保证指数最大不会超过0，于是避免了上溢。即便剩余的部分下溢出了，加了a之后，也能得到一个合理的值。</strong><br>并且<code>softmax</code>不受输入的常数偏移影响，即softmax(x)=softmax(x+c)证明如下：<br><img src="http://xukeqiniu.xukeai.cn/6017aaf5ca0f104f796ffa6b018419e4.png" alt=""></p><h3 id="测试准确率"><a href="#测试准确率" class="headerlink" title="测试准确率"></a>测试准确率</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"accuracy"</span></span><br><span class="line">  type: <span class="string">"Accuracy"</span></span><br><span class="line">  bottom: <span class="string">"ip2"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">  top: <span class="string">"accuracy"</span></span><br><span class="line">  accuracy_param&#123;</span><br><span class="line">    top_k:<span class="number">5</span></span><br><span class="line">  &#125;</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">message AccuracyParameter &#123;</span><br><span class="line">  <span class="comment">// When computing accuracy, count as correct by comparing the true label to</span></span><br><span class="line">  <span class="comment">// the top k scoring classes.  By default, only compare to the top scoring</span></span><br><span class="line">  <span class="comment">// class (i.e. argmax).</span></span><br><span class="line">  optional uint32 top_k = <span class="number">1</span> [<span class="keyword">default</span> = <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The "label" axis of the prediction blob, whose argmax corresponds to the</span></span><br><span class="line">  <span class="comment">// predicted label -- may be negative to index from the end (e.g., -1 for the</span></span><br><span class="line">  <span class="comment">// last axis).  For example, if axis == 1 and the predictions are</span></span><br><span class="line">  <span class="comment">// (N x C x H x W), the label blob is expected to contain N*H*W ground truth</span></span><br><span class="line">  <span class="comment">// labels with integer values in &#123;0, 1, ..., C-1&#125;.</span></span><br><span class="line">  optional int32 axis = <span class="number">2</span> [<span class="keyword">default</span> = <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If specified, ignore instances with the given label.</span></span><br><span class="line">  optional int32 ignore_label = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>softmax函数计算时候为什么要减去一个最大值？<br><a href="http://blog.csdn.net/shuzfan/article/details/51460895" target="_blank" rel="noopener">caffe层解读系列-softmax_loss</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> layer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之Dropout层</title>
      <link href="/2018/01/30/DeepLearning/caffe/layer/Dropout/"/>
      <url>/2018/01/30/DeepLearning/caffe/layer/Dropout/</url>
      <content type="html"><![CDATA[<blockquote><p><img src="http://xukeqiniu.xukeai.cn/2490fe237f606073b367b7b03551461b.png" alt=""><br>从零开始，一步一步学习caffe的使用，期间贯穿深度学习和调参的相关知识！<br><a id="more"></a></p><h3 id="Dropout-参数设置"><a href="#Dropout-参数设置" class="headerlink" title="Dropout 参数设置"></a>Dropout 参数设置</h3></blockquote><p><code>Dropout</code>是一个防止过拟合的层,只需要设置一个<code>dropout_ratio</code>就可以了。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"drop7"</span></span><br><span class="line">  type: <span class="string">"Dropout"</span></span><br><span class="line">  bottom: <span class="string">"fc7-conv"</span></span><br><span class="line">  top: <span class="string">"fc7-conv"</span></span><br><span class="line">  dropout_param &#123;</span><br><span class="line">    dropout_ratio: <span class="number">0.5</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="Dropout-实现代码"><a href="#Dropout-实现代码" class="headerlink" title="Dropout 实现代码"></a>Dropout 实现代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout</span><span class="params">(X, drop_probability)</span>:</span></span><br><span class="line">    keep_probability = <span class="number">1</span> - drop_probability</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= keep_probability &lt;= <span class="number">1</span></span><br><span class="line">    <span class="comment"># 这种情况下把全部元素都丢弃。</span></span><br><span class="line">    <span class="keyword">if</span> keep_probability == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X.zeros_like()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机选择一部分该层的输出作为丢弃元素。</span></span><br><span class="line">    mask = nd.random.uniform(</span><br><span class="line">        <span class="number">0</span>, <span class="number">1.0</span>, X.shape, ctx=X.context) &lt; keep_probability</span><br><span class="line">    <span class="comment"># 保证 E[dropout(X)] == X</span></span><br><span class="line">    scale =  <span class="number">1</span> / keep_probability</span><br><span class="line">    <span class="keyword">return</span> mask * X * scale</span><br></pre></td></tr></table></figure><h3 id="Dropout-意义"><a href="#Dropout-意义" class="headerlink" title="Dropout 意义"></a>Dropout 意义</h3><p>Dropout可以被认为是集成大量深层神经网络的实用Bagging方法。但是Bagging方法涉及训练多个模型，并且在每个测试样本上评估多个模型。当每个模型都是一个大型神经网络时，Bagging方法会耗费很多的时间和内存。而Dropout则提供了一种廉价的Bagging集成近似，能够训练和评估指数级数量的神经网络。</p><ul><li>Dropout训练的集成包括所有从基础网络中除去神经元（非输出单元）后形成的子网络。只需将一些单元的输出乘零就能有效的删除一个单元（称之为乘零的简单Dropout算法）。假如基本网络有$n$个非输出神经元，则一共有$2^n$个子网络。</li><li>Dropout的目标是在指数级数量的神经网络上近似Bagging过程。具体来说，在训练中使用Dropout时，我们会使用基于小批量产生较小步长的学习算法，如随机梯度下降。<ul><li>每次在小批量中加载一个样本，然后随机抽样（用于网络中所有输入和隐藏单元的）不同二值掩码。</li><li>对于每个单元，掩码是独立采样的。通常输入单元被包括的概率为$0.8$，隐藏单元被包括的概率为$0.5$。</li><li>然后与之前一样，运行前向传播、反向传播和学习更新。</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/exacity/simplified-deeplearning/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/img/Dropout.png" alt="Dropout"></p><h3 id="Dropout-小结"><a href="#Dropout-小结" class="headerlink" title="Dropout 小结"></a>Dropout 小结</h3><ul><li>Dropout优点<ul><li>计算方便。训练过程中使用Dropout产生$n$个随机二进制数与状态相乘即可。每个样本每次更新的时间复杂度：$O(n)$，空间复杂度：$O(n)$。</li><li>适用广。Dropout不怎么限制适用的模型或训练过程，几乎在所有使用分布式表示且可以用随机梯度下降训练的模型上都表现很好。包括：前馈神经网络、概率模型、受限波尔兹曼机、循环神经网络等。</li><li>相比其他正则化方法（如权重衰减、过滤器约束和稀疏激活）更有效。也可与其他形式的正则化合并，得到进一步提升。</li></ul></li><li>Dropout缺点<ul><li>不适合宽度太窄的网络。否则大部分网络没有输入到输出的路径。</li><li>不适合训练数据太小（如小于5000）的网络。训练数据太小时，Dropout没有其他方法表现好。</li><li>不适合非常大的数据集。数据集大的时候正则化效果有限（大数据集本身的泛化误差就很小），使用Dropout的代价可能超过正则化的好处。</li></ul></li><li>Dropout衍生方法<ul><li>Dropout作用于线性回归时，相当于每个输入特征具有不同权重衰减系数的$L^2$权重衰减，系数大小由其方差决定。但对深度模型而言，二者是不等同的。</li><li>快速Dropout (Wang and Manning，2013)：利用近似解的方法，减小梯度计算中的随机性析解，获得更快的收敛速度。</li><li>DropConnect (Wan，2013)：将一个标量权重和单个隐藏单元状态之间的每个乘积作为可以丢弃的一个单元。</li><li>$\mu$不取二值，而是服从正态分布，即$\mu\sim\mathcal{N}(1,I)$（Srivastava，2014）。</li></ul></li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>Deep Learning Book<br><a href="https://github.com/exacity/simplified-deeplearning/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96.md" target="_blank" rel="noopener">深度学习中的正则化</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> layer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之归一化层</title>
      <link href="/2018/01/28/DeepLearning/caffe/layer/%E5%BD%92%E4%B8%80%E5%8C%96/"/>
      <url>/2018/01/28/DeepLearning/caffe/layer/%E5%BD%92%E4%B8%80%E5%8C%96/</url>
      <content type="html"><![CDATA[<blockquote><p><img src="http://xukeqiniu.xukeai.cn/2490fe237f606073b367b7b03551461b.png" alt=""><br>从零开始，一步一步学习caffe的使用，期间贯穿深度学习和调参的相关知识！<br><a id="more"></a></p><h3 id="LRN-参数配置"><a href="#LRN-参数配置" class="headerlink" title="LRN 参数配置"></a>LRN 参数配置</h3></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"norm1"</span></span><br><span class="line">  type: <span class="string">"LRN"</span></span><br><span class="line">  bottom: <span class="string">"conv1"</span></span><br><span class="line">  top: <span class="string">"norm1"</span></span><br><span class="line">  lrn_param &#123;</span><br><span class="line">    local_size: <span class="number">5</span></span><br><span class="line">    alpha: <span class="number">0.0001</span></span><br><span class="line">    beta: <span class="number">0.75</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="LRN说明"><a href="#LRN说明" class="headerlink" title="LRN说明"></a>LRN说明</h3><h4 id="示意图"><a href="#示意图" class="headerlink" title="示意图"></a>示意图</h4><p><img src="http://xukeqiniu.xukeai.cn/90562df1f5d553a5069b931c5e570168.png" alt=""></p><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><p><img src="http://xukeqiniu.xukeai.cn/9aaa3f34020362e8a0eb26b565e77096.png" alt=""></p><h4 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h4><p><code>LRN</code>操作主要目的是在<strong>深度上进行平滑操作</strong>，使得数据在深度层面上有一定的联系。</p><h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p><code>LRN</code>层是在<code>AlexNet</code>网络中提出来的，但是因为其计算的复杂度和后期的效果并不理想，在后面的网络中基本上抛弃的LRN的归一化方法。<br><img src="http://xukeqiniu.xukeai.cn/e8f7db6d9c51ad3a8bf482ec1d9e1979.png" alt=""></p><h3 id="Batch-Norm-参数配置"><a href="#Batch-Norm-参数配置" class="headerlink" title="Batch Norm  参数配置"></a>Batch Norm  参数配置</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">bottom: <span class="string">"conv1"</span></span><br><span class="line">top: <span class="string">"conv1"</span></span><br><span class="line">name: <span class="string">"bn_conv1"</span></span><br><span class="line">type: <span class="string">"BatchNorm"</span></span><br><span class="line">batch_norm_param &#123;</span><br><span class="line">use_global_stats: <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">bottom: <span class="string">"conv1"</span></span><br><span class="line">top: <span class="string">"conv1"</span></span><br><span class="line">name: <span class="string">"scale_conv1"</span></span><br><span class="line">type: <span class="string">"Scale"</span></span><br><span class="line">scale_param &#123;</span><br><span class="line">bias_term: <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">message BatchNormParameter &#123;</span><br><span class="line">  <span class="comment">// 如果为真，则使用保存的均值和方差，否则采用滑动平均计算新的均值和方差。</span></span><br><span class="line">  <span class="comment">// 该参数缺省的时候，如果是测试阶段则等价为真，如果是训练阶段则等价为假。</span></span><br><span class="line">  optional <span class="keyword">bool</span> use_global_stats = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 滑动平均的衰减系数，默认为0.999</span></span><br><span class="line">  optional <span class="keyword">float</span> moving_average_fraction = <span class="number">2</span> [<span class="keyword">default</span> = <span class="number">.999</span>];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 分母附加值，防止除以方差时出现除0操作，默认为1e-5</span></span><br><span class="line">  optional <span class="keyword">float</span> eps = <span class="number">3</span> [<span class="keyword">default</span> = <span class="number">1e-5</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Batch-Norm-说明"><a href="#Batch-Norm-说明" class="headerlink" title="Batch Norm 说明"></a>Batch Norm 说明</h3><h4 id="公式-1"><a href="#公式-1" class="headerlink" title="公式"></a>公式</h4><p><img src="http://xukeqiniu.xukeai.cn/d67e7866d2191053775cfcc4a5a46259.png" alt=""></p><h4 id="意义-1"><a href="#意义-1" class="headerlink" title="意义"></a>意义</h4><p>在深度神经网络的训练过程中，先前层参数的调整会导致之后每一层输入值的分布发生变化，这种现象会使得模型的训练变得复杂。因为<strong>分布均匀且统一的数据往往更加容易使得算法训练出高准确率的模型</strong>。<code>Batch Norm</code>主要借鉴的是<strong>白化的思想</strong>，目的是<strong>将数据尽量维持在方差为1，均值为0的分布上</strong>。因为数据的总量十分庞大，我们无法计算出整个数据集的均值与方差，因此采用Batch的思想，<strong>分块的进行白化处理</strong>，<code>Batch Norm</code>的精华体现在最后一个公式上，<strong>白化之后进行拉伸与平移，也就是对数据分布进行修正。并且拉伸与平移的系数是通过训练得到的，这样，我们将<code>batch</code>在整个数据集的整体分布状态也学习了出来，防止以偏概全</strong>！下图可以清晰的看出，加入<code>Batch Norm</code>使得每一层的数据更够有效且均匀的传递到下一层中去。<br><img src="http://xukeqiniu.xukeai.cn/214ab5bd0a30fb683013e19d986e39b9.png" alt=""></p><h4 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h4><p><code>Batch Norm</code>在<code>ResNet</code>网络和<code>Inception-v2</code>、<code>Inception-v3</code>、<code>Inception-v4</code>中均有用到。<br><img src="http://xukeqiniu.xukeai.cn/5bb4cb20866f0e9a3bd3c73c84d961b9.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/a1fed639d061d60d386c84f564b4d918.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>批标准化 (Batch Normalization)<br><a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-08-batch-normalization/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-08-batch-normalization/</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> layer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之全连接层</title>
      <link href="/2018/01/26/DeepLearning/caffe/layer/%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82/"/>
      <url>/2018/01/26/DeepLearning/caffe/layer/%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82/</url>
      <content type="html"><![CDATA[<blockquote><p><img src="http://xukeqiniu.xukeai.cn/2490fe237f606073b367b7b03551461b.png" alt=""><br>从零开始，一步一步学习caffe的使用，期间贯穿深度学习和调参的相关知识！<br><a id="more"></a></p></blockquote><h3 id="全连接层参数说明"><a href="#全连接层参数说明" class="headerlink" title="全连接层参数说明"></a>全连接层参数说明</h3><p>全连接层，输出的是一个一维向量,参数跟卷积层一样。<strong>一般将全连接置于卷积神经网络的后几层</strong>。权重值的初始化采用<code>xavier</code>,偏置初始化为0.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"ip1"</span></span><br><span class="line">  type: <span class="string">"InnerProduct"</span> #全连接层</span><br><span class="line">  bottom: <span class="string">"pool2"</span> #输入</span><br><span class="line">  top: <span class="string">"ip1"</span> #输出</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: <span class="number">1</span> #权重学习率倍数</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: <span class="number">2</span> #偏置学习率倍数</span><br><span class="line">  &#125;</span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    num_output: <span class="number">500</span> #输出一维向量个数</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: <span class="string">"xavier"</span> #权重初始化方式</span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: <span class="string">"constant"</span> #偏置初始化方式</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="全连接配置参数意义"><a href="#全连接配置参数意义" class="headerlink" title="全连接配置参数意义"></a>全连接配置参数意义</h3><h4 id="全连接计算公式"><a href="#全连接计算公式" class="headerlink" title="全连接计算公式"></a>全连接计算公式</h4><p><img src="http://xukeqiniu.xukeai.cn/bbbdfeb8bcb537f3f3b56e7dd8cb690f.png" alt=""></p><h4 id="全连接计算图示"><a href="#全连接计算图示" class="headerlink" title="全连接计算图示"></a>全连接计算图示</h4><p><img src="http://xukeqiniu.xukeai.cn/c5461b9b48c076bd1be8041fa5ab3dda.png" alt=""></p><h4 id="全连接意义"><a href="#全连接意义" class="headerlink" title="全连接意义"></a>全连接意义</h4><p>全连接计算是神经网络的基本计算单元，从历史的角度考虑，全连接其实是前馈神经网络，多层感知机（MLP）方法在卷积神经网络的延用。因此，在全连接计算图示模块中我采用的是传统的MLP结构。全连接层一般置于卷积神经网络的结尾，<strong>因为其参数量和计算量对输入输出数据都比较敏感</strong>，若<strong>卷积神经网络结构前期采用全连接层容易造成参数量过大，数据计算冗余进一步使得模型容易过拟合</strong>，因此，我们采用卷积的计算过程减少了参数量，并更够提取合适的特征。但是<strong>随着深度的增加，数据信息会不断地丢失，最后采用全连接层能够保留住前面的重要信息</strong>，因此全连接与卷积的合理调整会对整个模型的性能产生至关重要的作用！</p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> layer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之池化层</title>
      <link href="/2018/01/24/DeepLearning/caffe/layer/%E6%B1%A0%E5%8C%96%E5%B1%82/"/>
      <url>/2018/01/24/DeepLearning/caffe/layer/%E6%B1%A0%E5%8C%96%E5%B1%82/</url>
      <content type="html"><![CDATA[<blockquote><p><img src="http://xukeqiniu.xukeai.cn/2490fe237f606073b367b7b03551461b.png" alt=""><br>从零开始，一步一步学习caffe的使用，期间贯穿深度学习和调参的相关知识！<br><a id="more"></a></p></blockquote><h3 id="池化层参数说明"><a href="#池化层参数说明" class="headerlink" title="池化层参数说明"></a>池化层参数说明</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"pool1"</span></span><br><span class="line">  type: <span class="string">"Pooling"</span></span><br><span class="line">  bottom: <span class="string">"conv1"</span></span><br><span class="line">  top: <span class="string">"pool1"</span></span><br><span class="line">  pooling_param &#123;</span><br><span class="line">    pool: MAX #池化方法，默认为MAX。目前可用的方法有MAX, AVE</span><br><span class="line">    kernel_size: <span class="number">3</span> #池化的核大小</span><br><span class="line">    stride: <span class="number">2</span> #池化的步长，默认为<span class="number">1</span>。一般我们设置为<span class="number">2</span>，即不重叠。</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>caffe池化层对应的参数说明文件，如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">message PoolingParameter &#123;</span><br><span class="line">  <span class="keyword">enum</span> PoolMethod &#123;</span><br><span class="line">    MAX = <span class="number">0</span>;</span><br><span class="line">    AVE = <span class="number">1</span>;</span><br><span class="line">    STOCHASTIC = <span class="number">2</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  optional PoolMethod pool = <span class="number">1</span> [<span class="keyword">default</span> = MAX]; <span class="comment">// The pooling method</span></span><br><span class="line">  <span class="comment">// Pad, kernel size, and stride are all given as a single value for equal</span></span><br><span class="line">  <span class="comment">// dimensions in height and width or as Y, X pairs.</span></span><br><span class="line">  optional uint32 pad = <span class="number">4</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// The padding size (equal in Y, X)</span></span><br><span class="line">  optional uint32 pad_h = <span class="number">9</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// The padding height</span></span><br><span class="line">  optional uint32 pad_w = <span class="number">10</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// The padding width</span></span><br><span class="line">  optional uint32 kernel_size = <span class="number">2</span>; <span class="comment">// The kernel size (square)</span></span><br><span class="line">  optional uint32 kernel_h = <span class="number">5</span>; <span class="comment">// The kernel height</span></span><br><span class="line">  optional uint32 kernel_w = <span class="number">6</span>; <span class="comment">// The kernel width</span></span><br><span class="line">  optional uint32 stride = <span class="number">3</span> [<span class="keyword">default</span> = <span class="number">1</span>]; <span class="comment">// The stride (equal in Y, X)</span></span><br><span class="line">  optional uint32 stride_h = <span class="number">7</span>; <span class="comment">// The stride height</span></span><br><span class="line">  optional uint32 stride_w = <span class="number">8</span>; <span class="comment">// The stride width</span></span><br><span class="line">  <span class="keyword">enum</span> Engine &#123;</span><br><span class="line">    DEFAULT = <span class="number">0</span>;</span><br><span class="line">    CAFFE = <span class="number">1</span>;</span><br><span class="line">    CUDNN = <span class="number">2</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  optional Engine engine = <span class="number">11</span> [<span class="keyword">default</span> = DEFAULT];</span><br><span class="line">  <span class="comment">// If global_pooling then it will pool over the size of the bottom by doing</span></span><br><span class="line">  <span class="comment">// kernel_h = bottom-&gt;height and kernel_w = bottom-&gt;width</span></span><br><span class="line">  optional <span class="keyword">bool</span> global_pooling = <span class="number">12</span> [<span class="keyword">default</span> = <span class="literal">false</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="池化层详解"><a href="#池化层详解" class="headerlink" title="池化层详解"></a>池化层详解</h3><h4 id="池化图解"><a href="#池化图解" class="headerlink" title="池化图解"></a>池化图解</h4><ul><li>Max pooling 与 Average pooling</li></ul><p><img src="http://xukeqiniu.xukeai.cn/9a56f1208b252c44913367ce9754a392.png" alt=""></p><ul><li>Global Average pooling</li></ul><p><img src="http://xukeqiniu.xukeai.cn/25afe15c7878c28bba72851c94aff523.png" alt=""></p><h4 id="池化层意义"><a href="#池化层意义" class="headerlink" title="池化层意义"></a>池化层意义</h4><p>因为卷积层每次作用在一个窗口，它对位置很敏感。池化层能够很好的缓解这个问题。它跟卷积类似每次看一个小窗口，然后选出窗口里面最大的元素，或者平均元素作为输出。这样做为后续操作减少了运算量，同时能有效避免数据过拟合的现象。<br><code>pooling</code>的操作如果采用<code>global</code>的方式进行，也就是对输入数据的一整个面进行<code>pool</code>操作，<strong>这种设计方法可以在某种情况下代替全连接层，减少参数的存储和计算量</strong>。例如<code>SqueezeNet</code>最后一层，<code>ResNet-50</code>倒数第二层j均采用了<code>global</code>的<code>ave-pooling</code>。</p><h4 id="max-pool的问题"><a href="#max-pool的问题" class="headerlink" title="max-pool的问题"></a>max-pool的问题</h4><p><strong>现在的最大池化层大约去掉了 75% 的激活函数</strong>。</p><ul><li>空间信息损失：当去掉 75% 的激活函数时，关于其来源的信息就会丢失。</li><li>最大池化层无法使用来自多层激活函数的信息。</li><li>反向传播只会提升最大池化的激活函数，即使其他激活函数的值可能出现错误。</li></ul><h4 id="sort-pool2d"><a href="#sort-pool2d" class="headerlink" title="sort_pool2d"></a>sort_pool2d</h4><p><code>sort_pool2d</code>的提出旨在尽量多的解决max-pooling上面提到的问题，具体实现方式如下：<br>设池化之前的层的输出为张量 T，大小为 [B, H, W, C]。定义一个超参数 <code>pool_range</code>，它可以是 [1,2,3,4] 中的任意一个。<code>pool_range</code> 指定激活函数（按照排列顺序保存）的数量。假设要被池化的张量 T 有 4 个激活函数，我首先按照 [a1, a2, a3, a4] 的顺序排列它们，其中 a1 ≥ a2 ≥ a3 ≥ a4。接着保留其中的第一个 <code>pool_range</code>，我称之为激活向量。</p><p>将 <code>pool_range</code> 的权重向量定义为 [w{1},…. w{pool_range}]。这里需要注意的是，如果这些权重中的任何一个是负值，则激活向量按强度排序且采用加权平均的假设将不成立。因此，没有直接使用权重，而是在权重向量上取一个 <code>softmax</code>，并将结果乘以激活向量。<br>其实<code>sort_pool2d</code>是<code>max-pooling</code>的一个扩展，假设<code>pool_range=1</code>对应的便是<code>max-pooling</code>。具体代码参考：<br>sort_pool2d 代码：<a href="https://github.com/singlasahil14/sortpool2d/blob/master/sortpool2d_test.py" target="_blank" rel="noopener">https://github.com/singlasahil14/sortpool2d/blob/master/sortpool2d_test.py</a><br>sort_pool2d 实现：<a href="https://github.com/singlasahil14/sortpool2d/blob/master/sort_pool2d.py" target="_blank" rel="noopener">https://github.com/singlasahil14/sortpool2d/blob/master/sort_pool2d.py</a></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>花式池化<br><a href="https://antkillerfarm.github.io/dl/2017/08/28/Deep_Learning_13.html" target="_blank" rel="noopener">https://antkillerfarm.github.io/dl/2017/08/28/Deep_Learning_13.html</a><br>新型池化层sort_pool2d实现更快更好的收敛：表现优于最大池化层<br><a href="http://mp.weixin.qq.com/s/XzOri12hwyOCdI1TgGQV3w" target="_blank" rel="noopener">http://mp.weixin.qq.com/s/XzOri12hwyOCdI1TgGQV3w</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> layer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之激活函数层</title>
      <link href="/2018/01/23/DeepLearning/caffe/layer/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
      <url>/2018/01/23/DeepLearning/caffe/layer/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
      <content type="html"><![CDATA[<blockquote><p><img src="http://xukeqiniu.xukeai.cn/2490fe237f606073b367b7b03551461b.png" alt=""><br>从零开始，一步一步学习caffe的使用，期间贯穿深度学习和调参的相关知识！<br><a id="more"></a></p><h3 id="激活函数参数配置"><a href="#激活函数参数配置" class="headerlink" title="激活函数参数配置"></a>激活函数参数配置</h3><p>在激活层中，对输入数据进行激活操作,是逐元素进行运算的,在运算过程中，没有改变数据的大小，即输入和输出的数据大小是相等的。神经网络中激活函数的主要作用是提供网络的非线性建模能力，如不特别说明，激活函数一般而言是非线性函数。假设一个示例神经网络中仅包含线性卷积和全连接运算，那么该网络仅能够表达线性映射，即便增加网络的深度也依旧还是线性映射，难以有效建模实际环境中非线性分布的数据。加入（非线性）激活函数之后，深度神经网络才具备了分层的非线性映射学习能力。因此，激活函数是深度神经网络中不可或缺的部分。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"test"</span></span><br><span class="line">  bottom: <span class="string">"conv"</span></span><br><span class="line">  top: <span class="string">"test"</span></span><br><span class="line">  type: <span class="string">"ReLU"</span> #激活函数类型</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="常用激活函数总结"><a href="#常用激活函数总结" class="headerlink" title="常用激活函数总结"></a>常用激活函数总结</h3><p>首先推荐一个常用激活函数可视化项目<a href="https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/" target="_blank" rel="noopener">visualising activation functions in neural networks</a></p><h4 id="Step"><a href="#Step" class="headerlink" title="Step"></a>Step</h4><p><img src="http://xukeqiniu.xukeai.cn/8f9102e8ab5f36d07be91a56c49e8fcd.png" alt=""><br>激活函数 Step 更倾向于理论而不是实际，它模仿了生物神经元要么全有要么全无的属性。它无法应用于神经网络，因为其导数是 0（除了零点导数无定义以外），这意味着基于梯度的优化方法并不可行。</p><h4 id="Identity"><a href="#Identity" class="headerlink" title="Identity"></a>Identity</h4><p><img src="http://xukeqiniu.xukeai.cn/d0c492a9eeff7a1dacc26225b3dedc5f.png" alt=""><br>通过激活函数 Identity，节点的输入等于输出。它完美适合于潜在行为是线性（与线性回归相似）的任务。当存在非线性，单独使用该激活函数是不够的，但它依然可以在最终输出节点上作为激活函数用于回归任务。</p><h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p><img src="http://xukeqiniu.xukeai.cn/f5c592a5ab20b51f480af6bd6bb9fb53.png" alt=""><br>修正线性单元（Rectified linear unit，ReLU）是神经网络中最常用的激活函数。它保留了 step 函数的生物学启发（只有输入超出阈值时神经元才激活），不过当输入为正的时候，导数不为零，从而允许基于梯度的学习（尽管在 x=0 的时候，导数是未定义的）。使用这个函数能使计算变得很快，因为无论是函数还是其导数都不包含复杂的数学运算。然而，当输入为负值的时候，ReLU 的学习速度可能会变得很慢，甚至使神经元直接无效，因为此时输入小于零而梯度为零，从而其权重无法得到更新，在剩下的训练过程中会一直保持静默。</p><h4 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h4><p><img src="http://xukeqiniu.xukeai.cn/4ef2b577e942c96fbc1f9b234c7a9d6b.png" alt=""><br>经典（以及广泛使用的）ReLU 激活函数的变体，带泄露修正线性单元（Leaky ReLU）的输出对负值输入有很小的坡度。由于导数总是不为零，这能减少静默神经元的出现，允许基于梯度的学习（虽然会很慢）。</p><h4 id="PReLU"><a href="#PReLU" class="headerlink" title="PReLU"></a>PReLU</h4><p><img src="http://xukeqiniu.xukeai.cn/bc93f355e147a7538f7074acf5a8c356.png" alt=""><br>参数化修正线性单元（Parameteric Rectified Linear Unit，PReLU）属于 ReLU 修正类激活函数的一员。它和 RReLU 以及 Leaky ReLU 有一些共同点，即为负值输入添加了一个线性项。而最关键的区别是，这个线性项的斜率实际上是在模型训练中学习到的。</p><h4 id="RReLU"><a href="#RReLU" class="headerlink" title="RReLU"></a>RReLU</h4><p><img src="http://xukeqiniu.xukeai.cn/09698defc9c75769c275aca3ddb59184.png" alt=""><br>随机带泄露的修正线性单元（Randomized Leaky Rectified Linear Unit，RReLU）也属于 ReLU 修正类激活函数的一员。和 Leaky ReLU 以及 PReLU 很相似，为负值输入添加了一个线性项。而最关键的区别是，这个线性项的斜率在每一个节点上都是随机分配的（通常服从均匀分布）。</p><h4 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h4><p><img src="http://xukeqiniu.xukeai.cn/315e03b49829df38d61bd9b32bd892cc.png" alt=""><br>指数线性单元（Exponential Linear Unit，ELU）也属于 ReLU 修正类激活函数的一员。和 PReLU 以及 RReLU 类似，为负值输入添加了一个非零输出。和其它修正类激活函数不同的是，它包括一个负指数项，从而防止静默神经元出现，导数收敛为零，从而提高学习效率。</p><h4 id="SELU"><a href="#SELU" class="headerlink" title="SELU"></a>SELU</h4><p><img src="http://xukeqiniu.xukeai.cn/c96eb8e0ba9a57e1f881319a9f8beaa2.png" alt=""><br>扩展指数线性单元（Scaled Exponential Linear Unit，SELU）是激活函数指数线性单元（ELU）的一个变种。其中λ和α是固定数值（分别为 1.0507 和 1.6726）。这些值背后的推论（零均值/单位方差）构成了自归一化神经网络的基础（SNN）。</p><h4 id="SReLU"><a href="#SReLU" class="headerlink" title="SReLU"></a>SReLU</h4><p><img src="http://xukeqiniu.xukeai.cn/b526ba7abbc43356810249d80ff3b3dc.png" alt=""><br>S 型整流线性激活单元（S-shaped Rectified Linear Activation Unit，SReLU）属于以 ReLU 为代表的整流激活函数族。它由三个分段线性函数组成。其中两种函数的斜度，以及函数相交的位置会在模型训练中被学习。</p><h4 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h4><p><img src="http://xukeqiniu.xukeai.cn/6429cec21cbbd5d65ace1caff1aff6ef.png" alt=""><br>Sigmoid 因其在 logistic 回归中的重要地位而被人熟知，值域在 0 到 1 之间。Logistic Sigmoid（或者按通常的叫法，Sigmoid）激活函数给神经网络引进了概率的概念。它的导数是非零的，并且很容易计算（是其初始输出的函数）。然而，在分类任务中，sigmoid 正逐渐被 Tanh 函数取代作为标准的激活函数，因为后者为奇函数（关于原点对称）。</p><h4 id="Hard-Sigmoid"><a href="#Hard-Sigmoid" class="headerlink" title="Hard Sigmoid"></a>Hard Sigmoid</h4><p><img src="http://xukeqiniu.xukeai.cn/9135d24de6c40ba1ddc4b930487ffdab.png" alt=""><br>Hard Sigmoid 是 Logistic Sigmoid 激活函数的分段线性近似。它更易计算，这使得学习计算的速度更快，尽管首次派生值为零可能导致静默神经元/过慢的学习速率（详见 ReLU）。</p><h4 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h4><p><img src="http://xukeqiniu.xukeai.cn/78e0eccb89a17610fc924f7a0f4d1d0d.png" alt=""><br>在分类任务中，双曲正切函数（Tanh）逐渐取代 Sigmoid 函数作为标准的激活函数，其具有很多神经网络所钟爱的特征。它是完全可微分的，反对称，对称中心在原点。为了解决学习缓慢和/或梯度消失问题，可以使用这个函数的更加平缓的变体（log-log、softsign、symmetrical sigmoid 等等）</p><h4 id="Hard-Tanh"><a href="#Hard-Tanh" class="headerlink" title="Hard Tanh"></a>Hard Tanh</h4><p><img src="http://xukeqiniu.xukeai.cn/ac7e6de0abe12f14d035b5b6c907cc0a.png" alt=""><br>Hard Tanh 是 Tanh 激活函数的线性分段近似。相较而言，它更易计算，这使得学习计算的速度更快，尽管首次派生值为零可能导致静默神经元/过慢的学习速率（详见 ReLU）。</p><h4 id="LeCun-Tanh"><a href="#LeCun-Tanh" class="headerlink" title="LeCun Tanh"></a>LeCun Tanh</h4><p><img src="http://xukeqiniu.xukeai.cn/2990bfbc681308727f6621c2fc41ee30.png" alt=""><br>LeCun Tanh（也被称作 Scaled Tanh）是 Tanh 激活函数的扩展版本。它具有以下几个可以改善学习的属性：f(± 1) = ±1；二阶导数在 x=1 最大化；且有效增益接近 1。</p><h4 id="ArcTan"><a href="#ArcTan" class="headerlink" title="ArcTan"></a>ArcTan</h4><p><img src="http://xukeqiniu.xukeai.cn/af66b328b33456285bea58d4bbe38f5f.png" alt=""><br>视觉上类似于双曲正切（Tanh）函数，ArcTan 激活函数更加平坦，这让它比其他双曲线更加清晰。在默认情况下，其输出范围在-π/2 和π/2 之间。其导数趋向于零的速度也更慢，这意味着学习的效率更高。但这也意味着，导数的计算比 Tanh 更加昂贵。</p><h4 id="Softsign"><a href="#Softsign" class="headerlink" title="Softsign"></a>Softsign</h4><p><img src="http://xukeqiniu.xukeai.cn/ceda6a6facb89736dc8da166fe3d2ae7.png" alt=""><br>Softsign 是 Tanh 激活函数的另一个替代选择。就像 Tanh 一样，Softsign 是反对称、去中心、可微分，并返回-1 和 1 之间的值。其更平坦的曲线与更慢的下降导数表明它可以更高效地学习。另一方面，导数的计算比 Tanh 更麻烦。</p><h4 id="SoftPlus"><a href="#SoftPlus" class="headerlink" title="SoftPlus"></a>SoftPlus</h4><p><img src="http://xukeqiniu.xukeai.cn/cfe2a2cf44ada80b1e0e5aad17272476.png" alt=""><br>作为 ReLU 的一个不错的替代选择，SoftPlus 能够返回任何大于 0 的值。与 ReLU 不同，SoftPlus 的导数是连续的、非零的，无处不在，从而防止出现静默神经元。然而，SoftPlus 另一个不同于 ReLU 的地方在于其不对称性，不以零为中心，这兴许会妨碍学习。此外，由于导数常常小于 1，也可能出现梯度消失的问题。</p><h4 id="Signum"><a href="#Signum" class="headerlink" title="Signum"></a>Signum</h4><p><img src="http://xukeqiniu.xukeai.cn/ce24d40aab0d55d09d27015cf4c6aaa6.png" alt=""><br>激活函数 Signum（或者简写为 Sign）是二值阶跃激活函数的扩展版本。它的值域为 [-1,1]，原点值是 0。尽管缺少阶跃函数的生物动机，Signum 依然是反对称的，这对激活函数来说是一个有利的特征。</p><h4 id="Bent-Identity"><a href="#Bent-Identity" class="headerlink" title="Bent Identity"></a>Bent Identity</h4><p><img src="http://xukeqiniu.xukeai.cn/f2c94ccdd5e2bec2b2b18e26674badfc.png" alt=""><br>激活函数 Bent Identity 是介于 Identity 与 ReLU 之间的一种折衷选择。它允许非线性行为，尽管其非零导数有效提升了学习并克服了与 ReLU 相关的静默神经元的问题。由于其导数可在 1 的任意一侧返回值，因此它可能容易受到梯度爆炸和消失的影响。</p><h4 id="Symmetrical-Sigmoid"><a href="#Symmetrical-Sigmoid" class="headerlink" title="Symmetrical Sigmoid"></a>Symmetrical Sigmoid</h4><p><img src="http://xukeqiniu.xukeai.cn/67bb1880981230394ee0a809c28971f5.png" alt=""><br>Symmetrical Sigmoid 是另一个 Tanh 激活函数的变种（实际上，它相当于输入减半的 Tanh）。和 Tanh 一样，它是反对称的、零中心、可微分的，值域在 -1 到 1 之间。它更平坦的形状和更慢的下降派生表明它可以更有效地进行学习。</p><h4 id="Log-Log"><a href="#Log-Log" class="headerlink" title="Log Log"></a>Log Log</h4><p><img src="http://xukeqiniu.xukeai.cn/bc9f28c60a6de50170d9c2155b5718c1.png" alt=""><br>Log Log 激活函数（由上图 f(x) 可知该函数为以 e 为底的嵌套指数函数）的值域为 [0,1]，Complementary Log Log 激活函数有潜力替代经典的 Sigmoid 激活函数。该函数饱和地更快，且零点值要高于 0.5。</p><h4 id="Gaussian"><a href="#Gaussian" class="headerlink" title="Gaussian"></a>Gaussian</h4><p><img src="http://xukeqiniu.xukeai.cn/1bcea31d208efd7e03943c66472ec853.png" alt=""><br>高斯激活函数（Gaussian）并不是径向基函数网络（RBFN）中常用的高斯核函数，高斯激活函数在多层感知机类的模型中并不是很流行。该函数处处可微且为偶函数，但一阶导会很快收敛到零。</p><h4 id="Absolute"><a href="#Absolute" class="headerlink" title="Absolute"></a>Absolute</h4><p><img src="http://xukeqiniu.xukeai.cn/234debdf62a4e4df7c1f5a38b5272ea4.png" alt=""><br>顾名思义，绝对值（Absolute）激活函数返回输入的绝对值。该函数的导数除了零点外处处有定义，且导数的量值处处为 1。这种激活函数一定不会出现梯度爆炸或消失的情况。</p><h4 id="Sinusoid"><a href="#Sinusoid" class="headerlink" title="Sinusoid"></a>Sinusoid</h4><p><img src="http://xukeqiniu.xukeai.cn/e0f252ab97d6457742b2a37bb7d17e4e.png" alt=""><br>如同余弦函数，Sinusoid（或简单正弦函数）激活函数为神经网络引入了周期性。该函数的值域为 [-1,1]，且导数处处连续。此外，Sinusoid 激活函数为零点对称的奇函数。</p><h4 id="Cos"><a href="#Cos" class="headerlink" title="Cos"></a>Cos</h4><p><img src="http://xukeqiniu.xukeai.cn/bef81df61d9ce282f76d4b055a32ab0f.png" alt=""><br>如同正弦函数，余弦激活函数（Cos/Cosine）为神经网络引入了周期性。它的值域为 [-1,1]，且导数处处连续。和 Sinusoid 函数不同，余弦函数为不以零点对称的偶函数。</p><h4 id="Sinc"><a href="#Sinc" class="headerlink" title="Sinc"></a>Sinc</h4><p><img src="http://xukeqiniu.xukeai.cn/af444d1bbb37e734d6317ebce0c57e08.png" alt=""><br>Sinc 函数（全称是 Cardinal Sine）在信号处理中尤为重要，因为它表征了矩形函数的傅立叶变换（Fourier transform）。作为一种激活函数，它的优势在于处处可微和对称的特性，不过它比较容易产生梯度消失的问题。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p> 从ReLU到Sinc，26种神经网络激活函数可视化<br> <a href="https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/" target="_blank" rel="noopener">visualising activation functions in neural networks</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> layer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之卷积层</title>
      <link href="/2018/01/22/DeepLearning/caffe/layer/%E5%8D%B7%E7%A7%AF%E5%B1%82/"/>
      <url>/2018/01/22/DeepLearning/caffe/layer/%E5%8D%B7%E7%A7%AF%E5%B1%82/</url>
      <content type="html"><![CDATA[<blockquote><p><img src="http://xukeqiniu.xukeai.cn/2490fe237f606073b367b7b03551461b.png" alt=""><br>从零开始，一步一步学习caffe的使用，期间贯穿深度学习和调参的相关知识！<br><a id="more"></a></p><h3 id="卷积层参数说明"><a href="#卷积层参数说明" class="headerlink" title="卷积层参数说明"></a>卷积层参数说明</h3></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"conv1"</span></span><br><span class="line">  type: <span class="string">"Convolution"</span></span><br><span class="line">  bottom: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"conv1"</span></span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: <span class="number">1</span>  #lr_mult: 学习率的系数，最终的学习率是这个数乘以solver.prototxt配置文件中的base_lr。如果有两个lr_mult, 则第一个表示权值的学习率，第二个表示偏置项的学习率。一般偏置项的学习率是权值学习率的两倍。</span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: <span class="number">2</span>  #偏置项的学习率</span><br><span class="line">  &#125;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: <span class="number">20</span> #卷积核（filter)的个数</span><br><span class="line">    kernel_size: <span class="number">5</span> #卷积核的大小</span><br><span class="line">    stride: <span class="number">1</span> #卷积核的步长，默认为<span class="number">1</span></span><br><span class="line">    pad: <span class="number">0</span> #扩充边缘，默认为<span class="number">0</span>，不扩充</span><br><span class="line">    group: <span class="number">2</span> #默认为<span class="number">0</span>（通达卷积的实现方式）</span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: <span class="string">"xavier"</span> #权值初始化。 默认为“constant<span class="string">",值全为0，很多时候我们用"</span>xavier<span class="string">"算法来进行初始化，也可以设置为”gaussian"</span></span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: <span class="string">"constant"</span> #偏置项的初始化。一般设置为<span class="string">"constant"</span>,值全为<span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="卷积配置参数意义"><a href="#卷积配置参数意义" class="headerlink" title="卷积配置参数意义"></a>卷积配置参数意义</h3><h4 id="卷积计算公式"><a href="#卷积计算公式" class="headerlink" title="卷积计算公式"></a>卷积计算公式</h4><p><img src="http://xukeqiniu.xukeai.cn/e9aca73c7366bb321c55954ba03891ca.png" alt=""></p><h4 id="卷积计算过程图示"><a href="#卷积计算过程图示" class="headerlink" title="卷积计算过程图示"></a>卷积计算过程图示</h4><p><img src="http://xukeqiniu.xukeai.cn/%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%A4%BA.gif" alt=""></p><p>上图取自CS231n，展示了三维卷积的计算过程，输入数据的三个维度，对应第二个卷积核生成了第二个<code>Feature Map</code></p><h4 id="Feature-Map大小计算"><a href="#Feature-Map大小计算" class="headerlink" title="Feature Map大小计算"></a>Feature Map大小计算</h4><p>如上图所示，输出<code>Feature Map</code>大小计算公式如下：</p><p><img src="http://xukeqiniu.xukeai.cn/b2f6ae03c269d8c5c08abaf1e47cc9de.png" alt=""></p><h3 id="权值与偏置的初始化方法"><a href="#权值与偏置的初始化方法" class="headerlink" title="权值与偏置的初始化方法"></a>权值与偏置的初始化方法</h3><p>caffe源文件<code>filler.hpp</code>中提供了7种权值初始化的方法。在计算机视觉的领域中权重参数的初始化常用<code>xavier</code>，偏置的初始化常用<code>constant</code>，并且初始化为0。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Filler&lt;Dtype&gt;* GetFiller(<span class="keyword">const</span> FillerParameter&amp; param) &#123;</span><br><span class="line">  <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; type = param.type();</span><br><span class="line">  <span class="keyword">if</span> (type == <span class="string">"constant"</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ConstantFiller&lt;Dtype&gt;(param);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type == <span class="string">"gaussian"</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> GaussianFiller&lt;Dtype&gt;(param);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type == <span class="string">"positive_unitball"</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> PositiveUnitballFiller&lt;Dtype&gt;(param);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type == <span class="string">"uniform"</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> UniformFiller&lt;Dtype&gt;(param);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type == <span class="string">"xavier"</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> XavierFiller&lt;Dtype&gt;(param);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type == <span class="string">"msra"</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> MSRAFiller&lt;Dtype&gt;(param);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (type == <span class="string">"bilinear"</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> BilinearFiller&lt;Dtype&gt;(param);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    CHECK(<span class="literal">false</span>) &lt;&lt; <span class="string">"Unknown filler name: "</span> &lt;&lt; param.type();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> (Filler&lt;Dtype&gt;*)(<span class="literal">NULL</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结合 <code>.prototxt</code> 文件中的 <code>FillerParameter</code>来看看怎么用</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">message FillerParameter &#123;</span><br><span class="line">  <span class="comment">// The filler type.</span></span><br><span class="line">  optional string type = 1 [default = 'constant'];</span><br><span class="line">  optional <span class="keyword">float</span> value = <span class="number">2</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// the value in constant filler</span></span><br><span class="line">  optional <span class="keyword">float</span> min = <span class="number">3</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// the min value in uniform filler</span></span><br><span class="line">  optional <span class="keyword">float</span> max = <span class="number">4</span> [<span class="keyword">default</span> = <span class="number">1</span>]; <span class="comment">// the max value in uniform filler</span></span><br><span class="line">  optional <span class="keyword">float</span> mean = <span class="number">5</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// the mean value in Gaussian filler</span></span><br><span class="line">  optional <span class="keyword">float</span> <span class="built_in">std</span> = <span class="number">6</span> [<span class="keyword">default</span> = <span class="number">1</span>]; <span class="comment">// the std value in Gaussian filler</span></span><br><span class="line">  <span class="comment">// The expected number of non-zero output weights for a given input in</span></span><br><span class="line">  <span class="comment">// Gaussian filler -- the default -1 means don't perform sparsification.</span></span><br><span class="line">  optional int32 sparse = <span class="number">7</span> [<span class="keyword">default</span> = <span class="number">-1</span>];</span><br><span class="line">  <span class="comment">// Normalize the filler variance by fan_in, fan_out, or their average.</span></span><br><span class="line">  <span class="comment">// Applies to 'xavier' and 'msra' fillers.</span></span><br><span class="line">  <span class="keyword">enum</span> VarianceNorm &#123;</span><br><span class="line">    FAN_IN = <span class="number">0</span>;</span><br><span class="line">    FAN_OUT = <span class="number">1</span>;</span><br><span class="line">    AVERAGE = <span class="number">2</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  optional VarianceNorm variance_norm = <span class="number">8</span> [<span class="keyword">default</span> = FAN_IN];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>常量初始化（<code>constant</code>）</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optional string type = 1 [default = 'constant'];</span><br><span class="line">optional <span class="keyword">float</span> value = <span class="number">2</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// the value in constant filler</span></span><br></pre></td></tr></table></figure><p>caffe中默认的初始化方式，它就是把权值或着偏置初始化为一个常数，具体是什么常数，自己可以定义。它的值等于上面的.prototxt文件中的 <code>value</code>的值，默认为<code>0</code>。</p><ul><li>高斯分布初始化（<code>gaussian</code>）</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><code>positive_unitball</code>初始化</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>均匀分布初始化（<code>uniform</code>）</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><code>xavier</code>初始化</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Normalize the filler variance by fan_in, fan_out, or their average.</span></span><br><span class="line"><span class="comment">// Applies to 'xavier' and 'msra' fillers.</span></span><br><span class="line"><span class="keyword">enum</span> VarianceNorm &#123;</span><br><span class="line">  FAN_IN = <span class="number">0</span>;</span><br><span class="line">  FAN_OUT = <span class="number">1</span>;</span><br><span class="line">  AVERAGE = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line">optional VarianceNorm variance_norm = <span class="number">8</span> [<span class="keyword">default</span> = FAN_IN];</span><br></pre></td></tr></table></figure><p><code>xavier</code>是和<code>relu</code>完美配合的初始化。<code>xavier</code>诞生时并没有用<code>relu</code>做例子，但是实际效果中<code>xavier</code>还是和<code>relu</code>很搭配的。</p><p><code>xavier</code>初始化定义为：定义参数所在层的输入维度为n，输出维度为m，那么参数将以均匀分布的方式在$[-\sqrt{\frac{6}{m+n}},\sqrt{\frac{6}{m+n}}]$的范围内进行初始化。具体的原理可以参靠<a href="https://zhuanlan.zhihu.com/p/22028079" target="_blank" rel="noopener">CNN数值——xavier</a><br>它的思想就是让一个神经元的输入权重的（当反向传播时，就变为输出了）的方差等于：1 / 输入的个数；这样做的目的就是可以让信息可以在网络中均匀的分布一下。<br>对于权值的分布：是一个让均值为0，方差为1 / 输入的个数 的均匀分布。<br>如果我们更注重前向传播的话，我们可以选择 <code>fan_in</code>，即正向传播的输入个数；如果更注重后向传播的话，我们选择 <code>fan_out</code>, 因为，等着反向传播的时候，<code>fan_out</code>就是神经元的输入个数；如果两者都考虑的话，那就选  <code>average</code> = (<code>fan_in</code> + <code>fan_out</code>) /2</p><ul><li><code>msra</code>初始化</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>双线性初始化（<code>bilinear</code>）</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://www.cnblogs.com/yinheyi/p/6165716.html" target="_blank" rel="noopener">caffe中权值初始化方法</a><br><a href="https://zhuanlan.zhihu.com/p/22028079" target="_blank" rel="noopener">CNN数值——xavier</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> layer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>caffe详解之数据层</title>
      <link href="/2018/01/20/DeepLearning/caffe/layer/%E6%95%B0%E6%8D%AE%E5%B1%82/"/>
      <url>/2018/01/20/DeepLearning/caffe/layer/%E6%95%B0%E6%8D%AE%E5%B1%82/</url>
      <content type="html"><![CDATA[<blockquote><p><img src="http://xukeqiniu.xukeai.cn/2490fe237f606073b367b7b03551461b.png" alt=""><br>从零开始，一步一步学习caffe的使用，期间贯穿深度学习和调参的相关知识！<br><a id="more"></a></p><h3 id="数据层参数说明"><a href="#数据层参数说明" class="headerlink" title="数据层参数说明"></a>数据层参数说明</h3></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"cifar"</span> #层的名字</span><br><span class="line">  type: <span class="string">"Data"</span> #表示是数据层</span><br><span class="line">  top: <span class="string">"data"</span>  #一般用bottom表示输入,top表示输出，多个top代表有多个输出</span><br><span class="line">  top: <span class="string">"label"</span> #此例中有两个输入（输入数据和最终的标准结果）</span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN #训练网络分为训练阶段（train）和测试阶段（test）,如果没写include则表示该层既在测试中，又在训练中！</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    mean_file: <span class="string">"examples/cifar10/mean.binaryproto"</span> #用一个配置文件来进行均值的操作</span><br><span class="line">    transform_param &#123;</span><br><span class="line">    scale: <span class="number">0.00390625</span> # <span class="number">1</span>/<span class="number">255</span>，用于像素点的归一化（<span class="number">0</span>，<span class="number">1</span>）之间</span><br><span class="line">    mirror: <span class="number">1</span>  # <span class="number">1</span>表示开启镜像，<span class="number">0</span>表示关闭，也可用ture和<span class="literal">false</span>来表示（图像数据集增强的一种方式）</span><br><span class="line">    crop_size: <span class="number">227</span> ## 剪裁一个 <span class="number">227</span>*<span class="number">227</span>的图块，在训练阶段随机剪裁，在测试阶段从中间裁剪（（图像数据集增强的一种方式））</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: <span class="string">"examples/cifar10/cifar10_train_lmdb"</span> #数据库来源（这里是采用lmdb的格式）</span><br><span class="line">    batch_size: <span class="number">64</span> #每次批处理的个数（批处理有助于算法优化，但是批处理的大小取决于GPU显存的大小）</span><br><span class="line">    backend: LMDB #选用数据的名称</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="三种常用的数据来源"><a href="#三种常用的数据来源" class="headerlink" title="三种常用的数据来源"></a>三种常用的数据来源</h3><h4 id="使用LMDB"><a href="#使用LMDB" class="headerlink" title="使用LMDB"></a>使用LMDB</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"mnist"</span></span><br><span class="line">  type: <span class="string">"Data"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"label"</span></span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    scale: <span class="number">0.00390625</span></span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: <span class="string">"examples/mnist/mnist_train_lmdb"</span></span><br><span class="line">    batch_size: <span class="number">64</span></span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使用HDF5"><a href="#使用HDF5" class="headerlink" title="使用HDF5"></a>使用HDF5</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"data"</span></span><br><span class="line">  type: <span class="string">"HDF5Data"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"label"</span></span><br><span class="line">  hdf5_data_param &#123;</span><br><span class="line">    source: <span class="string">"examples/hdf5_classification/data/train.txt"</span></span><br><span class="line">    batch_size: <span class="number">10</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="使用图片"><a href="#使用图片" class="headerlink" title="使用图片"></a>使用图片</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#/path/to/images/img3423.jpg <span class="number">2</span></span><br><span class="line">#/path/to/images/img3424.jpg <span class="number">13</span></span><br><span class="line">#/path/to/images/img3425.jpg <span class="number">8</span></span><br><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"data"</span></span><br><span class="line">  type: <span class="string">"ImageData"</span> #类型</span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"label"</span></span><br><span class="line">  transform_param &#123;</span><br><span class="line">    mirror: <span class="literal">false</span></span><br><span class="line">    crop_size: <span class="number">227</span></span><br><span class="line">    mean_file: <span class="string">"data/ilsvrc12/imagenet_mean.binaryproto"</span></span><br><span class="line">  &#125;</span><br><span class="line">  image_data_param &#123;</span><br><span class="line">    source: <span class="string">"examples/_temp/file_list.txt"</span> #按照前面三行所定义的方式（图片的路径 + 最终的分类结果）</span><br><span class="line">    batch_size: <span class="number">50</span></span><br><span class="line">    new_height: <span class="number">256</span> #如果设置就对图片进行resize操作</span><br><span class="line">    new_width: <span class="number">256</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Epochs-amp-amp-Batch-amp-amp-Iteration"><a href="#Epochs-amp-amp-Batch-amp-amp-Iteration" class="headerlink" title="Epochs &amp;&amp; Batch &amp;&amp; Iteration"></a>Epochs &amp;&amp; Batch &amp;&amp; Iteration</h3><ul><li><p><strong>当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个<code>epoch</code></strong>。一般情况下在迭代的过程中需要使用多次<code>epoch</code>防止模型欠拟合。</p></li><li><p>在不能将数据一次性通过神经网络的时候，就需要将数据集分成几个<code>batch</code>（Number of batches,简记为<code>batch_num</code>）。</p></li><li><p>一个 <code>batch</code> 中的样本总数(Batch Size,简记为<code>batch_size</code>)</p></li><li><p><strong><code>Iteration</code>是 <code>batch</code> 需要完成一个 <code>epoch</code> 的次数</strong>。记住：<strong>在一个 <code>epoch</code> 中，<code>batch_num</code> 和<code>Iteration</code>是相等的</strong>。比如对于一个有 2000 个训练样本的数据集。将 2000 个样本分成4个大小为 500 的 <code>batch_size</code>，那么完成一个 <code>epoch</code> 需要 4 个 <code>iteration</code>,对应的<code>batch_num</code>也是4。</p></li></ul><h3 id="图像识别中常用到的数据集"><a href="#图像识别中常用到的数据集" class="headerlink" title="图像识别中常用到的数据集"></a>图像识别中常用到的数据集</h3><h4 id="minist"><a href="#minist" class="headerlink" title="minist"></a>minist</h4><p><img src="http://xukeqiniu.xukeai.cn/ed887cc7ed9a38b82885e9ed75d0d8f5.png" alt=""><br>数字手写体识别数据集，常用来作为Deep Learning入门的基础数据集。它有60000个训练样本集和10000个测试样本集，每个样本图像的宽高为$28 \times 28$。此数据集是以二进制存储的，不能直接以图像格式查看，不过很容易找到将其转换成图像格式的工具。<br>数据集大小：~12MB<br>下载地址：<a href="http://yann.lecun.com/exdb/mnist/index.html" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/index.html</a></p><h4 id="Kaggle-cifar10"><a href="#Kaggle-cifar10" class="headerlink" title="Kaggle cifar10"></a>Kaggle cifar10</h4><p><img src="http://xukeqiniu.xukeai.cn/1a29cefb8b56e5aadd70fc7e429b24e1.png" alt=""><br>Kaggle cifar10分为训练数据集和测试数据集。训练集包含5万张图片。测试集包含30万张图片：其中有1万张图片用来计分，但为了防止人工标注测试集，里面另加了29万张不计分的图片。两个数据集都是png彩色图片，大小为$32 \times 32 \times 3$。训练集一共有10类图片，分别为飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。<br>数据集大小 ~700MB<br>下载地址：<a href="https://www.kaggle.com/c/cifar-10/data" target="_blank" rel="noopener">https://www.kaggle.com/c/cifar-10/data</a></p><h4 id="ImageNet"><a href="#ImageNet" class="headerlink" title="ImageNet"></a>ImageNet</h4><p><img src="http://xukeqiniu.xukeai.cn/12935f83014e81ceac1e2182b4b10ce4.png" alt=""><br>imageNet分为三个数据集：训练集，验证集，测试集。<br>Imagenet数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注，具体信息如下：<br>1）Total number of non-empty synsets: 21841<br>2）Total number of images: 14,197,122<br>3）Number of images with bounding box annotations: 1,034,908<br>4）Number of synsets with SIFT features: 1000<br>5）Number of images with SIFT features: 1.2 million<br>Imagenet数据集是目前深度学习图像领域应用得非常多的一个领域，关于图像分类、定位、检测等研究工作大多基于此数据集展开。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。<br>数据集大小：~1TB（ILSVRC2016比赛全部数据）<br>下载地址：<a href="http://www.image-net.org/about-stats" target="_blank" rel="noopener">http://www.image-net.org/about-stats</a></p><h4 id="COCO"><a href="#COCO" class="headerlink" title="COCO"></a>COCO</h4><p><img src="http://xukeqiniu.xukeai.cn/ade700cdc03b674bb46b212c5ae49a18.png" alt=""><br>COCO(Common Objects in Context)是一个新的图像识别、分割和图像语义数据集，它有如下特点：<br>1）Object segmentation<br>2）Recognition in Context<br>3）Multiple objects per image<br>4）More than 300,000 images<br>5）More than 2 Million instances<br>6）80 object categories<br>7）5 captions per image<br>8）Keypoints on 100,000 people</p><p>COCO数据集由微软赞助，其对于图像的标注信息不仅有类别、位置信息，还有对图像的语义文本描述，COCO数据集的开源使得近两三年来图像分割语义理解取得了巨大的进展，也几乎成为了图像语义理解算法性能评价的“标准”数据集。</p><p>Google开源的开源了图说生成模型show and tell就是在此数据集上测试的，想玩的可以下下来试试哈。</p><p>数据集大小：~40GB<br>下载地址：<a href="http://mscoco.org/" target="_blank" rel="noopener">http://mscoco.org/</a></p><h4 id="PASCAL-VOC"><a href="#PASCAL-VOC" class="headerlink" title="PASCAL VOC"></a>PASCAL VOC</h4><p> <img src="http://xukeqiniu.xukeai.cn/80b1577fb637d800a450554fb9fd7692.png" alt=""><br> PASCAL VOC挑战赛是视觉对象的分类识别和检测的一个基准测试，提供了检测算法和学习性能的标准图像注释数据集和标准的评估系统。PASCAL VOC图片集包括20个目录：人类；动物（鸟、猫、牛、狗、马、羊）；交通工具（飞机、自行车、船、公共汽车、小轿车、摩托车、火车）；室内（瓶子、椅子、餐桌、盆栽植物、沙发、电视）。PASCAL VOC挑战赛在2012年后便不再举办，但其数据集图像质量好，标注完备，非常适合用来测试算法性能。<br> 数据集大小：~2GB<br>下载地址：<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html" target="_blank" rel="noopener">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html</a></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>入门 | 神经网络训练中，Epoch、Batch Size和迭代傻傻分不清?<br><a href="https://www.jianshu.com/p/9990284bc4d5" target="_blank" rel="noopener">深度学习视觉领域常用数据集汇总</a></p>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> caffe </category>
          
          <category> layer </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>循环优化之最佳循环（Perfect Loop）</title>
      <link href="/2018/01/18/SDAccel/%E4%BC%98%E5%8C%96/kernel_opt/Loop_Perfect/"/>
      <url>/2018/01/18/SDAccel/%E4%BC%98%E5%8C%96/kernel_opt/Loop_Perfect/</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>前面我们介绍过了<a href="">循环优化之融合篇</a>现在我们再从另一个角度进行<code>loop</code>编程风格的优化。在<code>SDAccel</code>中，我们推荐的<code>loop</code>嵌套的形式是<code>perfect loop</code>(下图列出不同的loop循环结构)。对于非<code>perfect loop</code>的形式，我们可以采用一些方法将其优化为 <code>perfect loop</code> 的形式。同样以最近邻的程序，我们进行进一步的探讨。<br><img src="http://xukeqiniu.xukeai.cn/f87fea19bc5321ec6f9d6a3f2b751ffd.png" alt=""></p><a id="more"></a><h3 id="nearest-bad"><a href="#nearest-bad" class="headerlink" title="nearest_bad"></a>nearest_bad</h3><h4 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/*******************************************************************************</span></span><br><span class="line"><span class="comment">SDx Key Concept :</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    This is a nearest neighbor of a point example showcases how making a loop</span></span><br><span class="line"><span class="comment">    nest perfect or semi-perfect can help improve performance.</span></span><br><span class="line"><span class="comment">*******************************************************************************/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">Kernel Description : [Good case]</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    Finding the nearest neighbor of a point from a given set of points (of up to</span></span><br><span class="line"><span class="comment">    MAX_SIZE points). This is the bad version of the kernel where the loop nest</span></span><br><span class="line"><span class="comment">    is neither perfect nor semi-perfect. Hence automatic flattening of the loop</span></span><br><span class="line"><span class="comment">    nest does not happen and performance drops.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    Note : This is the bad version of the kernel. The good version is present in</span></span><br><span class="line"><span class="comment">           nearest_good.cl</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    Arguments :</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    int *in     (input )    --&gt; Input Points Array - represented as integer</span></span><br><span class="line"><span class="comment">    int *point  (input )    --&gt; Current Point for which the nearest neighbor</span></span><br><span class="line"><span class="comment">                                is found</span></span><br><span class="line"><span class="comment">    int *out    (output)    --&gt; Output Point</span></span><br><span class="line"><span class="comment">    int size    (input )    --&gt; Size of the input array</span></span><br><span class="line"><span class="comment">    int dim     (input )    --&gt; #Dimensions of the points</span></span><br><span class="line"><span class="comment">    Kernel Configuration :</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        MAX_DIM     - #Dimensions of the input points can be up to MAX_DIM</span></span><br><span class="line"><span class="comment">        MAX_SIZE    - Size of the input array can be up to MAX_SIZE</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Compute distances using unsigned long</span></span><br><span class="line"><span class="comment">// and to avoid square root operation.</span></span><br><span class="line"><span class="comment">// Maximum possible distance between two points</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INF_DIST ULONG_MAX</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SQUARE(x) ((x)*(x))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Maximum #Dimensions for a point</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_DIM 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Maximum size of point array</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_SIZE 16384</span></span><br><span class="line"></span><br><span class="line">__kernel __attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">nearest_bad</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> __global <span class="keyword">int</span> *in,     <span class="comment">// Input Points Array</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> __global <span class="keyword">int</span> *point,  <span class="comment">// Current Point</span></span></span></span><br><span class="line"><span class="function"><span class="params">    __global <span class="keyword">int</span> *out,          <span class="comment">// Output Point</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> size,                   <span class="comment">// Size of the input array</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> dim                     <span class="comment">// #Dimensions of the points</span></span></span></span><br><span class="line"><span class="function"><span class="params">    )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Local memory to store input and output matrices</span></span><br><span class="line">    <span class="comment">// Local memory is implemented as BRAM memory blocks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Holds the input array of points</span></span><br><span class="line">    <span class="keyword">int</span> in_local[MAX_SIZE][MAX_DIM] __attribute__((xcl_array_partition(complete, <span class="number">2</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Holds the point for which the nearest neighbor is to be found</span></span><br><span class="line">    <span class="keyword">int</span> point_local[MAX_DIM] __attribute__((xcl_array_partition(complete, <span class="number">1</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Holds the current nearest point</span></span><br><span class="line">    <span class="keyword">int</span> point_nearest[MAX_DIM] __attribute__((xcl_array_partition(complete, <span class="number">1</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// min_dist holds the minimum distance till now</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> min_dist = INF_DIST;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// curr_dist holds the value of distance between point_local and</span></span><br><span class="line">    <span class="comment">// the current point</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> curr_dist;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Burst reads on input from global memory, Points are read as</span></span><br><span class="line">    <span class="comment">// an array of integers and saved to in_local.</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    readInput: <span class="keyword">for</span>(<span class="keyword">int</span> itr = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; itr &lt; size*dim; itr++, j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j == dim) &#123; j = <span class="number">0</span>; i++;&#125;</span><br><span class="line">        in_local[i][j] = in[itr];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Burst reads the point for which nearest neighbor is to be found</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    readCurrPt: <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dim; i++)&#123;</span><br><span class="line">        point_local[i] = point[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Find the nearest neighbor</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// nearest1 loop goes over all the points</span></span><br><span class="line">    <span class="comment">// nearest2 loop finds the distance between point_local and the current</span></span><br><span class="line">    <span class="comment">// point. Based on this the minimum distance and the closest neighbor</span></span><br><span class="line">    <span class="comment">// are updated.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// In nearest2 loop, there are specific conditions like if(j==0).</span></span><br><span class="line">    <span class="comment">// This is for enabling loop flatten to improve performance.</span></span><br><span class="line">    nearest1: <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        curr_dist = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        __attribute__((xcl_pipeline_loop))</span><br><span class="line">        nearest2: <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; dim; j++) &#123;</span><br><span class="line">            curr_dist += SQUARE(point_local[j] - in_local[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(curr_dist &lt; min_dist) &#123;</span><br><span class="line">            min_dist = curr_dist;</span><br><span class="line"></span><br><span class="line">            __attribute__((opencl_unroll_hint))</span><br><span class="line">            nearest3: <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; MAX_DIM; k++) &#123;</span><br><span class="line">                point_nearest[k] = in_local[i][k];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Burst writes the nearest neighbor to out</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    wirteOuput: <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dim; i++) &#123;</span><br><span class="line">        out[i] = point_nearest[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="nearest-good"><a href="#nearest-good" class="headerlink" title="nearest_good"></a>nearest_good</h3><h4 id="源码-1"><a href="#源码-1" class="headerlink" title="源码"></a>源码</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*******************************************************************************</span></span><br><span class="line"><span class="comment">SDx Key Concept :</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    This is a nearest neighbor of a point example showcases how making a loop</span></span><br><span class="line"><span class="comment">    nest perfect or semi-perfect can help improve performance.</span></span><br><span class="line"><span class="comment">*******************************************************************************/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">Kernel Description : [Good case]</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    Finding the nearest neighbor of a point from a given set of points (of up to</span></span><br><span class="line"><span class="comment">    MAX_SIZE points). This example showcases how making a loop nest perfect or</span></span><br><span class="line"><span class="comment">    semi perfect can help improve performance.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    Note : This is the good version of the kernel. The bad version is present in</span></span><br><span class="line"><span class="comment">           nearest_bad.cl</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    Arguments :</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    int *in     (input )    --&gt; Input Points Array - represented as integer</span></span><br><span class="line"><span class="comment">    int *point  (input )    --&gt; Current Point for which the nearest neighbor</span></span><br><span class="line"><span class="comment">                                is found</span></span><br><span class="line"><span class="comment">    int *out    (output)    --&gt; Output Point</span></span><br><span class="line"><span class="comment">    int size    (input )    --&gt; Size of the input array</span></span><br><span class="line"><span class="comment">    int dim     (input )    --&gt; #Dimensions of the points</span></span><br><span class="line"><span class="comment">    Kernel Configuration :</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        MAX_DIM     - #Dimensions of the input points can be up to MAX_DIM</span></span><br><span class="line"><span class="comment">        MAX_SIZE    - Size of the input array can be up to MAX_SIZE</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;limits.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Compute distances using unsigned long</span></span><br><span class="line"><span class="comment">// and to avoid square root operation.</span></span><br><span class="line"><span class="comment">// Maximum possible distance between two points</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INF_DIST ULONG_MAX</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SQUARE(x) ((x)*(x))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Maximum #Dimensions for a point</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_DIM 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Maximum size of point array</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_SIZE 16384</span></span><br><span class="line"></span><br><span class="line">__kernel __attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">nearest_good</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> __global <span class="keyword">int</span> *in,     <span class="comment">// Input Points Array</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">const</span> __global <span class="keyword">int</span> *point,  <span class="comment">// Current Point</span></span></span></span><br><span class="line"><span class="function"><span class="params">    __global <span class="keyword">int</span> *out,          <span class="comment">// Output Point</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> size,                   <span class="comment">// Size of the input array</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> dim                     <span class="comment">// #Dimensions of the points</span></span></span></span><br><span class="line"><span class="function"><span class="params">    )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Local memory to store input and output matrices</span></span><br><span class="line">    <span class="comment">// Local memory is implemented as BRAM memory blocks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Holds the input array of points</span></span><br><span class="line">    <span class="keyword">int</span> in_local[MAX_SIZE][MAX_DIM] __attribute__((xcl_array_partition(complete, <span class="number">2</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Holds the point for which the nearest neighbor is to be found</span></span><br><span class="line">    <span class="keyword">int</span> point_local[MAX_DIM] __attribute__((xcl_array_partition(complete, <span class="number">1</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Holds the current nearest point</span></span><br><span class="line">    <span class="keyword">int</span> point_nearest[MAX_DIM] __attribute__((xcl_array_partition(complete, <span class="number">1</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// min_dist holds the minimum distance till now</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> min_dist = INF_DIST;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// curr_dist holds the value of distance between point_local and</span></span><br><span class="line">    <span class="comment">// the current point</span></span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">long</span> curr_dist;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Burst reads on input from global memory, Points are read as</span></span><br><span class="line">    <span class="comment">// an array of integers and saved to in_local.</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    readInput: <span class="keyword">for</span>(<span class="keyword">int</span> itr = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; itr &lt; size*dim; itr++, j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j == dim) &#123; j = <span class="number">0</span>; i++;&#125;</span><br><span class="line">        in_local[i][j] = in[itr];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Burst reads the point for which nearest neighbor is to be found</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    readCurrPt: <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dim; i++)&#123;</span><br><span class="line">        point_local[i] = point[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Find the nearest neighbor</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// nearest1 loop goes over all the points</span></span><br><span class="line">    <span class="comment">// nearest2 loop finds the distance between point_local and the current</span></span><br><span class="line">    <span class="comment">// point. Based on this the minimum distance and the closest neighbor</span></span><br><span class="line">    <span class="comment">// are updated.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// In nearest2 loop, there are specific conditions like if(j==0).</span></span><br><span class="line">    <span class="comment">// This is for enabling loop flatten to improve performance.</span></span><br><span class="line">    nearest1: <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        __attribute__((xcl_pipeline_loop))</span><br><span class="line">        nearest2: <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; dim; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(j == <span class="number">0</span>)  curr_dist = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">            curr_dist += SQUARE(point_local[j] - in_local[i][j]);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(j == dim<span class="number">-1</span> &amp;&amp; curr_dist &lt; min_dist) &#123;</span><br><span class="line">                min_dist = curr_dist;</span><br><span class="line">                nearest3: <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; MAX_DIM; k++) &#123;</span><br><span class="line">                    point_nearest[k] = in_local[i][k];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Burst writes the nearest neighbor to out</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    wirteOuput: <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dim; i++) &#123;</span><br><span class="line">        out[i] = point_nearest[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="对比分析"><a href="#对比分析" class="headerlink" title="对比分析"></a>对比分析</h3><h4 id="硬件仿真结果"><a href="#硬件仿真结果" class="headerlink" title="硬件仿真结果"></a>硬件仿真结果</h4><p><img src="http://xukeqiniu.xukeai.cn/741a623cd8d92e166db9d79d8e0d283a.png" alt=""></p><h4 id="performence分析"><a href="#performence分析" class="headerlink" title="performence分析"></a>performence分析</h4><ul><li><strong>nearest_bad</strong></li></ul><p><img src="http://xukeqiniu.xukeai.cn/df44559515195dcdeffb2cd6c94bed31.png" alt=""></p><ul><li><strong>nearest_good</strong></li></ul><p><img src="http://xukeqiniu.xukeai.cn/09694929eb1ce1ec9632ba64fe33d20a.png" alt=""></p><h4 id="资源占用分析"><a href="#资源占用分析" class="headerlink" title="资源占用分析"></a>资源占用分析</h4><ul><li><strong>nearest_bad</strong></li></ul><p><img src="http://xukeqiniu.xukeai.cn/fe2cec8057c709a7865d8acdf37e5210.png" alt=""></p><ul><li><strong>nearest_good</strong></li></ul><p><img src="http://xukeqiniu.xukeai.cn/73aede626cb25cbe72474268b7ad474e.png" alt=""></p><h4 id="参考代码风格"><a href="#参考代码风格" class="headerlink" title="参考代码风格"></a>参考代码风格</h4><p>下图基于此例子总结展示了如何将并列的for循环格式转换成为最佳的嵌套for循环。<br><img src="http://xukeqiniu.xukeai.cn/e4dd69ac1371e7a76792f6acda7ab40f.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Xilinx/SDAccel_Examples/tree/master/getting_started" target="_blank" rel="noopener">xilinx github SDAccel_Examples/getting_started </a></p><p><a href="http://xilinx.eetop.cn/viewnews-2830" target="_blank" rel="noopener">HLS视频教程18：FOR循环优化 — 嵌套的FOR循环</a></p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> kernel_opt </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> loop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>循环优化之融合篇（Fusion Loop）</title>
      <link href="/2018/01/18/SDAccel/%E4%BC%98%E5%8C%96/kernel_opt/Loop_Fusion/"/>
      <url>/2018/01/18/SDAccel/%E4%BC%98%E5%8C%96/kernel_opt/Loop_Fusion/</url>
      <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>通过最近邻算法的实现，展示在实际的优化中，如何巧妙的将嵌套的for循环进行融合。</p><h3 id="最近邻算法"><a href="#最近邻算法" class="headerlink" title="最近邻算法"></a>最近邻算法</h3><p>最近邻是机器学习中一种典型的算法，属于K紧邻算法的一种特殊形式。具体的实现过程如下图所示(图片取自百度百科)：<br><img src="http://xukeqiniu.xukeai.cn/25e2618899ac4119948b5ad214e4e12c.png" alt=""><br>从最近邻的计算角度来说，最重要的是从诸多已知点中，寻找对应当前点最近的一个点（默认采用欧几里得距离）。因此，我们主要在FPGA中实现寻找最近点的操作。</p><a id="more"></a><h3 id="嵌套for循环"><a href="#嵌套for循环" class="headerlink" title="嵌套for循环"></a>嵌套for循环</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_DIMS 5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment">// This is a simple implementation of a linear search algorithm. We use two</span></span><br><span class="line"> <span class="comment">// loops. The outer loop cycles through each of the search space and the inner</span></span><br><span class="line"> <span class="comment">// loop calculates the distance to a particular point.</span></span><br><span class="line"> <span class="comment">// len 指的是有多少已知点</span></span><br><span class="line"> <span class="comment">// dim 指的是每一个点的维度是多少</span></span><br><span class="line"> <span class="comment">// search_point 指的是确定的用于寻找最近邻的固定点（1个点，dim维）</span></span><br><span class="line"> <span class="comment">// point是随机的样本点（len个点，dim维）</span></span><br><span class="line"> <span class="comment">// out是search_point的最近邻点</span></span><br><span class="line">kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))) <span class="keyword">void</span></span><br><span class="line">nearest_neighbor(global <span class="keyword">int</span> *out, global <span class="keyword">const</span> <span class="keyword">int</span> *points,</span><br><span class="line">                      global <span class="keyword">const</span> <span class="keyword">int</span> *search_point, <span class="keyword">const</span> <span class="keyword">int</span> len,</span><br><span class="line">                      <span class="keyword">const</span> <span class="keyword">int</span> dim) &#123;</span><br><span class="line">    <span class="keyword">int</span> best_i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> best_dist = INT_MAX;</span><br><span class="line">    <span class="keyword">int</span> s_point[MAX_DIMS];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从global memory 搬移到 local memory</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt; dim; ++d) &#123;</span><br><span class="line">        s_point[d] = search_point[d];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历所有点与s_point的最近距离，记录最近距离以及第几个点</span></span><br><span class="line">    find_best:</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> p = <span class="number">0</span>; p &lt; len; ++p) &#123;</span><br><span class="line">        <span class="keyword">int</span> dist = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Calculate the distance in a n-dimensional space</span></span><br><span class="line">        __attribute__((xcl_pipeline_loop))</span><br><span class="line">        dist_calc:</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; dim; ++c) &#123;</span><br><span class="line">            <span class="keyword">int</span> dx = points[dim * p + c] - s_point[c];</span><br><span class="line">            dist += dx * dx;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (dist &lt; best_dist) &#123;</span><br><span class="line">            best_i = p;</span><br><span class="line">            best_dist = dist;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//将最近的距离点写回global memory</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    write_best:</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; dim; ++c) &#123;</span><br><span class="line">        out[c] = points[best_i * dim + c];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="嵌套for循环的融合"><a href="#嵌套for循环的融合" class="headerlink" title="嵌套for循环的融合"></a>嵌套for循环的融合</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_DIMS 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// This implementation fuses the distance calculation and the iteration through</span></span><br><span class="line"><span class="comment">// the search space into one loop.</span></span><br><span class="line"><span class="comment">// len 指的是有多少已知点</span></span><br><span class="line"><span class="comment">// dim 指的是每一个点的维度是多少</span></span><br><span class="line"><span class="comment">// search_point 指的是确定的用于寻找最近邻的固定点（1个点，dim维）</span></span><br><span class="line"><span class="comment">// point是随机的样本点（len个点，dim维）</span></span><br><span class="line"><span class="comment">// out是search_point的最近邻点</span></span><br><span class="line">kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))) <span class="keyword">void</span></span><br><span class="line">nearest_neighbor_loop_fusion(global <span class="keyword">int</span> *out, global <span class="keyword">const</span> <span class="keyword">int</span> *points,</span><br><span class="line">                       global <span class="keyword">const</span> <span class="keyword">int</span> *search_point, <span class="keyword">const</span> <span class="keyword">int</span> len,</span><br><span class="line">                       <span class="keyword">const</span> <span class="keyword">int</span> dim) &#123;</span><br><span class="line">    <span class="keyword">int</span> best_i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> best_dist = INT_MAX;</span><br><span class="line">    <span class="keyword">int</span> s_point[MAX_DIMS];</span><br><span class="line"></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt; dim; ++d) &#123;</span><br><span class="line">        s_point[d] = search_point[d];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> dist = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> iterations = len * dim;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// This loop iterates through each point and through each of the dimension.</span></span><br><span class="line">    <span class="comment">// The combined loop performs the same number of iterations as the previous</span></span><br><span class="line">    <span class="comment">// implementation but this approach give the compiler more opportunity to</span></span><br><span class="line">    <span class="comment">// optimize the operations.</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    find_best:</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> p = <span class="number">0</span>, c = <span class="number">0</span>, itr = <span class="number">0</span>; itr &lt; iterations; itr++) &#123;</span><br><span class="line">        <span class="keyword">int</span> dx = points[dim * p + c] - s_point[c];</span><br><span class="line">        dist += dx * dx;</span><br><span class="line">        <span class="comment">// Defines the end of the dimension calculation(The inner loop in the</span></span><br><span class="line">        <span class="comment">// previous example)</span></span><br><span class="line">        <span class="keyword">if</span> (c == dim - <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (dist &lt; best_dist) &#123;</span><br><span class="line">                best_i = p;</span><br><span class="line">                best_dist = dist;</span><br><span class="line">            &#125;</span><br><span class="line">            c = <span class="number">0</span>;</span><br><span class="line">            dist = <span class="number">0</span>;</span><br><span class="line">            p++;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            c++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    write_best:</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; dim; ++c) &#123;</span><br><span class="line">        out[c] = points[best_i * dim + c];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="软件仿真结果"><a href="#软件仿真结果" class="headerlink" title="软件仿真结果"></a>软件仿真结果</h4><p><img src="http://xukeqiniu.xukeai.cn/fd471014f85a0bd93247b5cc57c3b45c.png" alt=""></p><h4 id="performence分析"><a href="#performence分析" class="headerlink" title="performence分析"></a>performence分析</h4><ul><li><strong>nearest_neighbor</strong></li></ul><p><img src="http://xukeqiniu.xukeai.cn/18b52f20b924faf11ff03dc659d8b798.png" alt=""></p><ul><li><strong>nearest_neighbor_loop_fusion</strong></li></ul><p><img src="http://xukeqiniu.xukeai.cn/e3d7ba972921050fe62a3dcee6acaaf1.png" alt=""></p><h4 id="资源占用分析"><a href="#资源占用分析" class="headerlink" title="资源占用分析"></a>资源占用分析</h4><ul><li><strong>nearest_neighbor</strong></li></ul><p><img src="http://xukeqiniu.xukeai.cn/d21bb20737c6c8c08855e4964ff27df1.png" alt=""></p><ul><li><strong>nearest_neighbor_loop_fusion</strong></li></ul><p><img src="http://xukeqiniu.xukeai.cn/21bf811dd4e6fe52fa9e1e5fdb8f782b.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于嵌套的<code>for</code>循环，若是内层的<code>for</code>循环无法进行循环展开，那么我们只能将内层循环进行<code>pipeline</code>，而不能兼顾到外层循环的<code>pipeline</code>状况。编译器对于内层<code>for</code>循环进行<code>pipeline</code>后，会默认的去尝试能否对外层的<code>for</code>循环进行<code>flatten</code>，如果是<code>perfect loop</code> 或者 <code>semi-perfect loop</code>能够成功的<code>flatten</code>，但是对于<code>imperfect loop</code> 则不能。因此，为了避免<code>for</code>循环的嵌套带来的优化问题，一种很好的解决方式就是将<code>for</code>循环进行融合，直接对最外层的<code>for</code>循环进行<code>pipeline</code>处理。这样既能减少<code>Iteration Latency</code>。还能减少<code>FF</code>与<code>LUT</code>资源的利用率。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Xilinx/SDAccel_Examples/tree/master/getting_started" target="_blank" rel="noopener">xilinx github SDAccel_Examples/getting_started </a></p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> kernel_opt </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> loop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>经典设计结构之移位寄存器OpenCL实现</title>
      <link href="/2018/01/12/SDAccel/%E4%BC%98%E5%8C%96/kernel_opt/shift_register/"/>
      <url>/2018/01/12/SDAccel/%E4%BC%98%E5%8C%96/kernel_opt/shift_register/</url>
      <content type="html"><![CDATA[<blockquote class="blockquote-center"><br><br><img src="http://xukeqiniu.xukeai.cn/b5c947aad718d144ae96fb19c78f8211.png" alt=""><br><br>fir滤波器与移位寄存器的结构匹配度100%<br><br></blockquote><a id="more"></a><h3 id="fir滤波器原理"><a href="#fir滤波器原理" class="headerlink" title="fir滤波器原理"></a>fir滤波器原理</h3><p>FIR(Finite Impulse Response)滤波器：有限长单位冲激响应滤波器，又称为非递归型滤波器，是数字信号处理系统中最基本的元件，它可以在保证任意幅频特性的同时具有严格的线性相频特性，同时其单位抽样响应是有限长的，因而滤波器是稳定的系统。因此，FIR滤波器在通信、图像处理、模式识别等领域都有着广泛的应用。<br>在这里我们不具体谈FIR过多的原理知识，而是面向计算的优化，我们预先确定一组参数，来实现一个11阶的FIR滤波器。<br>计算流程如下：<br><img src="http://xukeqiniu.xukeai.cn/%E7%A7%BB%E4%BD%8D%E5%AF%84%E5%AD%98%E5%99%A8.gif" alt=""></p><h3 id="fir简单实现方式"><a href="#fir简单实现方式" class="headerlink" title="fir简单实现方式"></a>fir简单实现方式</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N_COEFF 11</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// A naive implementation of the Finite Impulse Response filter.</span></span><br><span class="line">__kernel __attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fir_naive</span><span class="params">(__global <span class="keyword">int</span>* <span class="keyword">restrict</span> output,</span></span></span><br><span class="line"><span class="function"><span class="params">               __global <span class="keyword">int</span>* <span class="keyword">restrict</span> signal,</span></span></span><br><span class="line"><span class="function"><span class="params">               __global <span class="keyword">int</span>* <span class="keyword">restrict</span> coeff,</span></span></span><br><span class="line"><span class="function"><span class="params">               <span class="keyword">long</span> signal_length)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> coeff_reg[N_COEFF];</span><br><span class="line">    read_coef: <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; N_COEFF ; i++) coeff_reg[i] = coeff[i];</span><br><span class="line"></span><br><span class="line">    outer_loop:</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; signal_length; j++) &#123;</span><br><span class="line">        <span class="keyword">int</span> acc = <span class="number">0</span>;</span><br><span class="line">        shift_loop:</span><br><span class="line">        __attribute__((xcl_pipeline_loop))</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = min(j,N_COEFF<span class="number">-1</span>); i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            acc += signal[j-i] * coeff_reg[i];</span><br><span class="line">        &#125;</span><br><span class="line">        output[j] = acc;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="fir移位寄存器实现方式"><a href="#fir移位寄存器实现方式" class="headerlink" title="fir移位寄存器实现方式"></a>fir移位寄存器实现方式</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// FIR using shift register</span></span><br><span class="line">__kernel __attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fir_shift_register</span><span class="params">(__global <span class="keyword">int</span>* <span class="keyword">restrict</span> output,</span></span></span><br><span class="line"><span class="function"><span class="params">                        __global <span class="keyword">int</span>* <span class="keyword">restrict</span> signal,</span></span></span><br><span class="line"><span class="function"><span class="params">                        __global <span class="keyword">int</span>* <span class="keyword">restrict</span> coeff,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">long</span> signal_length)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> coeff_reg[N_COEFF];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Partitioning of this array is required because the shift register</span></span><br><span class="line">    <span class="comment">// operation will need access to each of the values of the array in</span></span><br><span class="line">    <span class="comment">// the same clock. Without partitioning the operation will need to</span></span><br><span class="line">    <span class="comment">// be performed over multiple cycles because of the limited memory</span></span><br><span class="line">    <span class="comment">// ports available to the array.</span></span><br><span class="line">    <span class="keyword">int</span> shift_reg[N_COEFF] __attribute__((xcl_array_partition(complete, <span class="number">0</span>)));</span><br><span class="line"></span><br><span class="line">    init_loop:</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N_COEFF; i++) &#123;</span><br><span class="line">        shift_reg[i] = <span class="number">0</span>;</span><br><span class="line">        coeff_reg[i] = coeff[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    outer_loop:</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; signal_length; j++) &#123;</span><br><span class="line">        <span class="keyword">int</span> acc = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> x = signal[j];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// This is the shift register operation. The N_COEFF variable is defined</span></span><br><span class="line">        <span class="comment">// at compile time so the compiler knows the number of operations</span></span><br><span class="line">        <span class="comment">// performed by the loop. This loop does not require the unroll</span></span><br><span class="line">        <span class="comment">// attribute because the outer loop will be automatically pipelined so</span></span><br><span class="line">        <span class="comment">// the compiler will unroll this loop in the process.</span></span><br><span class="line">        shift_loop:</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = N_COEFF<span class="number">-1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i == <span class="number">0</span>) &#123;</span><br><span class="line">                acc += x * coeff_reg[<span class="number">0</span>];</span><br><span class="line">                shift_reg[<span class="number">0</span>] = x;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                shift_reg[i] = shift_reg[i<span class="number">-1</span>];</span><br><span class="line">                acc += shift_reg[i] * coeff_reg[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        output[j] = acc;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="移位寄存器应用总结"><a href="#移位寄存器应用总结" class="headerlink" title="移位寄存器应用总结"></a>移位寄存器应用总结</h3><p>在该例程中，巧妙的利用了移位寄存器的方法，对处理过的数据进行存储且移位，大幅度的减少了与global memory接口的频繁交互。移位寄存器在实现的过程中，需要注意的是因为牵扯到数组间的移位，因此一定要将数组切割成寄存器的形式，否则会产生carry dependency使得循环的II值变大。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Xilinx/SDAccel_Examples/tree/master/getting_started" target="_blank" rel="noopener">xilinx github SDAccel_Examples/getting_started </a></p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> kernel_opt </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> 移位寄存器 </tag>
            
            <tag> fir </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>经典设计结构之脉动阵列OpenCL实现</title>
      <link href="/2018/01/12/SDAccel/%E4%BC%98%E5%8C%96/kernel_opt/systolic_array/"/>
      <url>/2018/01/12/SDAccel/%E4%BC%98%E5%8C%96/kernel_opt/systolic_array/</url>
      <content type="html"><![CDATA[<blockquote class="blockquote-center"><br><br><img src="http://xukeqiniu.xukeai.cn/6d467ebf3a2314ec97aeb83d1dad5cfc.png" alt=""><br><br>脉动阵列，本身的核心概念就是让数据在运算单元的阵列中进行流动，减少访存的次数。<br>脉动阵列使得结构更加规整，布线更加统一，提高频率。<br><br></blockquote><a id="more"></a><h3 id="脉动阵列架构"><a href="#脉动阵列架构" class="headerlink" title="脉动阵列架构"></a>脉动阵列架构</h3><p><img src="http://xukeqiniu.xukeai.cn/6ea88b692aadc01c66a38210814213a0.png" alt=""><br>上图中上半部分是传统的计算系统的模型。一个处理单元（<code>PE</code>）从存储器（<code>memory</code>）读取数据，进行处理，然后再写回到存储器。这个系统的最大问题是：数据存取的速度往往大大低于数据处理的速度。因此，整个系统的处理能力（<code>MOPS</code>，每秒完成的操作）很大程度受限于访存的能力。脉动阵列架构用了一个很简单的方法：让数据尽量在处理单元中多流动一会儿。正如上图的下半部分所描述的，第一个数据首先进入第一个PE，经过处理以后被传递到下一个PE，同时第二个数据进入第一个PE。以此类推，当第一个数据到达最后一个PE，它已经被处理了多次。所以，脉动架构实际上是多次重用了输入数据。因此，它可以在消耗较小的<code>memory</code>带宽的情况下实现较高的运算吞吐率。当然，脉动架构还有其它一些好处，比如模块化的设计容易扩展，简单和规则的数据和控制流程，使用简单并且均匀的单元（<code>cell</code>），避免了全局广播和扇入（<code>fan-in</code>），以及快速的响应时间等等。<br>总结起来，脉动阵列架构有几个特征：</p><ul><li>由多个同构的<code>PE</code>构成，可以是一维或二维，串行、阵列或树的结构（现在我们看到的更多的是阵列形式）；</li><li><code>PE</code>功能相对简单，系统通过实现大量<code>PE</code>并行来提高运算的效率；</li><li><code>PE</code>只能向相邻的<code>PE</code>发送数据（在一些二维结构中，也可能有对角线方向的数据通道）。数据采用流水线的方式向“下游”流动，直到流出最后的<code>PE</code>。</li></ul><p>因此，脉动阵列架构是一种很特殊的设计，结构简单，实现成本低。但它灵活性较差，只适合特定运算。特别适合于卷积运算与矩阵运算。</p><h3 id="脉动阵列实现矩阵乘法"><a href="#脉动阵列实现矩阵乘法" class="headerlink" title="脉动阵列实现矩阵乘法"></a>脉动阵列实现矩阵乘法</h3><ul><li>脉动阵列矩阵乘法示意图</li></ul><p><img src="http://xukeqiniu.xukeai.cn/%E8%84%89%E5%8A%A8%E9%98%B5%E5%88%97%E7%A4%BA%E6%84%8F%E5%9B%BE.gif" alt=""></p><ul><li>源码实现</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//Maximum Array Size</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_SIZE 16</span></span><br><span class="line"></span><br><span class="line">__kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mmult</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span> *a,    <span class="comment">// Read-Only Matrix A</span></span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span> *b,    <span class="comment">// Read-Only Matrix B</span></span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span> *c,    <span class="comment">// Output Result</span></span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">int</span> a_row,    <span class="comment">// Matrix A Row Size</span></span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">int</span> a_col,    <span class="comment">// Matrix A Col Size</span></span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">int</span> b_col     <span class="comment">// Matrix B Col Size</span></span></span></span><br><span class="line"><span class="function"><span class="params">        )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> b_row = a_col;</span><br><span class="line">    <span class="keyword">int</span> c_row = a_row;</span><br><span class="line">    <span class="keyword">int</span> c_col = b_col;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Local memory to store input and output matrices</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> localA[MAX_SIZE][MAX_SIZE]  __attribute__((xcl_array_partition(complete, <span class="number">1</span>)));;</span><br><span class="line">    <span class="keyword">int</span> localB[MAX_SIZE][MAX_SIZE]  __attribute__((xcl_array_partition(complete, <span class="number">2</span>)));;</span><br><span class="line">    <span class="keyword">int</span> localC[MAX_SIZE][MAX_SIZE]  __attribute__((xcl_array_partition(complete, <span class="number">0</span>)));;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Burst reads on input matrices from global memory</span></span><br><span class="line">    <span class="comment">// Read Input A</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    readA: <span class="keyword">for</span>(<span class="keyword">int</span> loc = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; loc &lt; a_row*a_col; loc++, j++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(j == a_col) &#123; i++; j = <span class="number">0</span>;&#125;</span><br><span class="line">        localA[i][j] = a[loc];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Read Input B</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    readB: <span class="keyword">for</span>(<span class="keyword">int</span> loc = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; loc &lt; b_row*b_col; loc++, j++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(j == b_col) &#123; i++; j = <span class="number">0</span>; &#125;</span><br><span class="line">        localB[i][j] = b[loc];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Perform systolic matrix multiply</span></span><br><span class="line">    <span class="comment">// local matrices localA and localB have been partitioned in dimensions</span></span><br><span class="line">    <span class="comment">// 1 and 2 respectively. local matrix C has been partitioned completely</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// This partitioning enables to access MAX_SIZE elements in parallel in</span></span><br><span class="line">    <span class="comment">// the local matrices. Because of the mode of access of array elements,</span></span><br><span class="line">    <span class="comment">// we are able to perform MAX_SIZE*MAX_SIZE operations in parallel.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Note : i, j and k loops are interchanged.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// The top loop systolic1 runs only for a_col iterations instead of</span></span><br><span class="line">    <span class="comment">// MAX_SIZE like the inner loops. The inner loops have fixed loop</span></span><br><span class="line">    <span class="comment">// iteration counts to enable complete unroll</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// The following diagram explains how the matrix multiply happens</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">//        B_0        B_1        B_2        B_3</span></span><br><span class="line">    <span class="comment">//         |          |          |          |</span></span><br><span class="line">    <span class="comment">//         v          v          v          v</span></span><br><span class="line">    <span class="comment">//        ___        ___        ___        ___</span></span><br><span class="line">    <span class="comment">//       |   |      |   |      |   |      |   |</span></span><br><span class="line">    <span class="comment">//  A0_-&gt;|C00| ---- |C01| ---- |C02| ---- |C03|</span></span><br><span class="line">    <span class="comment">//       |___|      |___|      |___|      |___|</span></span><br><span class="line">    <span class="comment">//         |          |          |          |</span></span><br><span class="line">    <span class="comment">//        ___        ___        ___        ___</span></span><br><span class="line">    <span class="comment">//       |   |      |   |      |   |      |   |</span></span><br><span class="line">    <span class="comment">//  A1_-&gt;|C10| ---- |C11| ---- |C12| ---- |C13|</span></span><br><span class="line">    <span class="comment">//       |___|      |___|      |___|      |___|</span></span><br><span class="line">    <span class="comment">//         |          |          |          |</span></span><br><span class="line">    <span class="comment">//        ___        ___        ___        ___</span></span><br><span class="line">    <span class="comment">//       |   |      |   |      |   |      |   |</span></span><br><span class="line">    <span class="comment">//  A2_-&gt;|C20| ---- |C21| ---- |C22| ---- |C23|</span></span><br><span class="line">    <span class="comment">//       |___|      |___|      |___|      |___|</span></span><br><span class="line">    <span class="comment">//         |          |          |          |</span></span><br><span class="line">    <span class="comment">//        ___        ___        ___        ___</span></span><br><span class="line">    <span class="comment">//       |   |      |   |      |   |      |   |</span></span><br><span class="line">    <span class="comment">//  A3_-&gt;|C30| ---- |C31| ---- |C32| ---- |C33|</span></span><br><span class="line">    <span class="comment">//       |___|      |___|      |___|      |___|</span></span><br><span class="line"></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    systolic1: <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; a_col; k++) &#123;</span><br><span class="line">        systolic2: <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; MAX_SIZE; i++) &#123;</span><br><span class="line">            systolic3: <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; MAX_SIZE; j++) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Get previous sum</span></span><br><span class="line">                <span class="keyword">int</span> last = (k==<span class="number">0</span>) ? <span class="number">0</span> : localC[i][j];</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Update current sum</span></span><br><span class="line">                <span class="comment">// Handle boundary conditions</span></span><br><span class="line">                <span class="keyword">int</span> a_val = (i &lt; a_row &amp;&amp; k &lt; a_col)? localA[i][k] : <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">int</span> b_val = (k &lt; b_row &amp;&amp; j &lt; b_col)? localB[k][j] : <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">int</span> result = last + a_val*b_val;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Write back results</span></span><br><span class="line">                localC[i][j] = result;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Burst write from output matrices to global memory</span></span><br><span class="line">    <span class="comment">// Burst write from matrix C</span></span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    writeC: <span class="keyword">for</span>(<span class="keyword">int</span> loc = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; loc &lt; c_row*c_col; loc++, j++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(j == c_col) &#123; i++; j = <span class="number">0</span>; &#125;</span><br><span class="line">        c[loc] = localC[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h3><h3 id="脉动阵列实现卷积运算"><a href="#脉动阵列实现卷积运算" class="headerlink" title="脉动阵列实现卷积运算"></a>脉动阵列实现卷积运算</h3><h3 id="Google-TPU脉动阵列架构分析"><a href="#Google-TPU脉动阵列架构分析" class="headerlink" title="Google TPU脉动阵列架构分析"></a>Google TPU脉动阵列架构分析</h3><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://zhuanlan.zhihu.com/p/26522315" target="_blank" rel="noopener">脉动阵列 - 因Google TPU获得新生</a></p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> kernel_opt </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> 脉动阵列 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Seaborn学习笔记</title>
      <link href="/2018/01/09/Python/04-seaborn/"/>
      <url>/2018/01/09/Python/04-seaborn/</url>
      <content type="html"><![CDATA[<h2 id="Seaborn学习笔记"><a href="#Seaborn学习笔记" class="headerlink" title="Seaborn学习笔记"></a>Seaborn学习笔记</h2><p><img src="http://xukeqiniu.xukeai.cn/b46a61a77ccddf56426422e16c262a46.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/5257a902945a2ae575b2597a7e778194.png" alt=""></p><h3 id="Seaborn简介"><a href="#Seaborn简介" class="headerlink" title="Seaborn简介"></a>Seaborn简介</h3><p><code>Seaborn</code>是一种基于<code>matplotlib</code>的图形可视化python库。它提供了一种高度交互式界面，便于用户能够做出各种有吸引力的统计图表。<code>Seaborn</code>其实是<code>在matplotlib</code>的基础上进行了更高级的API封装，从而使得<strong>作图更加容易</strong>，在大多数情况下使用<code>Seaborn</code>就能做出很具有吸引力的图，而使用<code>matplotlib</code>就能制作具有更多特色的图。<strong>应该把<code>Seaborn</code>视为<code>matplotlib</code>的补充，而不是替代物</strong>。同时它能高度兼容<code>numpy</code>与<code>pandas</code>数据结构以及<code>scipy</code>与<code>statsmodels</code>等统计模式。掌握<code>Seaborn</code>能很大程度帮助我们<strong>更高效的观察数据与图表</strong>，并且更加深入了解它们。</p><p>其有如下特点：</p><ul><li>基于matplotlib aesthetics绘图风格，增加了一些绘图模式</li><li>增加调色板功能，利用色彩丰富的图像揭示您数据中的模式</li><li>运用数据子集绘制与比较单变量和双变量分布的功能</li><li>运用聚类算法可视化矩阵数据</li><li>灵活运用处理时间序列数据</li><li>利用网格建立复杂图像集</li></ul><a id="more"></a><h3 id="Seaborn样式"><a href="#Seaborn样式" class="headerlink" title="Seaborn样式"></a>Seaborn样式</h3><h4 id="matplotlib与seaborn绘图比较"><a href="#matplotlib与seaborn绘图比较" class="headerlink" title="matplotlib与seaborn绘图比较"></a>matplotlib与seaborn绘图比较</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sinplot</span><span class="params">(flip=<span class="number">1</span>)</span>:</span></span><br><span class="line">    x = np.linspace(<span class="number">0</span>, <span class="number">14</span>, <span class="number">100</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">7</span>):</span><br><span class="line">        plt.plot(x, np.sin(x + i * <span class="number">.5</span>) * (<span class="number">7</span> - i) * flip)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sinplot() <span class="comment">#采用matplotlib绘制</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_8_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.set() <span class="comment">#采用seaborn默认设置</span></span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_9_0.png" alt=""></p><h4 id="Seaborn-5种主题风格"><a href="#Seaborn-5种主题风格" class="headerlink" title="Seaborn 5种主题风格"></a>Seaborn 5种主题风格</h4><ul><li>darkgrid</li><li>whitegrid</li><li>dark</li><li>white</li><li>ticks</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.set_style(<span class="string">"darkgrid"</span>)</span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_11_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_12_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.set_style(<span class="string">"dark"</span>)</span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_13_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.set_style(<span class="string">"white"</span>)</span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_14_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.set_style(<span class="string">"ticks"</span>)</span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_15_0.png" alt=""></p><h4 id="用despine-移除轴线"><a href="#用despine-移除轴线" class="headerlink" title="用despine()移除轴线"></a>用despine()移除轴线</h4><p>样式<code>white</code>和<code>ticks</code>都可以通过去除<strong>上方和右方</strong>不必要的轴线来得到改善. 而这些是不可能在<code>matplotlib</code>里设置参数做到的,但是你可以调用seaborn的函数<code>despine()</code>来去除轴线:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sns.set_style(<span class="string">"ticks"</span>)</span><br><span class="line">sinplot()</span><br><span class="line">sns.despine() <span class="comment"># 去除上面与右面轴线</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_17_0.png" alt=""></p><p>有些布局也可以通过调整轴线距数据的偏移来改善,这也能在despine()里完成.当ticks不能覆盖轴线的整个范围时,trim参数可以限制显示的轴线的范围.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = np.random.normal(size=(<span class="number">20</span>, <span class="number">6</span>)) + np.arange(<span class="number">6</span>) / <span class="number">2</span> <span class="comment"># (20, 6) 二维数据</span></span><br><span class="line">f, ax = plt.subplots()</span><br><span class="line">sns.violinplot(data) <span class="comment"># 琴形图</span></span><br><span class="line">sns.despine(offset=<span class="number">10</span>,trim=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_19_0.png" alt=""></p><p>你也可能通过设置另外的参数来控制移除哪条轴线:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sns.set_style(<span class="string">"whitegrid"</span>)</span><br><span class="line">sns.boxplot(data=data, palette=<span class="string">"deep"</span>) <span class="comment">#箱型图</span></span><br><span class="line">sns.despine(left=<span class="keyword">True</span>) <span class="comment">#去除左边的轴线</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_21_0.png" alt=""></p><h4 id="临时设置图表样式"><a href="#临时设置图表样式" class="headerlink" title="临时设置图表样式"></a>临时设置图表样式</h4><p>尽管来回切换样式是很简单的,但是你也可以在<code>with</code>语句里用<code>axes_style()</code>函数来<strong>临时设置控制布局的参数</strong>.这也允许你<strong>用不同的风格来制作图表</strong>,这是一种常见的编程模式，使得控制样式和风格能够多变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> sns.axes_style(<span class="string">"darkgrid"</span>):</span><br><span class="line">    plt.subplot(<span class="number">211</span>)</span><br><span class="line">    sinplot()</span><br><span class="line">plt.subplot(<span class="number">212</span>)</span><br><span class="line">sinplot(<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_23_0.png" alt=""></p><h4 id="重载seaborn样式的元素"><a href="#重载seaborn样式的元素" class="headerlink" title="重载seaborn样式的元素"></a>重载seaborn样式的元素</h4><p>如果你想要自定义<code>seaborn</code>的样式,你可以用词典(<code>dictionary</code>)将一系列控制参数赋值给<code>axes_style()</code>函数和<code>set_style()</code>函数的<code>rc</code>参数里. 注意你<strong>只能通过这种方式重载样式定义的部分</strong>.(但是,更高级的<code>set()</code>函数可以处理包含任意<code>matplotlib</code>参数的词典)</p><p>如果你想要知道都包含了哪些参数,你可以调用没有参数的函数,它会返回当前设置:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.axes_style()</span><br></pre></td></tr></table></figure><pre><code>{&apos;axes.axisbelow&apos;: True, &apos;axes.edgecolor&apos;: &apos;.8&apos;, &apos;axes.facecolor&apos;: &apos;white&apos;, &apos;axes.grid&apos;: True, &apos;axes.labelcolor&apos;: &apos;.15&apos;, &apos;axes.linewidth&apos;: 1.0, &apos;figure.facecolor&apos;: &apos;white&apos;, &apos;font.family&apos;: [&apos;sans-serif&apos;], &apos;font.sans-serif&apos;: [&apos;Arial&apos;,  &apos;DejaVu Sans&apos;,  &apos;Liberation Sans&apos;,  &apos;Bitstream Vera Sans&apos;,  &apos;sans-serif&apos;], &apos;grid.color&apos;: &apos;.8&apos;, &apos;grid.linestyle&apos;: &apos;-&apos;, &apos;image.cmap&apos;: &apos;rocket&apos;, &apos;legend.frameon&apos;: False, &apos;legend.numpoints&apos;: 1, &apos;legend.scatterpoints&apos;: 1, &apos;lines.solid_capstyle&apos;: &apos;round&apos;, &apos;text.color&apos;: &apos;.15&apos;, &apos;xtick.color&apos;: &apos;.15&apos;, &apos;xtick.direction&apos;: &apos;out&apos;, &apos;xtick.major.size&apos;: 0.0, &apos;xtick.minor.size&apos;: 0.0, &apos;ytick.color&apos;: &apos;.15&apos;, &apos;ytick.direction&apos;: &apos;out&apos;, &apos;ytick.major.size&apos;: 0.0, &apos;ytick.minor.size&apos;: 0.0}</code></pre><p>然后你可以设置这些参数的不同版本:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.set_style(<span class="string">"darkgrid"</span>, &#123;<span class="string">"axes.facecolor"</span>: <span class="string">".9"</span>&#125;)</span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_27_0.png" alt=""></p><h4 id="使用plotting-context-和set-context-设置布局元素的规模"><a href="#使用plotting-context-和set-context-设置布局元素的规模" class="headerlink" title="使用plotting_context()和set_context()设置布局元素的规模"></a>使用<code>plotting_context()</code>和<code>set_context()</code>设置布局元素的规模</h4><p>布局元素的规模被独立的参数集合控制,这能让你使用相同的代码得到不同大小的规模合适的布局</p><p>首先让我们重新调用<code>set()</code>函数得到缺省设置:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.set()</span><br></pre></td></tr></table></figure><p>有4种预设好的上下文(context),按相对大小排序分别是:<code>paper</code>, <code>notebook</code>, <code>talk</code>,和<code>poster</code>.缺省的规模是<code>notebook</code>,上述的所有图表都是它.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sns.set_context(<span class="string">"paper"</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_32_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sns.set_context(<span class="string">"talk"</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_33_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sns.set_context(<span class="string">"poster"</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_34_0.png" alt=""></p><p>大部分你现在所稽首的样式函数都应该被转换成上下文函数.</p><p>你可以调用<code>set_context()</code>，将上下文的名字当作一个参数传入，然后你就可以通过提供一个写有各项设置值的词典重载上下文的参数。</p><p>在修改上下文时，你也可以单独修改字体大小。（更高级的<code>set()</code>里也可以这么做）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.set_context(<span class="string">"notebook"</span>, font_scale=<span class="number">1.5</span>, rc=&#123;<span class="string">"lines.linewidth"</span>: <span class="number">2.5</span>&#125;)</span><br><span class="line">sinplot()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_36_0.png" alt=""></p><h3 id="Seaborn配色方案"><a href="#Seaborn配色方案" class="headerlink" title="Seaborn配色方案"></a>Seaborn配色方案</h3><p>配色是图表设计里最重要的方面之一，因为如果配色方案好，它可以清晰展现数据的模式和规律，否则就会把这些规律和模式隐藏起来。<br><code>Seaborn</code>让选择和使用配色方案变得简单且适用于你工作的数据种类和你想要达到的可视化目标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">sns.set(rc=&#123;<span class="string">"figure.figsize"</span>: (<span class="number">6</span>, <span class="number">6</span>)&#125;)</span><br></pre></td></tr></table></figure><h4 id="调色板"><a href="#调色板" class="headerlink" title="调色板"></a>调色板</h4><ul><li>颜色很重要</li><li>color_palette()能传入任何Matplotlib所支持的颜色</li><li>color_palette()不写参数则默认颜色</li><li>set_palette()设置所有图的颜色</li></ul><h4 id="分类色板"><a href="#分类色板" class="headerlink" title="分类色板"></a>分类色板</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">current_palette = sns.color_palette()</span><br><span class="line">sns.palplot(current_palette)</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_41_0.png" alt=""></p><p>6个默认的颜色循环主题： <code>deep</code>, <code>muted</code>, <code>pastel</code>, <code>bright</code>, <code>dark</code>, <code>colorblind</code>.</p><h4 id="圆形画板"><a href="#圆形画板" class="headerlink" title="圆形画板"></a>圆形画板</h4><p>当你有六个以上的分类要区分时，最简单的方法就是在一个圆形的颜色空间中画出均匀间隔的颜色(这样的色调会保持亮度和饱和度不变)。这是大多数的当他们需要使用比当前默认颜色循环中设置的颜色更多时的默认方案。<br>最常用的方法是使用<code>hls</code>的颜色空间，这是<code>RGB</code>值的一个简单转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.color_palette(<span class="string">"hls"</span>, <span class="number">8</span>))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_44_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#应用调色板</span></span><br><span class="line">data = np.random.normal(size=(<span class="number">20</span>, <span class="number">8</span>)) + np.arange(<span class="number">8</span>) / <span class="number">2</span>  <span class="comment">#生成数据</span></span><br><span class="line">sns.boxplot(data=data,palette=sns.color_palette(<span class="string">"hls"</span>, <span class="number">8</span>))<span class="comment">#按照生成的颜色对应不同的分类</span></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1f2bc978&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_45_1.png" alt=""></p><p><code>hls_palette()</code>函数来控制颜色的亮度和饱和</p><ul><li>l-亮度 lightness</li><li>s-饱和 saturation</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.hls_palette(<span class="number">8</span>, l=<span class="number">.7</span>, s=<span class="number">.9</span>))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_47_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.color_palette(<span class="string">"Paired"</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_48_0.png" alt=""></p><h4 id="使用xkcd颜色自定义调色板"><a href="#使用xkcd颜色自定义调色板" class="headerlink" title="使用xkcd颜色自定义调色板"></a>使用xkcd颜色自定义调色板</h4><p><code>xkcd</code>包含了一套众包努力的针对随机<code>RGB</code>色的命名。产生了954个可以随时通过<code>xdcd_rgb</code>字典中调用的命名颜色。<br>可以通过<code>sns.xkcd_rgb</code>进行查看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], sns.xkcd_rgb[<span class="string">"pale red"</span>], lw=<span class="number">3</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">2</span>], sns.xkcd_rgb[<span class="string">"medium green"</span>], lw=<span class="number">3</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">3</span>], sns.xkcd_rgb[<span class="string">"denim blue"</span>], lw=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>[&lt;matplotlib.lines.Line2D at 0x1a1f3264e0&gt;]</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_50_1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">colors = [<span class="string">"windows blue"</span>, <span class="string">"amber"</span>, <span class="string">"greyish"</span>, <span class="string">"faded green"</span>, <span class="string">"dusty purple"</span>]</span><br><span class="line">sns.palplot(sns.xkcd_palette(colors))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_51_0.png" alt=""></p><h4 id="连续色板"><a href="#连续色板" class="headerlink" title="连续色板"></a>连续色板</h4><p>色彩随数据变换，比如数据越来越重要则颜色越来越深</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.color_palette(<span class="string">"Blues"</span>))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_53_0.png" alt=""></p><p>如果想要翻转渐变，可以在面板名称中添加一个_r后缀</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.color_palette(<span class="string">"BuGn_r"</span>))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_55_0.png" alt=""></p><h4 id="cubehelix-palette-调色板"><a href="#cubehelix-palette-调色板" class="headerlink" title="cubehelix_palette()调色板"></a><code>cubehelix_palette()</code>调色板</h4><p>色调线性变换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.color_palette(<span class="string">"cubehelix"</span>, <span class="number">8</span>))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_57_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.cubehelix_palette(<span class="number">8</span>, start=<span class="number">.5</span>, rot=<span class="number">-.75</span>))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_58_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.cubehelix_palette(<span class="number">8</span>, start=<span class="number">1.75</span>, rot=<span class="number">-.150</span>))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_59_0.png" alt=""></p><h4 id="light-palette-和dark-palette-调用定制连续调色板"><a href="#light-palette-和dark-palette-调用定制连续调色板" class="headerlink" title="light_palette() 和dark_palette()调用定制连续调色板"></a><code>light_palette()</code> 和<code>dark_palette()</code>调用定制连续调色板</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.light_palette(<span class="string">"green"</span>))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_61_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.dark_palette(<span class="string">"purple"</span>))</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_62_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.palplot(sns.light_palette(<span class="string">"navy"</span>, reverse=<span class="keyword">True</span>)) <span class="comment">#渐变翻转</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_63_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#应用调色板</span></span><br><span class="line">data = np.random.normal(size=(<span class="number">20</span>, <span class="number">8</span>)) + np.arange(<span class="number">8</span>) / <span class="number">2</span>  <span class="comment">#生成数据</span></span><br><span class="line">sns.boxplot(data=data,palette=sns.cubehelix_palette(<span class="number">8</span>, start=<span class="number">.5</span>, rot=<span class="number">-.75</span>))<span class="comment">#按照生成的颜色对应不同的分类</span></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1f1c59b0&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_64_1.png" alt=""></p><h3 id="Seaborn变量分析绘图"><a href="#Seaborn变量分析绘图" class="headerlink" title="Seaborn变量分析绘图"></a>Seaborn变量分析绘图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats, integrate</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set(color_codes=<span class="keyword">True</span>)</span><br><span class="line">np.random.seed(sum(map(ord, <span class="string">"distributions"</span>)))</span><br></pre></td></tr></table></figure><h4 id="使用sns-distplot-函数画直方图"><a href="#使用sns-distplot-函数画直方图" class="headerlink" title="使用sns.distplot()函数画直方图"></a>使用<code>sns.distplot()</code>函数画直方图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.normal(size=<span class="number">100</span>)</span><br><span class="line">sns.distplot(x,kde=<span class="keyword">False</span>) <span class="comment">#distplot()函数会根据输入数据自动绘制直方图</span></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1fb2a240&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_68_1.png" alt=""></p><p>你也可以通过bins自己划分直方图的切分粒度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(x, bins=<span class="number">20</span>, kde=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a202ebdd8&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_70_1.png" alt=""></p><p>通过<code>fit</code>查看数据分布的情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.gamma(<span class="number">6</span>, size=<span class="number">200</span>)</span><br><span class="line">sns.distplot(x, kde=<span class="keyword">False</span>, fit=stats.gamma)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a204df5f8&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_72_1.png" alt=""></p><h4 id="使用jointplot-函数绘制散点图"><a href="#使用jointplot-函数绘制散点图" class="headerlink" title="使用jointplot()函数绘制散点图"></a>使用<code>jointplot()</code>函数绘制散点图</h4><p>观测两个变量之间的分布关系最好用散点图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成数据</span></span><br><span class="line">mean, cov = [<span class="number">0</span>, <span class="number">1</span>], [(<span class="number">1</span>, <span class="number">.5</span>), (<span class="number">.5</span>, <span class="number">1</span>)] <span class="comment">#自定义均值与协方差</span></span><br><span class="line">data = np.random.multivariate_normal(mean, cov, <span class="number">200</span>) <span class="comment">#生成200个数据</span></span><br><span class="line">df = pd.DataFrame(data, columns=[<span class="string">"x"</span>, <span class="string">"y"</span>]) <span class="comment">#通过pandas读入数据</span></span><br><span class="line">print(df.head())</span><br></pre></td></tr></table></figure><pre><code>          x         y0  0.585042  1.1626821  0.722117  2.1415802  0.120990  0.4988213 -0.795773  2.0852614 -0.614260  2.215906</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.jointplot(x=<span class="string">"x"</span>, y=<span class="string">"y"</span>, data=df)</span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.JointGrid at 0x1a206a5b00&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_75_1.png" alt=""></p><p>通过<code>kind=&quot;hex&quot;</code>使散点图具备<strong>透视性</strong>，更加容易查看数据的散点分布密度情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x, y = np.random.multivariate_normal(mean, cov, <span class="number">1000</span>).T</span><br><span class="line"><span class="keyword">with</span> sns.axes_style(<span class="string">"white"</span>):</span><br><span class="line">    sns.jointplot(x=x, y=y, kind=<span class="string">"hex"</span>, color=<span class="string">"k"</span>)</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_77_0.png" alt=""></p><h4 id="使用pairplot-函数绘制关系图"><a href="#使用pairplot-函数绘制关系图" class="headerlink" title="使用pairplot()函数绘制关系图"></a>使用<code>pairplot()</code>函数绘制关系图</h4><p><strong>两不同变量比较绘制散点图，变量自身比较绘制直方图</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iris = sns.load_dataset(<span class="string">"iris"</span>) <span class="comment">#载入鸢尾花数据集</span></span><br><span class="line">sns.pairplot(iris) <span class="comment">#绘制</span></span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.PairGrid at 0x1a20ef4588&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_79_1.png" alt=""></p><h3 id="Seaborn回归分析绘图"><a href="#Seaborn回归分析绘图" class="headerlink" title="Seaborn回归分析绘图"></a>Seaborn回归分析绘图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set(color_codes=<span class="keyword">True</span>)</span><br><span class="line">np.random.seed(sum(map(ord, <span class="string">"regression"</span>)))</span><br><span class="line"></span><br><span class="line">tips = sns.load_dataset(<span class="string">"tips"</span>) <span class="comment"># 导入tips数据集</span></span><br><span class="line"></span><br><span class="line">tips.head() <span class="comment">#查看数据集</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/00a68e9a952c8e724174122657262e3b.png" alt=""></p><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>total_bill</th><br>      <th>tip</th><br>      <th>sex</th><br>      <th>smoker</th><br>      <th>day</th><br>      <th>time</th><br>      <th>size</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>16.99</td><br>      <td>1.01</td><br>      <td>Female</td><br>      <td>No</td><br>      <td>Sun</td><br>      <td>Dinner</td><br>      <td>2</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>10.34</td><br>      <td>1.66</td><br>      <td>Male</td><br>      <td>No</td><br>      <td>Sun</td><br>      <td>Dinner</td><br>      <td>3</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>21.01</td><br>      <td>3.50</td><br>      <td>Male</td><br>      <td>No</td><br>      <td>Sun</td><br>      <td>Dinner</td><br>      <td>3</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>23.68</td><br>      <td>3.31</td><br>      <td>Male</td><br>      <td>No</td><br>      <td>Sun</td><br>      <td>Dinner</td><br>      <td>2</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>24.59</td><br>      <td>3.61</td><br>      <td>Female</td><br>      <td>No</td><br>      <td>Sun</td><br>      <td>Dinner</td><br>      <td>4</td><br>    </tr><br>  </tbody><br></table><br></div><h4 id="regplot-绘制回归关系图"><a href="#regplot-绘制回归关系图" class="headerlink" title="regplot()绘制回归关系图"></a><code>regplot()</code>绘制回归关系图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#采用regplot绘制拟合的数据线</span></span><br><span class="line">sns.regplot(x=<span class="string">"total_bill"</span>, y=<span class="string">"tip"</span>, data=tips) <span class="comment">#x轴代表花的钱的数据，y轴对应给小费的数据</span></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x22d8d8db518&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_84_1.png" alt=""></p><h4 id="lmplot-绘制回归关系图"><a href="#lmplot-绘制回归关系图" class="headerlink" title="lmplot()绘制回归关系图"></a><code>lmplot()</code>绘制回归关系图</h4><p><code>lmplot</code>是一种集合基础绘图与基于数据建立回归模型的绘图方法。旨在创建一个<strong>方便拟合数据集回归模型的绘图方法</strong>，利用<code>hue</code>、<code>col</code>、<code>row</code>参数来控制绘图变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seaborn.lmplot(x, y, data, hue=<span class="keyword">None</span>, col=<span class="keyword">None</span>, row=<span class="keyword">None</span>, palette=<span class="keyword">None</span>, col_wrap=<span class="keyword">None</span>, size=<span class="number">5</span>, aspect=<span class="number">1</span>, markers=<span class="string">'o'</span>, sharex=<span class="keyword">True</span>, sharey=<span class="keyword">True</span>, hue_order=<span class="keyword">None</span>, col_order=<span class="keyword">None</span>, row_order=<span class="keyword">None</span>, legend=<span class="keyword">True</span>, legend_out=<span class="keyword">True</span>, x_estimator=<span class="keyword">None</span>, x_bins=<span class="keyword">None</span>, x_ci=<span class="string">'ci'</span>, scatter=<span class="keyword">True</span>, fit_reg=<span class="keyword">True</span>, ci=<span class="number">95</span>, n_boot=<span class="number">1000</span>, units=<span class="keyword">None</span>, order=<span class="number">1</span>, logistic=<span class="keyword">False</span>, lowess=<span class="keyword">False</span>, robust=<span class="keyword">False</span>, logx=<span class="keyword">False</span>, x_partial=<span class="keyword">None</span>, y_partial=<span class="keyword">None</span>, truncate=<span class="keyword">False</span>, x_jitter=<span class="keyword">None</span>, y_jitter=<span class="keyword">None</span>, scatter_kws=<span class="keyword">None</span>, line_kws=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li>hue, col, row : strings #定义数据子集的变量，并在不同的图像子集中绘制</li><li>size : scalar, optional #定义子图的高度</li><li>markers : matplotlib marker code or list of marker codes, optional #定义散点的图标</li><li>col_wrap : int, optional  #设置每行子图数量</li><li>order : int, optional    #多项式回归，设定指数</li><li>logistic : bool, optional #逻辑回归</li><li>logx : bool, optional    #转化为log(x)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#研究小费tips与总消费金额total_bill在吸烟与不吸烟人之间的关系</span></span><br><span class="line">g = sns.lmplot(x=<span class="string">"total_bill"</span>, y=<span class="string">"tip"</span>, hue=<span class="string">"smoker"</span>, data=tips,palette=<span class="string">"Set1"</span>)</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_88_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#研究在不同星期下，消费总额与消费的回归关系</span></span><br><span class="line"><span class="comment"># col|hue控制子图不同的变量day，col_wrap控制每行子图数量，size控制子图高度</span></span><br><span class="line">g = sns.lmplot(x=<span class="string">"total_bill"</span>, y=<span class="string">"tip"</span>, col=<span class="string">"day"</span>, hue=<span class="string">"day"</span>,data=tips, col_wrap=<span class="number">2</span>, size=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_89_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pokemon=pd.read_csv(<span class="string">'../dataset/Pokemon.csv'</span>) <span class="comment">#载入宠物小精灵战斗力数据集</span></span><br><span class="line">pokemon.head()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/75b789cefa55954428acf20da1a31b84.png" alt=""></p><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>#</th><br>      <th>Name</th><br>      <th>Type 1</th><br>      <th>Type 2</th><br>      <th>Total</th><br>      <th>HP</th><br>      <th>Attack</th><br>      <th>Defense</th><br>      <th>Sp. Atk</th><br>      <th>Sp. Def</th><br>      <th>Speed</th><br>      <th>Generation</th><br>      <th>Legendary</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1</td><br>      <td>Bulbasaur</td><br>      <td>Grass</td><br>      <td>Poison</td><br>      <td>318</td><br>      <td>45</td><br>      <td>49</td><br>      <td>49</td><br>      <td>65</td><br>      <td>65</td><br>      <td>45</td><br>      <td>1</td><br>      <td>False</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2</td><br>      <td>Ivysaur</td><br>      <td>Grass</td><br>      <td>Poison</td><br>      <td>405</td><br>      <td>60</td><br>      <td>62</td><br>      <td>63</td><br>      <td>80</td><br>      <td>80</td><br>      <td>60</td><br>      <td>1</td><br>      <td>False</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3</td><br>      <td>Venusaur</td><br>      <td>Grass</td><br>      <td>Poison</td><br>      <td>525</td><br>      <td>80</td><br>      <td>82</td><br>      <td>83</td><br>      <td>100</td><br>      <td>100</td><br>      <td>80</td><br>      <td>1</td><br>      <td>False</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>3</td><br>      <td>VenusaurMega Venusaur</td><br>      <td>Grass</td><br>      <td>Poison</td><br>      <td>625</td><br>      <td>80</td><br>      <td>100</td><br>      <td>123</td><br>      <td>122</td><br>      <td>120</td><br>      <td>80</td><br>      <td>1</td><br>      <td>False</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>4</td><br>      <td>Charmander</td><br>      <td>Fire</td><br>      <td>NaN</td><br>      <td>309</td><br>      <td>39</td><br>      <td>52</td><br>      <td>43</td><br>      <td>60</td><br>      <td>50</td><br>      <td>65</td><br>      <td>1</td><br>      <td>False</td><br>    </tr><br>  </tbody><br></table><br></div><p><img src="http://xukeqiniu.xukeai.cn/d3cb1254e4c281c743e4c402bc3ee7d9.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#观察每一代攻击与防御的分布，利用二次多项式逼近</span></span><br><span class="line">sns.lmplot(x=<span class="string">"Defense"</span>, y=<span class="string">"Attack"</span>,data=pokemon,col=<span class="string">"Generation"</span>, hue=<span class="string">"Generation"</span>,col_wrap=<span class="number">3</span>, size=<span class="number">3</span>,order=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x22d8bce32e8&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_92_1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#继续在同一图中观察不同代的sp.Atk,Sp.Def线性关系</span></span><br><span class="line">sns.lmplot(x=<span class="string">"Sp. Atk"</span>, y=<span class="string">"Sp. Def"</span>, data=pokemon, hue=<span class="string">'Generation'</span>, size=<span class="number">5</span>,order=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x22d8be4b5f8&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_93_1.png" alt=""></p><h3 id="Seaborn分类分析绘图"><a href="#Seaborn分类分析绘图" class="headerlink" title="Seaborn分类分析绘图"></a>Seaborn分类分析绘图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set(style=<span class="string">"whitegrid"</span>, color_codes=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(sum(map(ord, <span class="string">"categorical"</span>)))</span><br><span class="line">titanic = sns.load_dataset(<span class="string">"titanic"</span>) <span class="comment">#导入泰坦尼克数据集</span></span><br><span class="line">tips = sns.load_dataset(<span class="string">"tips"</span>) <span class="comment">#导入小费数据集</span></span><br><span class="line">iris = sns.load_dataset(<span class="string">"iris"</span>) <span class="comment">#导入鸢尾花数据集</span></span><br></pre></td></tr></table></figure><h4 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.stripplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, data=tips)</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_98_0.png" alt=""></p><p>问题：<strong>有重叠，无法看见数据的密度</strong>。</p><ul><li>解决方法一：通过<code>jitter</code>抖动</li></ul><p>抖动是平时可视化中的常用的观察“密度”的方法，除了使用参数抖动，特定的抖动需求也可以用numpy在数据上处理实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.stripplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, data=tips, jitter=<span class="keyword">True</span>) <span class="comment"># jitter抖动</span></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x22d8a3216a0&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_101_1.png" alt=""></p><ul><li>解决方法二：通过<code>swarmplot()</code>函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.swarmplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, data=tips)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x22d87f3b128&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_103_1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.swarmplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, hue=<span class="string">"sex"</span>,data=tips) <span class="comment">#hue 参数控制分组绘图</span></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x22d8a428860&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_104_1.png" alt=""></p><h4 id="箱型图"><a href="#箱型图" class="headerlink" title="箱型图"></a>箱型图</h4><p>箱形图（Box-plot）又称为盒须图、盒式图或箱线图，是一种<strong>用作显示一组数据分散情况资料的统计图</strong>。因形状如箱子而得名。</p><p><img src="http://xukeqiniu.xukeai.cn/f17c7a91ac25ffa688b5359c1a652180.png" alt=""></p><p>如上图所示，标示了图中每条线表示的含义，其中应用到了分位值（数）的概念。<br>主要包含六个数据节点，将一组数据从大到小排列，分别计算出他的上边缘，上四分位数Q3，中位数，下四分位数Q1，下边缘，还有一个异常值。</p><p>举例说明，以下是箱形图的具体例子：<br><img src="http://xukeqiniu.xukeai.cn/ce523c9da97187aa135dec1c743ad5b6.png" alt=""><br>这组数据显示出：</p><ul><li>最小值(minimum)=5</li><li>下四分位数(Q1)=7</li><li>中位数(Med–也就是Q2)=8.5</li><li>上四分位数(Q3)=9</li><li>最大值(maximum)=10</li><li>平均值=8</li><li>四分位间距=Q3-Q1=2 (即ΔQ)</li><li>最大值区间： Q3+1.5ΔQ = 12</li><li>最小值区间： Q1-1.5ΔQ = 4</li><li>mild outlier = 3.5</li><li>extreme outlier = 0.5</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.boxplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, hue=<span class="string">"time"</span>, data=tips)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x22d8bbd7240&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_108_1.png" alt=""></p><h4 id="琴形图"><a href="#琴形图" class="headerlink" title="琴形图"></a>琴形图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seaborn.violinplot(x=<span class="keyword">None</span>, y=<span class="keyword">None</span>, hue=<span class="keyword">None</span>, data=<span class="keyword">None</span>, order=<span class="keyword">None</span>, hue_order=<span class="keyword">None</span>, bw=<span class="string">'scott'</span>, cut=<span class="number">2</span>, scale=<span class="string">'area'</span>, scale_hue=<span class="keyword">True</span>, gridsize=<span class="number">100</span>, width=<span class="number">0.8</span>, inner=<span class="string">'box'</span>, split=<span class="keyword">False</span>, orient=<span class="keyword">None</span>, linewidth=<span class="keyword">None</span>, color=<span class="keyword">None</span>, palette=<span class="keyword">None</span>, saturation=<span class="number">0.75</span>, ax=<span class="keyword">None</span>, **kwargs)</span><br></pre></td></tr></table></figure><ul><li>split: bool, optional #琴形图是否从中间分开两部分</li><li>scale: {“area”, “count”, “width”}, optional #用于调整琴形图的宽带。<ul><li>area——每个琴图拥有相同的面域；</li><li>count——根据样本数量来调节宽度；</li><li>width——每个琴图则拥有相同的宽度。</li></ul></li><li>inner: {“box”, “quartile”, “point”, “stick”, None}, optional #控制琴图内部数据点的形态。<ul><li>box——绘制微型 boxplot；</li><li>quartiles——绘制四分位的分布；</li><li>point/stick——绘制点或小竖条。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.violinplot(x=<span class="string">"total_bill"</span>, y=<span class="string">"day"</span>, hue=<span class="string">"time"</span>, data=tips)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x22d8a9f97b8&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_112_1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.violinplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, hue=<span class="string">"sex"</span>, data=tips, split=<span class="keyword">True</span>) <span class="comment">#split: bool, optional #琴形图是否从中间分开两部分</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_113_0.png" alt=""></p><h4 id="条形图"><a href="#条形图" class="headerlink" title="条形图"></a>条形图</h4><p>显示值的集中趋势可以用条形图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.barplot(x=<span class="string">"sex"</span>, y=<span class="string">"survived"</span>, hue=<span class="string">"class"</span>, data=titanic)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x22d8a5bc358&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_115_1.png" alt=""></p><h4 id="点图"><a href="#点图" class="headerlink" title="点图"></a>点图</h4><p>点图可以更好的描述变化<strong>差异</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.pointplot(x=<span class="string">"sex"</span>, y=<span class="string">"survived"</span>, hue=<span class="string">"class"</span>, data=titanic)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x22d8a5bcda0&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_117_1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#详细指定属性值</span></span><br><span class="line">sns.pointplot(x=<span class="string">"class"</span>, y=<span class="string">"survived"</span>, hue=<span class="string">"sex"</span>, data=titanic,</span><br><span class="line">              palette=&#123;<span class="string">"male"</span>: <span class="string">"g"</span>, <span class="string">"female"</span>: <span class="string">"m"</span>&#125;, <span class="comment">#  指定颜色</span></span><br><span class="line">              markers=[<span class="string">"^"</span>, <span class="string">"o"</span>],  <span class="comment"># 指定点样式</span></span><br><span class="line">              linestyles=[<span class="string">"-"</span>, <span class="string">"--"</span>]); <span class="comment"># 指定线型样式</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_118_0.png" alt=""></p><h4 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#琴型图 + 分散点图</span></span><br><span class="line">sns.violinplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, data=tips, inner=<span class="keyword">None</span>)</span><br><span class="line">sns.swarmplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, data=tips, color=<span class="string">"w"</span>, alpha=<span class="number">.5</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x22d8a3f4908&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_120_1.png" alt=""></p><h4 id="多层面板分类图"><a href="#多层面板分类图" class="headerlink" title="多层面板分类图"></a>多层面板分类图</h4><p><code>factorplot()</code>函数是对各种图形的一个<strong>更高级别的API封装</strong>，在Seaborn中非常常用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seaborn.factorplot(x=<span class="keyword">None</span>, y=<span class="keyword">None</span>, hue=<span class="keyword">None</span>, data=<span class="keyword">None</span>, row=<span class="keyword">None</span>, col=<span class="keyword">None</span>, col_wrap=<span class="keyword">None</span>, estimator=&lt;function mean&gt;, ci=<span class="number">95</span>, n_boot=<span class="number">1000</span>, units=<span class="keyword">None</span>, order=<span class="keyword">None</span>, hue_order=<span class="keyword">None</span>, row_order=<span class="keyword">None</span>, col_order=<span class="keyword">None</span>, kind=<span class="string">'point'</span>, size=<span class="number">4</span>, aspect=<span class="number">1</span>, orient=<span class="keyword">None</span>, color=<span class="keyword">None</span>, palette=<span class="keyword">None</span>, legend=<span class="keyword">True</span>, legend_out=<span class="keyword">True</span>, sharex=<span class="keyword">True</span>, sharey=<span class="keyword">True</span>, margin_titles=<span class="keyword">False</span>, facet_kws=<span class="keyword">None</span>, **kwargs)</span><br></pre></td></tr></table></figure><p>参数说明：</p><ul><li>x,y  数据集变量（变量名）</li><li>hue  控制分组绘图（变量名）</li><li>date 数据集 （数据集名）</li><li>row,col 更多分类变量进行平铺显示 （变量名）</li><li>col_wrap 每行的最高平铺数 （整数）</li><li>estimator 在每个分类中进行矢量到标量的映射 （矢量）</li><li>ci 置信区间 （浮点数或None）</li><li>n_boot 计算置信区间时使用的引导迭代次数 （整数）</li><li>units 采样单元的标识符，用于执行多级引导和重复测量设计 （数据变量或向量数据）</li><li>order, hue_order 对应排序列表 （字符串列表）</li><li>row_order, col_order 对应排序列表 （字符串列表）</li><li>kind : 可选：point 默认, bar 柱形图, count 频次, box 箱体, violin 提琴, strip 散点，swarm 分散点</li><li>size 每个面的高度（英寸） （标量）</li><li>aspect 纵横比 （标量）</li><li>orient 方向 （”v”/“h”）</li><li>color 颜色 （matplotlib颜色）</li><li>palette 调色板 （seaborn颜色色板或字典）</li><li>legend hue的信息面板 （True/False）</li><li>legend_out 是否扩展图形，并将信息框绘制在中心右边 （True/False）</li><li>share{x,y} 共享轴线 （True/False）</li><li>facet_kws FacetGrid的其他参数 （字典）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.factorplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, hue=<span class="string">"smoker"</span>, data=tips) <span class="comment">#默认是点图</span></span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x22d8a79def0&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_124_1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.factorplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, hue=<span class="string">"smoker"</span>, data=tips, kind=<span class="string">"bar"</span>) <span class="comment">#绘制条形图</span></span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x22d8a648748&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_125_1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.factorplot(x=<span class="string">"day"</span>, y=<span class="string">"total_bill"</span>, hue=<span class="string">"smoker"</span>,</span><br><span class="line">               col=<span class="string">"time"</span>, data=tips, kind=<span class="string">"swarm"</span>) <span class="comment">#绘制分散点图</span></span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x22d8a867be0&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_126_1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.factorplot(x=<span class="string">"time"</span>, y=<span class="string">"total_bill"</span>, hue=<span class="string">"smoker"</span>,</span><br><span class="line">               col=<span class="string">"day"</span>, data=tips, kind=<span class="string">"box"</span>, size=<span class="number">4</span>, aspect=<span class="number">.5</span>) <span class="comment">#绘制箱型图</span></span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x22d8a8bcb00&gt;</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_127_1.png" alt=""></p><h3 id="Seaborn热图绘制"><a href="#Seaborn热图绘制" class="headerlink" title="Seaborn热图绘制"></a>Seaborn热图绘制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns;</span><br><span class="line">sns.set()</span><br></pre></td></tr></table></figure><h4 id="热图基础"><a href="#热图基础" class="headerlink" title="热图基础"></a>热图基础</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seaborn.heatmap(data, vmin=<span class="keyword">None</span>, vmax=<span class="keyword">None</span>, cmap=<span class="keyword">None</span>, center=<span class="keyword">None</span>, robust=<span class="keyword">False</span>, annot=<span class="keyword">None</span>, fmt=<span class="string">'.2g'</span>, annotkws=<span class="keyword">None</span>, linewidths=<span class="number">0</span>, linecolor=<span class="string">'white'</span>, cbar=<span class="keyword">True</span>, cbarkws=<span class="keyword">None</span>, cbar_ax=<span class="keyword">None</span>, square=<span class="keyword">False</span>, ax=<span class="keyword">None</span>, xticklabels=<span class="keyword">True</span>, yticklabels=<span class="keyword">True</span>, mask=<span class="keyword">None</span>, **kwargs)</span><br></pre></td></tr></table></figure><ul><li>data：矩阵数据集，可以使numpy的数组（array），如果是pandas的dataframe，则df的index/column信息会分别对应到heatmap的columns和rows</li><li>linewidths,热力图矩阵之间的间隔大小</li><li>vmax,vmin, 图例中最大值和最小值的显示值，没有该参数时默认不显示</li><li>cmap：matplotlib的colormap名称或颜色对象；如果没有提供，默认为cubehelix map (数据集为连续数据集时) 或 RdBu_r (数据集为离散数据集时)</li><li>center:将数据设置为图例中的均值数据，即图例中心的数据值；通过设置center值，可以调整生成的图像颜色的整体深浅；设置center数据时，如果有数据溢出，则手动设置的vmax、vmin会自动改变</li><li>xticklabels: 如果是True，则绘制dataframe的列名。如果是False，则不绘制列名。如果是列表，则绘制列表中的内容作为xticklabels。 如果是整数n，则绘制列名，但每个n绘制一个label。 默认为True。</li><li>yticklabels: 如果是True，则绘制dataframe的行名。如果是False，则不绘制行名。如果是列表，则绘制列表中的内容作为yticklabels。 如果是整数n，则绘制列名，但每个n绘制一个label。 默认为True。默认为True。</li><li>annotate的缩写，annot默认为False，当annot为True时，在heatmap中每个方格写入数据</li><li>annot_kws，当annot为True时，可设置各个参数，包括大小，颜色，加粗，斜体字等</li><li>fmt，格式设置</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">uniform_data = np.random.rand(<span class="number">3</span>, <span class="number">3</span>) <span class="comment">#生成数据</span></span><br><span class="line"><span class="keyword">print</span> (uniform_data)</span><br><span class="line">heatmap = sns.heatmap(uniform_data) <span class="comment"># 生成热力图</span></span><br></pre></td></tr></table></figure><pre><code>[[ 0.64272796  0.0229858   0.21897478] [ 0.41076627  0.28860677  0.94805105] [ 0.96513582  0.57781451  0.96400349]]</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_133_1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 改变颜色映射的值范围</span></span><br><span class="line">ax = sns.heatmap(uniform_data, vmin=<span class="number">0.2</span>, vmax=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_134_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#为以0为中心的数据绘制一张热图</span></span><br><span class="line">ax = sns.heatmap(uniform_data, center=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_135_0.png" alt=""></p><h4 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flights = sns.load_dataset(<span class="string">"flights"</span>) <span class="comment">#加载航班数据集</span></span><br><span class="line">flights.head() <span class="comment">#显示部分数据</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/b234c8b563f6749f423d4a13e735fab4.png" alt=""></p><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>year</th><br>      <th>month</th><br>      <th>passengers</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1949</td><br>      <td>January</td><br>      <td>112</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>1949</td><br>      <td>February</td><br>      <td>118</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>1949</td><br>      <td>March</td><br>      <td>132</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>1949</td><br>      <td>April</td><br>      <td>129</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>1949</td><br>      <td>May</td><br>      <td>121</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flights = flights.pivot(<span class="string">"month"</span>, <span class="string">"year"</span>, <span class="string">"passengers"</span>) <span class="comment">#修改数据排列</span></span><br><span class="line">flights.head()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/bf0b138ded9277313858a25433d88a4a.png" alt=""></p><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th>year</th><br>      <th>1949</th><br>      <th>1950</th><br>      <th>1951</th><br>      <th>1952</th><br>      <th>1953</th><br>      <th>1954</th><br>      <th>1955</th><br>      <th>1956</th><br>      <th>1957</th><br>      <th>1958</th><br>      <th>1959</th><br>      <th>1960</th><br>    </tr><br>    <tr><br>      <th>month</th><br>      <th></th><br>      <th></th><br>      <th></th><br>      <th></th><br>      <th></th><br>      <th></th><br>      <th></th><br>      <th></th><br>      <th></th><br>      <th></th><br>      <th></th><br>      <th></th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>January</th><br>      <td>112</td><br>      <td>115</td><br>      <td>145</td><br>      <td>171</td><br>      <td>196</td><br>      <td>204</td><br>      <td>242</td><br>      <td>284</td><br>      <td>315</td><br>      <td>340</td><br>      <td>360</td><br>      <td>417</td><br>    </tr><br>    <tr><br>      <th>February</th><br>      <td>118</td><br>      <td>126</td><br>      <td>150</td><br>      <td>180</td><br>      <td>196</td><br>      <td>188</td><br>      <td>233</td><br>      <td>277</td><br>      <td>301</td><br>      <td>318</td><br>      <td>342</td><br>      <td>391</td><br>    </tr><br>    <tr><br>      <th>March</th><br>      <td>132</td><br>      <td>141</td><br>      <td>178</td><br>      <td>193</td><br>      <td>236</td><br>      <td>235</td><br>      <td>267</td><br>      <td>317</td><br>      <td>356</td><br>      <td>362</td><br>      <td>406</td><br>      <td>419</td><br>    </tr><br>    <tr><br>      <th>April</th><br>      <td>129</td><br>      <td>135</td><br>      <td>163</td><br>      <td>181</td><br>      <td>235</td><br>      <td>227</td><br>      <td>269</td><br>      <td>313</td><br>      <td>348</td><br>      <td>348</td><br>      <td>396</td><br>      <td>461</td><br>    </tr><br>    <tr><br>      <th>May</th><br>      <td>121</td><br>      <td>125</td><br>      <td>172</td><br>      <td>183</td><br>      <td>229</td><br>      <td>234</td><br>      <td>270</td><br>      <td>318</td><br>      <td>355</td><br>      <td>363</td><br>      <td>420</td><br>      <td>472</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ax = sns.heatmap(flights) <span class="comment">#绘制热图</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_139_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ax = sns.heatmap(flights, annot=<span class="keyword">True</span>,fmt=<span class="string">"d"</span>) <span class="comment">#在heatmap中每个方格写入数据，按照整数形式</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_140_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ax = sns.heatmap(flights, linewidths=<span class="number">.5</span>) <span class="comment">#热力图矩阵之间的间隔大小</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_141_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ax = sns.heatmap(flights, cmap=<span class="string">"YlGnBu"</span>) <span class="comment">#修改热图颜色</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_142_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ax = sns.heatmap(flights, cbar=<span class="keyword">False</span>) <span class="comment">#不显示热图图例</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/pandas_output_143_0.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://seaborn.pydata.org/tutorial/aesthetics.html#aesthetics-tutorial" target="_blank" rel="noopener">Style functions</a></p><p><a href="http://seaborn.pydata.org/tutorial/color_palettes.html#palette-tutorial" target="_blank" rel="noopener">Color palettes</a></p><p><a href="http://seaborn.pydata.org/tutorial/distributions.html#distribution-tutorial" target="_blank" rel="noopener">Distribution plots</a></p><p><a href="http://seaborn.pydata.org/tutorial/categorical.html#categorical-tutorial" target="_blank" rel="noopener">Categorical plots</a></p><p><a href="http://seaborn.pydata.org/tutorial/regression.html#regression-tutorial" target="_blank" rel="noopener">Regression plots</a></p><p><a href="http://seaborn.pydata.org/tutorial/axis_grids.html#grid-tutorial" target="_blank" rel="noopener">Axis grid objects</a><br><a href="https://zhuanlan.zhihu.com/p/24464836" target="_blank" rel="noopener">10分钟python图表绘制</a></p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Seaborn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pandas学习笔记</title>
      <link href="/2018/01/07/Python/03-Pandas/"/>
      <url>/2018/01/07/Python/03-Pandas/</url>
      <content type="html"><![CDATA[<h2 id="Pandas学习笔记"><a href="#Pandas学习笔记" class="headerlink" title="Pandas学习笔记"></a>Pandas学习笔记</h2><p><img src="http://xukeqiniu.xukeai.cn/0c22d321140e5e4bb6a6704609678ce8.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/707ad11afbcd17e7bba41544d3e265a8.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Pandas数据结构"><a href="#Pandas数据结构" class="headerlink" title="Pandas数据结构"></a>Pandas数据结构</h3><h4 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h4><p><code>Series</code>是<strong>一维的数据结构</strong>。</p><p>通过list构建Series</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ser_obj =pd.Series(range(<span class="number">10</span>,<span class="number">15</span>))</span><br><span class="line">print(type(ser_obj)) <span class="comment"># &lt;class 'pandas.core.series.Series'&gt;</span></span><br><span class="line">print(ser_obj)</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.series.Series&apos;&gt;0    101    112    123    134    14dtype: int32</code></pre><p>获取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(type(ser_obj.values)) <span class="comment"># &lt;class 'numpy.ndarray'&gt;</span></span><br><span class="line">print(ser_obj.values) <span class="comment"># [10 11 12 13 14]</span></span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;numpy.ndarray&apos;&gt;[10 11 12 13 14]</code></pre><p>获取索引</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(type(ser_obj.index)) <span class="comment"># &lt;class 'pandas.core.indexes.range.RangeIndex'&gt;</span></span><br><span class="line">print(ser_obj.index) <span class="comment"># RangeIndex(start=0, stop=5, step=1)</span></span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.indexes.range.RangeIndex&apos;&gt;RangeIndex(start=0, stop=5, step=1)</code></pre><p>注意<strong>索引对象不可变</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 索引对象不可变</span></span><br><span class="line">ser_obj.index[<span class="number">0</span>] = <span class="number">2</span></span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)&lt;ipython-input-53-ce46badf9dd7&gt; in &lt;module&gt;()----&gt; 1 ser_obj.index[0] = 2G:\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in __setitem__(self, key, value)   1668   1669     def __setitem__(self, key, value):-&gt; 1670         raise TypeError(&quot;Index does not support mutable operations&quot;)   1671   1672     def __getitem__(self, key):TypeError: Index does not support mutable operations</code></pre><p>预览数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(ser_obj.head(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><pre><code>0    101    112    12dtype: int32</code></pre><p>通过索引获取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(ser_obj[<span class="number">0</span>]) <span class="comment"># 10</span></span><br></pre></td></tr></table></figure><pre><code>10</code></pre><p>索引与数据的对应关系仍保持在数组运算的结果中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(ser_obj &gt; <span class="number">12</span>)</span><br><span class="line">print(ser_obj[ser_obj &gt; <span class="number">12</span>])</span><br></pre></td></tr></table></figure><pre><code>0    False1    False2    False3     True4     Truedtype: bool3    134    14dtype: int32</code></pre><p>整合代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过list构建Series</span></span><br><span class="line">ser_obj =pd.Series(range(<span class="number">10</span>,<span class="number">15</span>))</span><br><span class="line">print(type(ser_obj)) <span class="comment"># &lt;class 'pandas.core.series.Series'&gt;</span></span><br><span class="line">print(ser_obj)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">print(type(ser_obj.values)) <span class="comment"># &lt;class 'numpy.ndarray'&gt;</span></span><br><span class="line">print(ser_obj.values) <span class="comment"># [10 11 12 13 14]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取索引</span></span><br><span class="line">print(type(ser_obj.index)) <span class="comment"># &lt;class 'pandas.core.indexes.range.RangeIndex'&gt;</span></span><br><span class="line">print(ser_obj.index) <span class="comment"># RangeIndex(start=0, stop=5, step=1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预览数据</span></span><br><span class="line">print(ser_obj.head(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#通过索引获取数据</span></span><br><span class="line">print(ser_obj[<span class="number">0</span>]) <span class="comment"># 10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 索引与数据的对应关系仍保持在数组运算的结果中</span></span><br><span class="line">print(ser_obj &gt; <span class="number">12</span>)</span><br><span class="line">print(ser_obj[ser_obj &gt; <span class="number">12</span>])</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.series.Series&apos;&gt;0    101    112    123    134    14dtype: int32&lt;class &apos;numpy.ndarray&apos;&gt;[10 11 12 13 14]&lt;class &apos;pandas.core.indexes.range.RangeIndex&apos;&gt;RangeIndex(start=0, stop=5, step=1)0    101    112    12dtype: int32100    False1    False2    False3     True4     Truedtype: bool3    134    14dtype: int32</code></pre><p>通过dict构建Series(注意：<strong>字典的key自动作为索引</strong>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">year_data = &#123;<span class="number">2001</span>: <span class="number">17.8</span>, <span class="number">2002</span>: <span class="number">20.1</span>, <span class="number">2003</span>: <span class="number">16.5</span>&#125;</span><br><span class="line">ser_obj2 = pd.Series(year_data)</span><br><span class="line">print(type(ser_obj2)) <span class="comment"># &lt;class 'pandas.core.series.Series'&gt;</span></span><br><span class="line">print(ser_obj2)</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.series.Series&apos;&gt;2001    17.82002    20.12003    16.5dtype: float64</code></pre><p>获取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(type(ser_obj2.values)) <span class="comment"># &lt;class 'numpy.ndarray'&gt;</span></span><br><span class="line">print(ser_obj2.values) <span class="comment"># [ 17.8  20.1  16.5]</span></span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;numpy.ndarray&apos;&gt;[ 17.8  20.1  16.5]</code></pre><p>获取索引</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(type(ser_obj2.index)) <span class="comment"># &lt;class 'pandas.core.indexes.numeric.Int64Index'&gt;</span></span><br><span class="line">print(ser_obj2.index) <span class="comment"># Int64Index([2001, 2002, 2003], dtype='int64')</span></span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.indexes.numeric.Int64Index&apos;&gt;Int64Index([2001, 2002, 2003], dtype=&apos;int64&apos;)</code></pre><p>预览数据（<strong>head()不加参数则显示全部</strong>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(ser_obj2.head())</span><br></pre></td></tr></table></figure><pre><code>2001    17.82002    20.12003    16.5dtype: float64</code></pre><p>通过索引获取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(ser_obj2[<span class="number">2001</span>]) <span class="comment"># 17.8</span></span><br></pre></td></tr></table></figure><pre><code>17.8</code></pre><p>整合代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过dict构建Series(注意：字典的key自动作为索引)</span></span><br><span class="line">year_data = &#123;<span class="number">2001</span>: <span class="number">17.8</span>, <span class="number">2002</span>: <span class="number">20.1</span>, <span class="number">2003</span>: <span class="number">16.5</span>&#125;</span><br><span class="line">ser_obj2 = pd.Series(year_data)</span><br><span class="line">print(type(ser_obj2)) <span class="comment"># &lt;class 'pandas.core.series.Series'&gt;</span></span><br><span class="line">print(ser_obj2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">print(type(ser_obj2.values)) <span class="comment"># &lt;class 'numpy.ndarray'&gt;</span></span><br><span class="line">print(ser_obj2.values) <span class="comment"># [ 17.8  20.1  16.5]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取索引</span></span><br><span class="line">print(type(ser_obj2.index)) <span class="comment"># &lt;class 'pandas.core.indexes.numeric.Int64Index'&gt;</span></span><br><span class="line">print(ser_obj2.index) <span class="comment"># Int64Index([2001, 2002, 2003], dtype='int64')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预览数据（head()不加参数则显示全部）</span></span><br><span class="line">print(ser_obj2.head())</span><br><span class="line"></span><br><span class="line"><span class="comment">#通过索引获取数据</span></span><br><span class="line">print(ser_obj2[<span class="number">2001</span>]) <span class="comment"># 17.8</span></span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.series.Series&apos;&gt;2001    17.82002    20.12003    16.5dtype: float64&lt;class &apos;numpy.ndarray&apos;&gt;[ 17.8  20.1  16.5]&lt;class &apos;pandas.core.indexes.numeric.Int64Index&apos;&gt;Int64Index([2001, 2002, 2003], dtype=&apos;int64&apos;)2001    17.82002    20.12003    16.5dtype: float6417.8</code></pre><h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>一个<code>Dataframe</code>就是一张表格，<code>Series</code>表示的是一维数组，<code>Dataframe</code>则是一个<strong>二维数组</strong>，可以类比成一张<code>excel</code>的<code>spreadsheet</code>。<strong>也可以把 <code>Dataframe</code>当做一组<code>Series</code>的集合</strong>。</p><p>通过ndarray构建DataFrame</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过ndarray构建DataFrame</span></span><br><span class="line">array = np.random.randn(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">print(array)</span><br><span class="line"></span><br><span class="line">df_obj = pd.DataFrame(array)</span><br><span class="line">print(df_obj.head())</span><br></pre></td></tr></table></figure><pre><code>[[ 0.7346628  -1.13733651  0.72853785  0.38743511] [ 0.49549724  3.96998008  1.13567695 -0.21425912] [ 0.22094222  0.7766603   0.46086182  0.33199643] [-0.46279419  0.85898771  0.41993259 -0.61997791] [-0.83296535  1.19450707 -1.45531366 -0.13990243]]          0         1         2         30  0.734663 -1.137337  0.728538  0.3874351  0.495497  3.969980  1.135677 -0.2142592  0.220942  0.776660  0.460862  0.3319963 -0.462794  0.858988  0.419933 -0.6199784 -0.832965  1.194507 -1.455314 -0.139902</code></pre><p>通过dict构建DataFrame</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dict_data = &#123;<span class="string">'A'</span>: <span class="number">1.</span>,</span><br><span class="line">             <span class="string">'B'</span>: pd.Timestamp(<span class="string">'20180316'</span>),</span><br><span class="line">             <span class="string">'C'</span>: pd.Series(<span class="number">1</span>, index=list(range(<span class="number">4</span>)),dtype=<span class="string">'float32'</span>),</span><br><span class="line">             <span class="string">'D'</span>: np.array([<span class="number">3</span>] * <span class="number">4</span>,dtype=<span class="string">'int32'</span>),</span><br><span class="line">             <span class="string">'E'</span> : pd.Categorical([<span class="string">"Python"</span>,<span class="string">"Java"</span>,<span class="string">"C++"</span>,<span class="string">"C#"</span>])</span><br><span class="line">            &#125;</span><br><span class="line">print(dict_data)</span><br><span class="line">df_obj2 = pd.DataFrame(dict_data)</span><br><span class="line">print(df_obj2.head())</span><br></pre></td></tr></table></figure><pre><code>{&apos;A&apos;: 1.0, &apos;B&apos;: Timestamp(&apos;2018-03-16 00:00:00&apos;), &apos;C&apos;: 0    1.01    1.02    1.03    1.0dtype: float32, &apos;D&apos;: array([3, 3, 3, 3]), &apos;E&apos;: [Python, Java, C++, C#]Categories (4, object): [C#, C++, Java, Python]}     A          B    C  D       E0  1.0 2018-03-16  1.0  3  Python1  1.0 2018-03-16  1.0  3    Java2  1.0 2018-03-16  1.0  3     C++3  1.0 2018-03-16  1.0  3      C#</code></pre><p>通过列索引获取列数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(df_obj2[<span class="string">'A'</span>])</span><br><span class="line">print(type(df_obj2[<span class="string">'A'</span>]))</span><br><span class="line"></span><br><span class="line">print(df_obj2.A)</span><br></pre></td></tr></table></figure><pre><code>0    1.01    1.02    1.03    1.0Name: A, dtype: float64&lt;class &apos;pandas.core.series.Series&apos;&gt;0    1.01    1.02    1.03    1.0Name: A, dtype: float64</code></pre><p>通过行索引(<strong>.loc</strong>)获取行数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(df_obj2.loc[<span class="number">0</span>])</span><br><span class="line">print(type(df_obj2.loc[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><pre><code>A                      1B    2018-03-16 00:00:00C                      1D                      3E                 PythonName: 0, dtype: object&lt;class &apos;pandas.core.series.Series&apos;&gt;</code></pre><p>增加列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_obj2[<span class="string">'F'</span>] = df_obj2[<span class="string">'D'</span>] + <span class="number">4</span></span><br><span class="line">print(df_obj2.head())</span><br></pre></td></tr></table></figure><pre><code>     A          B    C  D       E  F0  1.0 2018-03-16  1.0  3  Python  71  1.0 2018-03-16  1.0  3    Java  72  1.0 2018-03-16  1.0  3     C++  73  1.0 2018-03-16  1.0  3      C#  7</code></pre><p>删除列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span>(df_obj2[<span class="string">'F'</span>] )</span><br><span class="line">print(df_obj2.head())</span><br></pre></td></tr></table></figure><pre><code>     A          B    C  D       E0  1.0 2018-03-16  1.0  3  Python1  1.0 2018-03-16  1.0  3    Java2  1.0 2018-03-16  1.0  3     C++3  1.0 2018-03-16  1.0  3      C#</code></pre><p>整合代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过ndarray构建DataFrame</span></span><br><span class="line">array = np.random.randn(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">print(array)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过dict构建DataFrame</span></span><br><span class="line">df_obj = pd.DataFrame(array)</span><br><span class="line">print(df_obj.head())</span><br><span class="line"></span><br><span class="line">dict_data = &#123;<span class="string">'A'</span>: <span class="number">1.</span>,</span><br><span class="line">             <span class="string">'B'</span>: pd.Timestamp(<span class="string">'20180316'</span>),</span><br><span class="line">             <span class="string">'C'</span>: pd.Series(<span class="number">1</span>, index=list(range(<span class="number">4</span>)),dtype=<span class="string">'float32'</span>),</span><br><span class="line">             <span class="string">'D'</span>: np.array([<span class="number">3</span>] * <span class="number">4</span>,dtype=<span class="string">'int32'</span>),</span><br><span class="line">             <span class="string">'E'</span> : pd.Categorical([<span class="string">"Python"</span>,<span class="string">"Java"</span>,<span class="string">"C++"</span>,<span class="string">"C#"</span>])</span><br><span class="line">            &#125;</span><br><span class="line">print(dict_data)</span><br><span class="line">df_obj2 = pd.DataFrame(dict_data)</span><br><span class="line">print(df_obj2.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过列索引获取列数据</span></span><br><span class="line">print(df_obj2[<span class="string">'A'</span>])</span><br><span class="line">print(type(df_obj2[<span class="string">'A'</span>]))</span><br><span class="line"></span><br><span class="line">print(df_obj2.A)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过行索引获取行数据</span></span><br><span class="line">print(df_obj2.loc[<span class="number">0</span>])</span><br><span class="line">print(type(df_obj2.loc[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加列</span></span><br><span class="line">df_obj2[<span class="string">'G'</span>] = df_obj2[<span class="string">'D'</span>] + <span class="number">4</span></span><br><span class="line">print(df_obj2.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除列</span></span><br><span class="line"><span class="keyword">del</span>(df_obj2[<span class="string">'G'</span>] )</span><br><span class="line">print(df_obj2.head())</span><br></pre></td></tr></table></figure><pre><code>[[ 0.23758715 -1.13751056 -0.0863061  -0.71309414] [ 0.08129935  1.32099551 -0.27057527  0.49270974] [ 0.96111551  1.08307556  1.5094844   0.96117055] [-0.31003598  1.33959047 -0.42150857 -1.20605423] [ 0.12655879 -1.01810288 -1.34025171  0.98758417]]          0         1         2         30  0.237587 -1.137511 -0.086306 -0.7130941  0.081299  1.320996 -0.270575  0.4927102  0.961116  1.083076  1.509484  0.9611713 -0.310036  1.339590 -0.421509 -1.2060544  0.126559 -1.018103 -1.340252  0.987584{&apos;A&apos;: 1.0, &apos;B&apos;: Timestamp(&apos;2018-03-16 00:00:00&apos;), &apos;C&apos;: 0    1.01    1.02    1.03    1.0dtype: float32, &apos;D&apos;: array([3, 3, 3, 3]), &apos;E&apos;: [Python, Java, C++, C#]Categories (4, object): [C#, C++, Java, Python]}     A          B    C  D       E0  1.0 2018-03-16  1.0  3  Python1  1.0 2018-03-16  1.0  3    Java2  1.0 2018-03-16  1.0  3     C++3  1.0 2018-03-16  1.0  3      C#0    1.01    1.02    1.03    1.0Name: A, dtype: float64&lt;class &apos;pandas.core.series.Series&apos;&gt;0    1.01    1.02    1.03    1.0Name: A, dtype: float64A                      1B    2018-03-16 00:00:00C                      1D                      3E                 PythonName: 0, dtype: object&lt;class &apos;pandas.core.series.Series&apos;&gt;     A          B    C  D       E  G0  1.0 2018-03-16  1.0  3  Python  71  1.0 2018-03-16  1.0  3    Java  72  1.0 2018-03-16  1.0  3     C++  73  1.0 2018-03-16  1.0  3      C#  7     A          B    C  D       E0  1.0 2018-03-16  1.0  3  Python1  1.0 2018-03-16  1.0  3    Java2  1.0 2018-03-16  1.0  3     C++3  1.0 2018-03-16  1.0  3      C#</code></pre><h3 id="Pandas-数据操作"><a href="#Pandas-数据操作" class="headerlink" title="Pandas 数据操作"></a>Pandas 数据操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><h4 id="Series索引"><a href="#Series索引" class="headerlink" title="Series索引"></a>Series索引</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ser_obj = pd.Series(range(<span class="number">5</span>), index = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>])</span><br><span class="line">ser_obj.head()</span><br></pre></td></tr></table></figure><pre><code>a    0b    1c    2d    3e    4dtype: int32</code></pre><p>行索引</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 行索引</span></span><br><span class="line">ser_obj[<span class="string">'a'</span>] <span class="comment">#等同描述ser_obj[0]</span></span><br></pre></td></tr></table></figure><pre><code>0</code></pre><p>切片索引可以按照<strong>默认索引号</strong>，也可以按照<strong>实际索引值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切片索引（按索引号）</span></span><br><span class="line">ser_obj[<span class="number">1</span>:<span class="number">3</span>] <span class="comment">#python索引默认是左闭右开</span></span><br></pre></td></tr></table></figure><pre><code>b    1c    2dtype: int32</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 切片索引（按索引值）</span></span><br><span class="line">ser_obj[<span class="string">'b'</span>:<span class="string">'d'</span>]</span><br></pre></td></tr></table></figure><pre><code>b    1c    2d    3dtype: int32</code></pre><p>不连续索引，同样可以按照<strong>默认索引号</strong>，也可以按照<strong>实际索引值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不连续索引表达一（按索引号）</span></span><br><span class="line">ser_obj[[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>]]</span><br></pre></td></tr></table></figure><pre><code>a    0c    2e    4dtype: int32</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不连续索引表达二（按索引值）</span></span><br><span class="line">ser_obj[[<span class="string">'a'</span>, <span class="string">'e'</span>]]</span><br></pre></td></tr></table></figure><pre><code>a    0e    4dtype: int32</code></pre><p>布尔索引</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 布尔索引</span></span><br><span class="line">ser_bool = ser_obj &gt; <span class="number">2</span></span><br><span class="line">print(ser_bool)</span><br><span class="line">print()</span><br><span class="line">print(ser_obj[ser_bool])</span><br><span class="line">print()</span><br><span class="line">print(ser_obj[ser_obj &gt; <span class="number">2</span>])</span><br></pre></td></tr></table></figure><pre><code>a    Falseb    Falsec    Falsed     Truee     Truedtype: boold    3e    4dtype: int32d    3e    4dtype: int32</code></pre><h4 id="DataFrame索引"><a href="#DataFrame索引" class="headerlink" title="DataFrame索引"></a>DataFrame索引</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df_obj = pd.DataFrame(np.random.randn(<span class="number">5</span>,<span class="number">4</span>), columns = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>])</span><br><span class="line">df_obj.head()</span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>a</th><br>      <th>b</th><br>      <th>c</th><br>      <th>d</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>0.983790</td><br>      <td>1.063804</td><br>      <td>0.854634</td><br>      <td>-1.269025</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>0.161653</td><br>      <td>-0.904602</td><br>      <td>-1.840041</td><br>      <td>0.138183</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>-1.256608</td><br>      <td>-1.740634</td><br>      <td>-1.653686</td><br>      <td>-0.412524</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>0.165782</td><br>      <td>1.116089</td><br>      <td>0.065008</td><br>      <td>-1.693706</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>1.313987</td><br>      <td>0.734437</td><br>      <td>-0.625647</td><br>      <td>-1.738446</td><br>    </tr><br>  </tbody><br></table><br></div><p>列索引</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列索引</span></span><br><span class="line">print(type(df_obj[<span class="string">'a'</span>])) <span class="comment"># 返回Series类型</span></span><br><span class="line">df_obj[<span class="string">'a'</span>] <span class="comment"># 返回对应列值</span></span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.series.Series&apos;&gt;0    0.9837901    0.1616532   -1.2566083    0.1657824    1.313987Name: a, dtype: float64</code></pre><p>行索引</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 行索引</span></span><br><span class="line">print(type(df_obj.loc[<span class="number">0</span>])) <span class="comment"># 返回Series类型</span></span><br><span class="line">df_obj.loc[<span class="number">0</span>] <span class="comment"># 返回对应行值</span></span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.series.Series&apos;&gt;a    0.983790b    1.063804c    0.854634d   -1.269025Name: 0, dtype: float64</code></pre><p>不连续索引</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不连续列索引</span></span><br><span class="line">df_obj[[<span class="string">'a'</span>,<span class="string">'c'</span>]]  <span class="comment">#不连续列索引</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>a</th><br>      <th>c</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>0.983790</td><br>      <td>0.854634</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>0.161653</td><br>      <td>-1.840041</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>-1.256608</td><br>      <td>-1.653686</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>0.165782</td><br>      <td>0.065008</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>1.313987</td><br>      <td>-0.625647</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不连续行索引</span></span><br><span class="line">df_obj.loc[[<span class="number">1</span>, <span class="number">3</span>]] <span class="comment">#不连续行索引</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>a</th><br>      <th>b</th><br>      <th>c</th><br>      <th>d</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>1</th><br>      <td>0.161653</td><br>      <td>-0.904602</td><br>      <td>-1.840041</td><br>      <td>0.138183</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>0.165782</td><br>      <td>1.116089</td><br>      <td>0.065008</td><br>      <td>-1.693706</td><br>    </tr><br>  </tbody><br></table><br></div><p>混合索引</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 混合索引 loc</span></span><br><span class="line">print(df_obj.loc[<span class="number">0</span>:<span class="number">2</span>, <span class="string">'a'</span>]) <span class="comment"># 连续行加列索引(这里是从0-2)</span></span><br><span class="line">print()</span><br><span class="line">print(df_obj.loc[[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>], <span class="string">'a'</span>]) <span class="comment"># 不连续行加列索引</span></span><br></pre></td></tr></table></figure><pre><code>0   -1.0189411    0.0892752   -2.210780Name: a, dtype: float640   -1.0189412   -2.2107804    1.435787Name: a, dtype: float64</code></pre><h4 id="运算与对齐"><a href="#运算与对齐" class="headerlink" title="运算与对齐"></a>运算与对齐</h4><h5 id="Series-对齐操作"><a href="#Series-对齐操作" class="headerlink" title="Series 对齐操作"></a><code>Series</code> 对齐操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">s1 = pd.Series(range(<span class="number">10</span>, <span class="number">13</span>), index = range(<span class="number">3</span>))</span><br><span class="line">s2 = pd.Series(range(<span class="number">20</span>, <span class="number">25</span>), index = range(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'s1: '</span> )</span><br><span class="line">print(s1)</span><br><span class="line"></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'s2: '</span>)</span><br><span class="line">print(s2)</span><br></pre></td></tr></table></figure><pre><code>s1:0    101    112    12dtype: int32s2:0    201    212    223    234    24dtype: int32</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Series 对齐运算</span></span><br><span class="line">print(s1 + s2) <span class="comment"># 没有对应上的部分会显示NaN</span></span><br><span class="line">print()</span><br><span class="line">print(s1.add(s2, fill_value = <span class="number">-1</span>)) <span class="comment"># 没有对应上的部分会填充-1，然后运算</span></span><br><span class="line">print()</span><br><span class="line">s3 = s1 + s2</span><br><span class="line">s3_filled = s3.fillna(<span class="number">-1</span>)</span><br><span class="line">print(s3_filled) <span class="comment">## 先运算，然后NaN填充为-1</span></span><br></pre></td></tr></table></figure><pre><code>0    30.01    32.02    34.03     NaN4     NaNdtype: float640    30.01    32.02    34.03    22.04    23.0dtype: float640    30.01    32.02    34.03    -1.04    -1.0dtype: float64</code></pre><h5 id="DataFrame-对齐操作"><a href="#DataFrame-对齐操作" class="headerlink" title="DataFrame 对齐操作"></a><code>DataFrame</code> 对齐操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df1 = pd.DataFrame(np.ones((<span class="number">2</span>,<span class="number">2</span>)), columns = [<span class="string">'a'</span>, <span class="string">'b'</span>])</span><br><span class="line">df2 = pd.DataFrame(np.ones((<span class="number">3</span>,<span class="number">3</span>)), columns = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'df1: '</span>)</span><br><span class="line">print(df1)</span><br><span class="line"></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line">print(<span class="string">'df2: '</span>)</span><br><span class="line">print(df2)</span><br></pre></td></tr></table></figure><pre><code>df1:     a    b0  1.0  1.01  1.0  1.0df2:     a    b    c0  1.0  1.0  1.01  1.0  1.0  1.02  1.0  1.0  1.0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame对齐操作</span></span><br><span class="line">df1 + df2 <span class="comment"># 没有对应上的部分会显示NaN</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>a</th><br>      <th>b</th><br>      <th>c</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>2.0</td><br>      <td>2.0</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2.0</td><br>      <td>2.0</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1.add(df2, fill_value = <span class="number">0</span>) <span class="comment"># 加法操作，没有对应上的补零</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>a</th><br>      <th>b</th><br>      <th>c</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>2.0</td><br>      <td>2.0</td><br>      <td>1.0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2.0</td><br>      <td>2.0</td><br>      <td>1.0</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>1.0</td><br>      <td>1.0</td><br>      <td>1.0</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1 - df2 <span class="comment"># 没有对应上的部分会显示NaN</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>a</th><br>      <th>b</th><br>      <th>c</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>0.0</td><br>      <td>0.0</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>0.0</td><br>      <td>0.0</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df1.sub(df2, fill_value = <span class="number">2</span>) <span class="comment"># 加法操作，没有对应上的补2(先补充后运算)</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>a</th><br>      <th>b</th><br>      <th>c</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>0.0</td><br>      <td>0.0</td><br>      <td>1.0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>0.0</td><br>      <td>0.0</td><br>      <td>1.0</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>1.0</td><br>      <td>1.0</td><br>      <td>1.0</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df3 = df1 + df2</span><br><span class="line">df3.fillna(<span class="number">100</span>, inplace = <span class="keyword">True</span>) <span class="comment"># 先运行加法操作，没有对应上的补2(先运算，后补充)</span></span><br><span class="line">df3</span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>a</th><br>      <th>b</th><br>      <th>c</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>2.0</td><br>      <td>2.0</td><br>      <td>100.0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>2.0</td><br>      <td>2.0</td><br>      <td>100.0</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>100.0</td><br>      <td>100.0</td><br>      <td>100.0</td><br>    </tr><br>  </tbody><br></table><br></div><h4 id="函数应用"><a href="#函数应用" class="headerlink" title="函数应用"></a>函数应用</h4><p>可以与<code>NumPy</code>中的<code>ufunc</code>函数结合操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Numpy ufunc 函数</span></span><br><span class="line">df = pd.DataFrame(np.random.randn(<span class="number">5</span>,<span class="number">4</span>) - <span class="number">1</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>0</th><br>      <th>1</th><br>      <th>2</th><br>      <th>3</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>-0.938212</td><br>      <td>-2.487779</td><br>      <td>-1.805374</td><br>      <td>-1.130723</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>-0.533441</td><br>      <td>0.196536</td><br>      <td>-1.094895</td><br>      <td>-1.819312</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>-3.233318</td><br>      <td>0.255510</td><br>      <td>-1.560183</td><br>      <td>-2.404621</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>-1.956924</td><br>      <td>-2.947539</td><br>      <td>-1.640760</td><br>      <td>-0.757321</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>0.198618</td><br>      <td>0.344484</td><br>      <td>-0.893815</td><br>      <td>-0.498036</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.abs(df) <span class="comment">#取绝对值（还有其他诸多NumPy中的函数可以操作）</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>0</th><br>      <th>1</th><br>      <th>2</th><br>      <th>3</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>0.938212</td><br>      <td>2.487779</td><br>      <td>1.805374</td><br>      <td>1.130723</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>0.533441</td><br>      <td>0.196536</td><br>      <td>1.094895</td><br>      <td>1.819312</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>3.233318</td><br>      <td>0.255510</td><br>      <td>1.560183</td><br>      <td>2.404621</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>1.956924</td><br>      <td>2.947539</td><br>      <td>1.640760</td><br>      <td>0.757321</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>0.198618</td><br>      <td>0.344484</td><br>      <td>0.893815</td><br>      <td>0.498036</td><br>    </tr><br>  </tbody><br></table><br></div><p>使用apply应用<strong>行或列数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用apply应用行或列数据</span></span><br><span class="line"><span class="comment"># f = lambda x : x.max() # lambda存在意义就是对简单函数的简洁表示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.max()</span><br><span class="line"></span><br><span class="line">df.apply(f) <span class="comment"># 默认按行比较（得到每列的最大值）</span></span><br></pre></td></tr></table></figure><pre><code>0    0.1986181    0.3444842   -0.8938153   -0.498036dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.apply(<span class="keyword">lambda</span> x : x.max(), axis=<span class="number">1</span>) <span class="comment"># 按列比较（得到每行的最大值）</span></span><br></pre></td></tr></table></figure><pre><code>0   -0.9382121    0.1965362    0.2555103   -0.7573214    0.344484dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.apply(<span class="keyword">lambda</span> x : x.max(), axis=<span class="number">0</span>) <span class="comment"># # 按行比较（得到每列的最大值）</span></span><br></pre></td></tr></table></figure><pre><code>0    0.1986181    0.3444842   -0.8938153   -0.498036dtype: float64</code></pre><p>使用applymap应用到<strong>每个数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用applymap应用到每个数据</span></span><br><span class="line">f2 = <span class="keyword">lambda</span> x : <span class="string">'%.2f'</span> % x <span class="comment">#每个数据显示只保留两位小数</span></span><br><span class="line">df.applymap(f2)</span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>0</th><br>      <th>1</th><br>      <th>2</th><br>      <th>3</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>-0.94</td><br>      <td>-2.49</td><br>      <td>-1.81</td><br>      <td>-1.13</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>-0.53</td><br>      <td>0.20</td><br>      <td>-1.09</td><br>      <td>-1.82</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>-3.23</td><br>      <td>0.26</td><br>      <td>-1.56</td><br>      <td>-2.40</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>-1.96</td><br>      <td>-2.95</td><br>      <td>-1.64</td><br>      <td>-0.76</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>0.20</td><br>      <td>0.34</td><br>      <td>-0.89</td><br>      <td>-0.50</td><br>    </tr><br>  </tbody><br></table><br></div><h4 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h4><p><code>Series</code>索引排序 &amp; 值排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#索引乱序生成</span></span><br><span class="line">s4 = pd.Series([<span class="number">10</span>,<span class="number">13</span>,<span class="number">12</span>,<span class="number">25</span>,<span class="number">14</span>], index = [<span class="number">2</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">s4</span><br></pre></td></tr></table></figure><pre><code>2    101    135    123    254    14dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 索引排序</span></span><br><span class="line">s4.sort_index(ascending=<span class="keyword">False</span>) <span class="comment">#  索引倒序排列</span></span><br></pre></td></tr></table></figure><pre><code>5    124    143    252    101    13dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 值排序</span></span><br><span class="line">s4.sort_values()</span><br></pre></td></tr></table></figure><pre><code>2    105    121    134    143    25dtype: int64</code></pre><p><code>DataFrame</code> 索引排序 &amp; 值排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df4 = pd.DataFrame(np.random.randn(<span class="number">3</span>, <span class="number">4</span>),</span><br><span class="line">                   index=[<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>],</span><br><span class="line">                   columns=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">df4</span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>1</th><br>      <th>4</th><br>      <th>2</th><br>      <th>3</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>1</th><br>      <td>0.948112</td><br>      <td>0.076323</td><br>      <td>0.089607</td><br>      <td>0.091737</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>-1.254556</td><br>      <td>1.483504</td><br>      <td>0.468995</td><br>      <td>0.286249</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>-0.806738</td><br>      <td>-0.842388</td><br>      <td>-1.127489</td><br>      <td>-0.020803</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按索引排序</span></span><br><span class="line">df4.sort_index(ascending=<span class="keyword">False</span>)<span class="comment"># 对横轴按倒序排列</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>1</th><br>      <th>4</th><br>      <th>2</th><br>      <th>3</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>3</th><br>      <td>-1.254556</td><br>      <td>1.483504</td><br>      <td>0.468995</td><br>      <td>0.286249</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>-0.806738</td><br>      <td>-0.842388</td><br>      <td>-1.127489</td><br>      <td>-0.020803</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>0.948112</td><br>      <td>0.076323</td><br>      <td>0.089607</td><br>      <td>0.091737</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按索引排序</span></span><br><span class="line">df4.sort_index(axis=<span class="number">1</span>) <span class="comment">#列轴按序排列</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>1</th><br>      <th>2</th><br>      <th>3</th><br>      <th>4</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>1</th><br>      <td>0.948112</td><br>      <td>0.089607</td><br>      <td>0.091737</td><br>      <td>0.076323</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>-1.254556</td><br>      <td>0.468995</td><br>      <td>0.286249</td><br>      <td>1.483504</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>-0.806738</td><br>      <td>-1.127489</td><br>      <td>-0.020803</td><br>      <td>-0.842388</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按列排序</span></span><br><span class="line">df4.sort_values(by=<span class="number">1</span>) <span class="comment"># by参数的作用是针对某一（些）列进行排序（不能对行使用 by 参数）</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>1</th><br>      <th>4</th><br>      <th>2</th><br>      <th>3</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>3</th><br>      <td>-1.254556</td><br>      <td>1.483504</td><br>      <td>0.468995</td><br>      <td>0.286249</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>-0.806738</td><br>      <td>-0.842388</td><br>      <td>-1.127489</td><br>      <td>-0.020803</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>0.948112</td><br>      <td>0.076323</td><br>      <td>0.089607</td><br>      <td>0.091737</td><br>    </tr><br>  </tbody><br></table><br></div><h4 id="处理缺失数据"><a href="#处理缺失数据" class="headerlink" title="处理缺失数据"></a>处理缺失数据</h4><p>生成数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_data = pd.DataFrame([np.random.randn(<span class="number">3</span>), [<span class="number">1.</span>, np.nan, np.nan],</span><br><span class="line">                       [<span class="number">4.</span>, np.nan, np.nan], [<span class="number">1.</span>, np.nan, <span class="number">2.</span>]])</span><br><span class="line">df_data.head()</span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>0</th><br>      <th>1</th><br>      <th>2</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1.089477</td><br>      <td>-0.486706</td><br>      <td>-0.322284</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>1.000000</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>4.000000</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>1.000000</td><br>      <td>NaN</td><br>      <td>2.000000</td><br>    </tr><br>  </tbody><br></table><br></div><p><strong>二值化</strong>（NaN为False，非NaN为True）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># isnull</span></span><br><span class="line">df_data.isnull()</span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>0</th><br>      <th>1</th><br>      <th>2</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>False</td><br>      <td>False</td><br>      <td>False</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>False</td><br>      <td>True</td><br>      <td>True</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>False</td><br>      <td>True</td><br>      <td>True</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>False</td><br>      <td>True</td><br>      <td>False</td><br>    </tr><br>  </tbody><br></table><br></div><p><strong>丢掉</strong>有NaN的行或列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dropna</span></span><br><span class="line">print(df_data.dropna()) <span class="comment">#默认丢掉有NaN的行</span></span><br><span class="line">print()</span><br><span class="line">print(df_data.dropna(axis=<span class="number">1</span>)) <span class="comment">#丢掉有NaN的列</span></span><br></pre></td></tr></table></figure><pre><code>          0         1         20  1.089477 -0.486706 -0.322284          00  1.0894771  1.0000002  4.0000003  1.000000</code></pre><p><strong>填充</strong>NaN值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fillna</span></span><br><span class="line">df_data.fillna(<span class="number">-100.</span>) <span class="comment"># NaN值填充为-100</span></span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>0</th><br>      <th>1</th><br>      <th>2</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1.089477</td><br>      <td>-0.486706</td><br>      <td>-0.322284</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>1.000000</td><br>      <td>-100.000000</td><br>      <td>-100.000000</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>4.000000</td><br>      <td>-100.000000</td><br>      <td>-100.000000</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>1.000000</td><br>      <td>-100.000000</td><br>      <td>2.000000</td><br>    </tr><br>  </tbody><br></table><br></div><h3 id="数据统计计算和描述"><a href="#数据统计计算和描述" class="headerlink" title="数据统计计算和描述"></a>数据统计计算和描述</h3><h4 id="常用的统计计算"><a href="#常用的统计计算" class="headerlink" title="常用的统计计算"></a>常用的统计计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_obj = pd.DataFrame(np.random.randn(<span class="number">5</span>,<span class="number">4</span>), columns = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>])</span><br><span class="line">df_obj</span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>a</th><br>      <th>b</th><br>      <th>c</th><br>      <th>d</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>0.145119</td><br>      <td>-2.398595</td><br>      <td>0.640806</td><br>      <td>0.696701</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>-0.877139</td><br>      <td>-0.261616</td><br>      <td>-2.211734</td><br>      <td>0.140729</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>-0.644545</td><br>      <td>0.523667</td><br>      <td>-1.460002</td><br>      <td>-0.341459</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>1.369260</td><br>      <td>1.039981</td><br>      <td>0.164075</td><br>      <td>0.380755</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>0.089507</td><br>      <td>-0.371051</td><br>      <td>1.348191</td><br>      <td>-0.828315</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_obj.sum()</span><br></pre></td></tr></table></figure><pre><code>a    0.082203b   -1.467614c   -1.518663d    0.048410dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_obj.max()</span><br></pre></td></tr></table></figure><pre><code>a    1.369260b    1.039981c    1.348191d    0.696701dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_obj.min(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>0   -2.3985951   -2.2117342   -1.4600023    0.1640754   -0.828315dtype: float64</code></pre><h4 id="统计描述"><a href="#统计描述" class="headerlink" title="统计描述"></a>统计描述</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_obj.describe()</span><br></pre></td></tr></table></figure><div><br><style><br>    .dataframe thead tr:only-child th {<br>        text-align: right;<br>    }<br><br>    .dataframe thead th {<br>        text-align: left;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>a</th><br>      <th>b</th><br>      <th>c</th><br>      <th>d</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>count</th><br>      <td>5.000000</td><br>      <td>5.000000</td><br>      <td>5.000000</td><br>      <td>5.000000</td><br>    </tr><br>    <tr><br>      <th>mean</th><br>      <td>0.016441</td><br>      <td>-0.293523</td><br>      <td>-0.303733</td><br>      <td>0.009682</td><br>    </tr><br>    <tr><br>      <th>std</th><br>      <td>0.878550</td><br>      <td>1.311906</td><br>      <td>1.484695</td><br>      <td>0.602578</td><br>    </tr><br>    <tr><br>      <th>min</th><br>      <td>-0.877139</td><br>      <td>-2.398595</td><br>      <td>-2.211734</td><br>      <td>-0.828315</td><br>    </tr><br>    <tr><br>      <th>25%</th><br>      <td>-0.644545</td><br>      <td>-0.371051</td><br>      <td>-1.460002</td><br>      <td>-0.341459</td><br>    </tr><br>    <tr><br>      <th>50%</th><br>      <td>0.089507</td><br>      <td>-0.261616</td><br>      <td>0.164075</td><br>      <td>0.140729</td><br>    </tr><br>    <tr><br>      <th>75%</th><br>      <td>0.145119</td><br>      <td>0.523667</td><br>      <td>0.640806</td><br>      <td>0.380755</td><br>    </tr><br>    <tr><br>      <th>max</th><br>      <td>1.369260</td><br>      <td>1.039981</td><br>      <td>1.348191</td><br>      <td>0.696701</td><br>    </tr><br>  </tbody><br></table><br></div><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><img src="http://xukeqiniu.xukeai.cn/0efefdc4079c7e5ec5ce6e3dcf63f94a.png" alt=""></p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Pandas </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Matplotlib学习笔记</title>
      <link href="/2018/01/04/Python/02-matplotlib/"/>
      <url>/2018/01/04/Python/02-matplotlib/</url>
      <content type="html"><![CDATA[<p><img src="http://xukeqiniu.xukeai.cn/a1be79a54d907e40f78eafe507542997.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/5257a902945a2ae575b2597a7e778194.png" alt=""></p><h2 id="matplotlib学习笔记"><a href="#matplotlib学习笔记" class="headerlink" title="matplotlib学习笔记"></a>matplotlib学习笔记</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#notebook模式下</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="matplotlib三种代码风格"><a href="#matplotlib三种代码风格" class="headerlink" title="matplotlib三种代码风格"></a>matplotlib三种代码风格</h3><h4 id="pyplot"><a href="#pyplot" class="headerlink" title="pyplot"></a>pyplot</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">y=np.random.randn(len(x))</span><br><span class="line">plt.plot(x,y) <span class="comment">#绘制以x为横坐标，y为纵坐标的折线图</span></span><br><span class="line">plt.title(<span class="string">'pyplot'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_4_0.png" alt=""></p><h4 id="pylab"><a href="#pylab" class="headerlink" title="pylab"></a>pylab</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pylab不推荐使用</span></span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> *</span><br><span class="line">x=arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">y=randn(len(x))</span><br><span class="line">plot(x,y) <span class="comment">#绘制以x为横坐标，y为纵坐标的折线图</span></span><br><span class="line">title(<span class="string">'pylab'</span>)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_6_0.png" alt=""></p><h4 id="Object-Oriented"><a href="#Object-Oriented" class="headerlink" title="Object Oriented"></a>Object Oriented</h4><p>在matplotlib中，整个图像为一个Figure对象。在Figure对象中可以包含一个，或者多个Axes对象。每个Axes对象都是一个拥有自己坐标系统的绘图区域。其逻辑关系如下：<img src="http://xukeqiniu.xukeai.cn/4dc52933144ff9bbf0c5a26e5991f130.png" alt=""><br>整个图像是fig对象。我们的绘图中只有一个坐标系区域，也就是ax。此外还有以下对象。</p><ul><li>Data: 数据区，包括数据点、描绘形状</li><li>Axis: 坐标轴，包括 X 轴、 Y 轴及其标签、刻度尺及其标签</li><li>Title: 标题，数据图的描述</li><li>Legend: 图例，区分图中包含的多种曲线或不同分类的数据</li><li>其他的还有图形文本 (Text)、注解 (Annotate)等其他描述<br><img src="http://xukeqiniu.xukeai.cn/71941bcaa6b9ea62c2895c5393cc06e0.png" alt=""><br>Title为标题。Axis为坐标轴，Label为坐标轴标注。Tick为刻度线，Tick Label为刻度注释。各个对象之间有下面的对象隶属关系：<img src="https://images0.cnblogs.com/blog/413416/201301/29220639-06b363b5f4e14b9585be1be494712f8c.png" alt=""></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 推荐使用</span></span><br><span class="line">x=np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">y=np.random.randn(len(x))</span><br><span class="line">fig=plt.figure() <span class="comment">#定义图像的对象</span></span><br><span class="line">ax=fig.add_subplot(<span class="number">111</span>) <span class="comment">#定义坐标系区域</span></span><br><span class="line">ax.plot(x,y) <span class="comment">#绘制以x为横坐标，y为纵坐标的折线图</span></span><br><span class="line">ax.set_title(<span class="string">'object oriented'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_8_0.png" alt=""></p><h3 id="子图"><a href="#子图" class="headerlink" title="子图"></a>子图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">221</span>) <span class="comment"># 定义2*2个子图（左一）</span></span><br><span class="line">ax1.plot(x,x) <span class="comment"># 绘制左一折线图</span></span><br><span class="line"></span><br><span class="line">ax2 = fig.add_subplot(<span class="number">222</span>)</span><br><span class="line">ax2.plot(x,-x) <span class="comment"># 绘制右一折线图（右一）</span></span><br><span class="line"></span><br><span class="line">ax3 = fig.add_subplot(<span class="number">223</span>)</span><br><span class="line">ax3.plot(x,x*x) <span class="comment"># 绘制左二折线图（左二）</span></span><br><span class="line"></span><br><span class="line">ax4 = fig.add_subplot(<span class="number">224</span>)</span><br><span class="line">ax4.plot(x,np.log(x)) <span class="comment"># 绘制右二折线图（右二）</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_10_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line">plt.subplot(<span class="number">221</span>) <span class="comment"># 第一行的左图</span></span><br><span class="line">plt.plot(x,x)</span><br><span class="line">plt.subplot(<span class="number">222</span>) <span class="comment"># 第一行的右图</span></span><br><span class="line">plt.plot(x,-x)</span><br><span class="line">plt.subplot(<span class="number">212</span>) <span class="comment"># 第二整行</span></span><br><span class="line">plt.plot(x,x*x)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_11_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#简化写法</span></span><br><span class="line">fig,axes = plt.subplots(ncols=<span class="number">2</span>,nrows=<span class="number">2</span>) <span class="comment">#定义子图为两行两列</span></span><br><span class="line">ax1,ax2,ax3,ax4 = axes.ravel() <span class="comment">#按照先行再列的顺序分配子图</span></span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line">ax1.plot(x,x)</span><br><span class="line">ax2.plot(x,-x)</span><br><span class="line">ax3.plot(x,x*x)</span><br><span class="line">ax4.plot(x,np.log(x))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_12_0.png" alt=""></p><h3 id="多图"><a href="#多图" class="headerlink" title="多图"></a>多图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig1 = plt.figure() <span class="comment"># plt派生一个图对象</span></span><br><span class="line">ax1 = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax1.plot([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">fig2 = plt.figure() <span class="comment"># plt派生另一个图对象</span></span><br><span class="line">ax2 = fig2.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax2.plot([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">plt.show() <span class="comment"># plt统一显示</span></span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_14_0.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/output_14_1.png" alt=""></p><h3 id="散点图（scatter）"><a href="#散点图（scatter）" class="headerlink" title="散点图（scatter）"></a>散点图（scatter）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">fig,axes = plt.subplots(ncols=<span class="number">2</span>,nrows=<span class="number">2</span>)</span><br><span class="line">ax1,ax2,ax3,ax4 = axes.ravel()</span><br><span class="line"><span class="comment"># example1</span></span><br><span class="line">height=[<span class="number">161</span>,<span class="number">170</span>,<span class="number">182</span>,<span class="number">175</span>,<span class="number">173</span>,<span class="number">165</span>]</span><br><span class="line">weight=[<span class="number">50</span>,<span class="number">58</span>,<span class="number">80</span>,<span class="number">70</span>,<span class="number">69</span>,<span class="number">55</span>]</span><br><span class="line">ax1.scatter(height,weight) <span class="comment">#绘制横坐标为身高，纵坐标为体重散点图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># example2</span></span><br><span class="line">N = <span class="number">1000</span></span><br><span class="line">x = np.random.randn(N) <span class="comment">#随机生成一千个点</span></span><br><span class="line">y = np.random.randn(N) <span class="comment">#随机生成一千个点</span></span><br><span class="line">ax2.scatter(x,y) <span class="comment">#绘制横坐标为x，纵坐标为y散点图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># example3</span></span><br><span class="line"></span><br><span class="line">open,close=np.loadtxt(<span class="string">'000001.csv'</span>,delimiter=<span class="string">','</span>,skiprows=<span class="number">1</span>,usecols=(<span class="number">1</span>,<span class="number">4</span>),unpack=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># loadtxt(fname, dtype=&lt;class 'float'&gt;, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0)</span></span><br><span class="line"><span class="comment"># fname：读取文件的文件名。例如 '000001.csv'。</span></span><br><span class="line"><span class="comment"># dtype：数据类型。如float，str等。默认为float</span></span><br><span class="line"><span class="comment"># comments 注释</span></span><br><span class="line"><span class="comment"># delimiter：数据之间的分隔符。如使用逗号','。默认是空格</span></span><br><span class="line"><span class="comment"># skiprows跳过前几行读取，默认是0，必须是int整型。</span></span><br><span class="line"><span class="comment"># usecols：选取数据的列。</span></span><br><span class="line"><span class="comment"># unpack如果为True，将分列读取。</span></span><br><span class="line"></span><br><span class="line">change=close-open</span><br><span class="line">yesterday=change[:<span class="number">-1</span>]</span><br><span class="line">today=change[<span class="number">1</span>:]</span><br><span class="line">ax3.scatter(today,yesterday)</span><br><span class="line"></span><br><span class="line"><span class="comment"># example4</span></span><br><span class="line">ax4.scatter(today,yesterday,s=<span class="number">50</span>,c=<span class="string">'r'</span>,marker=<span class="string">'&lt;'</span>,alpha=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># s：尺寸大小</span></span><br><span class="line"><span class="comment"># c: 颜色类型</span></span><br><span class="line"><span class="comment"># marker: 标记形状</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_16_0.png" alt=""></p><h3 id="条形图-bar"><a href="#条形图-bar" class="headerlink" title="条形图 (bar)"></a>条形图 (bar)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">fig,axes = plt.subplots(ncols=<span class="number">2</span>,nrows=<span class="number">2</span>)</span><br><span class="line">ax1,ax2,ax3,ax4 = axes.ravel()</span><br><span class="line"></span><br><span class="line">N=<span class="number">5</span></span><br><span class="line"></span><br><span class="line">y=[<span class="number">20</span>,<span class="number">10</span>,<span class="number">30</span>,<span class="number">25</span>,<span class="number">15</span>]</span><br><span class="line"></span><br><span class="line">index = np.arange(N)</span><br><span class="line"><span class="comment"># bar绘制条形图</span></span><br><span class="line">ax1.bar(left=index, height=y,width=<span class="number">0.3</span>) <span class="comment">#left：横坐标值 height:纵坐标值 width：条形图宽度</span></span><br><span class="line">ax2.bar(left=index, height=y,color=<span class="string">'red'</span>,width=<span class="number">0.3</span>) <span class="comment"># color：设置条形图颜色</span></span><br><span class="line">ax3.bar(left=<span class="number">0</span>, bottom=index, width=y,height=<span class="number">0.5</span>,orientation=<span class="string">'horizontal'</span>)<span class="comment"># orientation：'horizontal'设置为横向</span></span><br><span class="line">ax4.barh(bottom=index,width=y,height=<span class="number">0.5</span>) <span class="comment"># barh 横向条形图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_18_0.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">fig,axes = plt.subplots(ncols=<span class="number">2</span>,nrows=<span class="number">2</span>)</span><br><span class="line">ax1,ax2,ax3,ax4 = axes.ravel()</span><br><span class="line"></span><br><span class="line">index=np.arange(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">sales_BJ=[<span class="number">52</span>,<span class="number">55</span>,<span class="number">63</span>,<span class="number">53</span>]</span><br><span class="line">sales_SH=[<span class="number">44</span>,<span class="number">66</span>,<span class="number">55</span>,<span class="number">41</span>]</span><br><span class="line">bar_width=<span class="number">0.3</span></span><br><span class="line"></span><br><span class="line">ax1.bar(index,sales_BJ,bar_width,color=<span class="string">'b'</span>)</span><br><span class="line">ax1.bar(index+bar_width,sales_SH,bar_width,color=<span class="string">'r'</span>) <span class="comment"># index+bar_width实现横向并排</span></span><br><span class="line"></span><br><span class="line">ax2.bar(index,sales_BJ,bar_width,color=<span class="string">'b'</span>)</span><br><span class="line">ax2.bar(index,sales_SH,bar_width,color=<span class="string">'r'</span>,bottom=sales_BJ) <span class="comment"># bottom=sales_BJ实现纵向叠加</span></span><br><span class="line"></span><br><span class="line">ax3.barh(bottom=index,width=sales_BJ,height=<span class="number">0.3</span>,color=<span class="string">'b'</span>)</span><br><span class="line">ax3.barh(bottom=index+bar_width,width=sales_SH,height=<span class="number">0.3</span>,color=<span class="string">'r'</span>) <span class="comment"># bottom=index+bar_width</span></span><br><span class="line"></span><br><span class="line">ax4.barh(bottom=index,width=sales_BJ,height=<span class="number">0.3</span>,color=<span class="string">'b'</span>)</span><br><span class="line">ax4.barh(bottom=index,width=sales_SH,height=<span class="number">0.3</span>,color=<span class="string">'r'</span>,left=sales_BJ) <span class="comment"># left=sales_BJ</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_19_0.png" alt=""></p><h3 id="直方图（hist）"><a href="#直方图（hist）" class="headerlink" title="直方图（hist）"></a>直方图（hist）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">221</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">222</span>)</span><br><span class="line">ax3 = fig.add_subplot(<span class="number">212</span>)</span><br><span class="line"></span><br><span class="line">mu = <span class="number">100</span></span><br><span class="line">sigma = <span class="number">20</span></span><br><span class="line">x = mu +sigma * np.random.randn(<span class="number">2000</span>)</span><br><span class="line"></span><br><span class="line">ax1.hist(x,bins=<span class="number">10</span>,color=<span class="string">'green'</span>,normed=<span class="keyword">True</span>) <span class="comment">#输入数据，bins=总共有几条条状图，color=颜色，normed=True:纵坐标总共为1</span></span><br><span class="line"></span><br><span class="line">ax2.hist(x,bins=<span class="number">50</span>,color=<span class="string">'red'</span>,normed=<span class="keyword">False</span>) <span class="comment">#normed=False:纵坐标显示实际值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.random.randn(<span class="number">1000</span>)+<span class="number">2</span></span><br><span class="line">y = np.random.randn(<span class="number">1000</span>)+<span class="number">3</span></span><br><span class="line"></span><br><span class="line">ax3.hist2d(x,y,bins=<span class="number">10</span>) <span class="comment">#二维直方图</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_21_0.png" alt=""></p><h3 id="饼状图（pie）"><a href="#饼状图（pie）" class="headerlink" title="饼状图（pie）"></a>饼状图（pie）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">labels=<span class="string">'frogs'</span>,<span class="string">'hogs'</span>,<span class="string">'dogs'</span>,<span class="string">'logs'</span></span><br><span class="line">sizes=<span class="number">15</span>,<span class="number">20</span>,<span class="number">45</span>,<span class="number">10</span></span><br><span class="line">colors=<span class="string">'yellowgreen'</span>,<span class="string">'gold'</span>,<span class="string">'lightskyblue'</span>,<span class="string">'lightcoral'</span></span><br><span class="line">explode=<span class="number">0</span>,<span class="number">0.1</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">plt.pie(sizes,explode=explode,labels=labels,colors=colors,labeldistance = <span class="number">1.1</span> ,autopct=<span class="string">'%3.1f%%'</span>,shadow=<span class="keyword">True</span>,startangle=<span class="number">90</span>,pctdistance = <span class="number">0.6</span>)</span><br><span class="line"><span class="comment">#labeldistance，文本的位置离远点有多远，1.1指1.1倍半径的位置</span></span><br><span class="line"><span class="comment">#autopct，圆里面的文本格式，%3.1f%%表示小数有三位，整数有一位的浮点数</span></span><br><span class="line"><span class="comment">#shadow，饼是否有阴影</span></span><br><span class="line"><span class="comment">#startangle，起始角度，0，表示从0开始逆时针转，为第一块。一般选择从90度开始比较好看</span></span><br><span class="line"><span class="comment">#pctdistance，百分比的text离圆心的距离</span></span><br><span class="line">plt.axis(<span class="string">'equal'</span>) <span class="comment">#修正为正圆 设置x，y轴刻度一致，这样饼图才能是圆的</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/8c34e7292cda91433806af454a797ce4.png" alt=""></p><h3 id="箱型图（boxplot）"><a href="#箱型图（boxplot）" class="headerlink" title="箱型图（boxplot）"></a>箱型图（boxplot）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">211</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">212</span>)</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">data = np.random.normal(size=<span class="number">1000</span>, loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">ax1.boxplot(data,sym=<span class="string">'o'</span>,whis=<span class="number">1.5</span>)</span><br><span class="line"><span class="comment"># plt.boxplot(x, notch=None, sym=None, vert=None, whis=None, positions=None, widths=None, patch_artist=None, meanline=None, showmeans=None, showcaps=None, showbox=None, showfliers=None, boxprops=None, labels=None, flierprops=None, medianprops=None, meanprops=None, capprops=None, whiskerprops=None)</span></span><br><span class="line"><span class="comment"># x：指定要绘制箱线图的数据；</span></span><br><span class="line"><span class="comment"># notch：是否是凹口的形式展现箱线图，默认非凹口；</span></span><br><span class="line"><span class="comment"># sym：指定异常点的形状，默认为+号显示；</span></span><br><span class="line"><span class="comment"># vert：是否需要将箱线图垂直摆放，默认垂直摆放；</span></span><br><span class="line"><span class="comment"># whis：指定上下须与上下四分位的距离，默认为1.5倍的四分位差；</span></span><br><span class="line"><span class="comment"># positions：指定箱线图的位置，默认为[0,1,2…]；</span></span><br><span class="line"><span class="comment"># widths：指定箱线图的宽度，默认为0.5；</span></span><br><span class="line"><span class="comment"># patch_artist：是否填充箱体的颜色；</span></span><br><span class="line"><span class="comment"># meanline：是否用线的形式表示均值，默认用点来表示；</span></span><br><span class="line"><span class="comment"># showmeans：是否显示均值，默认不显示；</span></span><br><span class="line"><span class="comment"># showcaps：是否显示箱线图顶端和末端的两条线，默认显示；</span></span><br><span class="line"><span class="comment"># showbox：是否显示箱线图的箱体，默认显示；</span></span><br><span class="line"><span class="comment"># showfliers：是否显示异常值，默认显示；</span></span><br><span class="line"><span class="comment"># boxprops：设置箱体的属性，如边框色，填充色等；</span></span><br><span class="line"><span class="comment"># labels：为箱线图添加标签，类似于图例的作用；</span></span><br><span class="line"><span class="comment"># filerprops：设置异常值的属性，如异常点的形状、大小、填充色等；</span></span><br><span class="line"><span class="comment"># medianprops：设置中位数的属性，如线的类型、粗细等；</span></span><br><span class="line"><span class="comment"># meanprops：设置均值的属性，如点的大小、颜色等；</span></span><br><span class="line"><span class="comment"># capprops：设置箱线图顶端和末端线条的属性，如颜色、粗细等；</span></span><br><span class="line"><span class="comment"># whiskerprops：设置须的属性，如颜色、粗细、线的类型等；</span></span><br><span class="line">data = np.random.normal(size=(<span class="number">100</span>, <span class="number">4</span>), loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">labels = [<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>]</span><br><span class="line"></span><br><span class="line">ax2.boxplot(data, labels=labels)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_25_0.png" alt=""></p><h3 id="颜色与样式"><a href="#颜色与样式" class="headerlink" title="颜色与样式"></a>颜色与样式</h3><h4 id="颜色"><a href="#颜色" class="headerlink" title="颜色"></a>颜色</h4><p><img src="http://xukeqiniu.xukeai.cn/cc3762519e8fa4e2bfb3a75a46abc376.png" alt=""></p><h4 id="样式"><a href="#样式" class="headerlink" title="样式"></a>样式</h4><ul><li>线条样式</li></ul><p><img src="http://xukeqiniu.xukeai.cn/2ddb1d67c59e9bedc5366fff124efdd8.png" alt=""></p><ul><li>标记样式</li></ul><p><img src="http://xukeqiniu.xukeai.cn/a69256ea85cace486b4c697ec6fc573a.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax1 = fig.add_subplot(<span class="number">321</span>)</span><br><span class="line">ax2 = fig.add_subplot(<span class="number">322</span>)</span><br><span class="line">ax3 = fig.add_subplot(<span class="number">323</span>)</span><br><span class="line">ax4 = fig.add_subplot(<span class="number">324</span>)</span><br><span class="line">ax5 = fig.add_subplot(<span class="number">313</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#内建默认颜色</span></span><br><span class="line">y=np.arange(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">ax1.plot(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#灰色阴影，html,RGB</span></span><br><span class="line">y=np.arange(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">ax2.plot(y,<span class="string">'y'</span>)   <span class="comment">#内建默认颜色</span></span><br><span class="line">ax2.plot(y+<span class="number">1</span>,color=(<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>)) <span class="comment">#RGB</span></span><br><span class="line">ax2.plot(y+<span class="number">2</span>,<span class="string">'#FF00FF'</span>)  <span class="comment">#html</span></span><br><span class="line">ax2.plot(y+<span class="number">3</span>,color=<span class="string">'0.5'</span>) <span class="comment">#灰色阴影</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#线条样式</span></span><br><span class="line">y=np.arange(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">ax3.plot(y,<span class="string">'--'</span>); <span class="comment"># 线条</span></span><br><span class="line">ax3.plot(y+<span class="number">1</span>,<span class="string">'-.'</span>); <span class="comment"># 点线</span></span><br><span class="line">ax3.plot(y+<span class="number">2</span>,<span class="string">':'</span>); <span class="comment"># 点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#点样式,是否加线条取决于marker</span></span><br><span class="line">y=np.arange(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">ax4.plot(y,marker=<span class="string">'o'</span>);</span><br><span class="line">ax4.plot(y+<span class="number">1</span>,marker=<span class="string">'D'</span>);</span><br><span class="line">ax4.plot(y+<span class="number">2</span>,marker=<span class="string">'^'</span>);</span><br><span class="line">ax4.plot(y+<span class="number">3</span>,<span class="string">'s'</span>);</span><br><span class="line">ax4.plot(y+<span class="number">4</span>,<span class="string">'p'</span>);</span><br><span class="line">ax4.plot(y+<span class="number">5</span>,<span class="string">'x'</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#样式字符串，同时表示颜色，点型，线性（重要！！）</span></span><br><span class="line">y=np.arange(<span class="number">1</span>,<span class="number">5</span>)</span><br><span class="line">ax5.plot(y,<span class="string">'cx--'</span>);</span><br><span class="line">ax5.plot(y+<span class="number">1</span>,<span class="string">'kp:'</span>);</span><br><span class="line">ax5.plot(y+<span class="number">2</span>,<span class="string">'mo-.'</span>);</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_27_0.png" alt=""></p><h3 id="网格（grid）"><a href="#网格（grid）" class="headerlink" title="网格（grid）"></a>网格（grid）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">y = np.arange(<span class="number">1</span>,<span class="number">5</span>,<span class="number">0.1</span>)</span><br><span class="line">plt.plot(y,y**<span class="number">2</span>)</span><br><span class="line">plt.grid(<span class="keyword">True</span>,color=<span class="string">'r'</span>,linestyle=<span class="string">'--'</span>,linewidth=<span class="string">'2'</span>)</span><br><span class="line"><span class="comment"># True 显示网格</span></span><br><span class="line"><span class="comment"># color 设置网格的颜色</span></span><br><span class="line"><span class="comment"># linestyle 设置线显示的类型(一共四种)</span></span><br><span class="line"><span class="comment"># linewidth 设置网格的宽度</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">x= np.arange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">0.1</span>)</span><br><span class="line">fig1 = plt.figure()</span><br><span class="line">ax1 = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax1.grid()</span><br><span class="line">ax1.plot(x,np.log(x))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_29_0.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/output_29_1.png" alt=""></p><h3 id="图例（legend）"><a href="#图例（legend）" class="headerlink" title="图例（legend）"></a>图例（legend）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">11</span>,<span class="number">1</span>)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.plot(x,x*<span class="number">2</span>,label=<span class="string">'Normal'</span>)</span><br><span class="line">ax.plot(x,x*<span class="number">3</span>,label=<span class="string">'Fast'</span>)</span><br><span class="line">ax.plot(x,x*<span class="number">4</span>,label=<span class="string">'Faster'</span>)</span><br><span class="line">ax.plot(x,x*<span class="number">5</span>,label=<span class="string">'cool'</span>)</span><br><span class="line">ax.plot(x,x*<span class="number">6</span>,label=<span class="string">'Best'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># loc= 0-10</span></span><br><span class="line"><span class="comment"># 0: ‘best'</span></span><br><span class="line"><span class="comment"># 1: ‘upper right'</span></span><br><span class="line"><span class="comment"># 2: ‘upper left'</span></span><br><span class="line"><span class="comment"># 3: ‘lower left'</span></span><br><span class="line"><span class="comment"># 4: ‘lower right'</span></span><br><span class="line"><span class="comment"># 5: ‘right'</span></span><br><span class="line"><span class="comment"># 6: ‘center left'</span></span><br><span class="line"><span class="comment"># 7: ‘center right'</span></span><br><span class="line"><span class="comment"># 8: ‘lower center'</span></span><br><span class="line"><span class="comment"># 9: ‘upper center'</span></span><br><span class="line"><span class="comment"># 10: ‘cente ’</span></span><br><span class="line"><span class="comment"># 参考 http://blog.csdn.net/helunqu2017/article/details/78641290</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_31_0.png" alt=""></p><h3 id="坐标轴范围调整"><a href="#坐标轴范围调整" class="headerlink" title="坐标轴范围调整"></a>坐标轴范围调整</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#ax.axis([0,10,0,100]) [x左，x右，y下，y上]</span></span><br><span class="line">x = np.arange(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">0.1</span>)</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.plot(x,x**<span class="number">2</span>)</span><br><span class="line">ax.axis([<span class="number">0</span>,<span class="number">10</span>,<span class="number">0</span>,<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#plt.xlim([-5,5]) ，调整x轴坐标范围</span></span><br><span class="line">fig1 = plt.figure()</span><br><span class="line">ax1 = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax1.plot(x,x**<span class="number">2</span>)</span><br><span class="line">plt.xlim([<span class="number">-5</span>,<span class="number">5</span>])</span><br><span class="line"><span class="comment">#相当于 plt.xlim(xmin=-5,xmax=5)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#plt.ylim([0,60]) ，调整y轴坐标范围</span></span><br><span class="line">fig2 = plt.figure()</span><br><span class="line">ax2 = fig2.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax2.plot(x,x**<span class="number">2</span>)</span><br><span class="line">plt.ylim([<span class="number">0</span>,<span class="number">60</span>])</span><br><span class="line"><span class="comment">#相当于 plt.ylim(ymin=0,ymax=60)</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_33_0.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/output_33_1.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/output_33_2.png" alt=""></p><h3 id="坐标轴刻度调整"><a href="#坐标轴刻度调整" class="headerlink" title="坐标轴刻度调整"></a>坐标轴刻度调整</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">11</span>,<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">fig1 = plt.figure()</span><br><span class="line">ax1  = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax1.plot(x,x)</span><br><span class="line"><span class="comment">#ax1.locator_params(nbins=20) #同时调整x轴与y轴</span></span><br><span class="line"><span class="comment">#ax1.locator_params('x',nbins=20) #只调整x轴</span></span><br><span class="line">ax1.locator_params(<span class="string">'y'</span>,nbins=<span class="number">20</span>) <span class="comment">#只调整y轴</span></span><br><span class="line">plt.axis([<span class="number">0</span>,<span class="number">10</span>,<span class="number">0</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#日期的相关调整</span></span><br><span class="line">start = datetime.datetime(<span class="number">2015</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">stop  = datetime.datetime(<span class="number">2016</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">delta = datetime.timedelta(days=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">dates = mpl.dates.drange(start,stop,delta)</span><br><span class="line">y = np.random.rand(len(dates))</span><br><span class="line"></span><br><span class="line">fig2 = plt.figure()</span><br><span class="line">ax2  = fig2.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax2.plot_date(dates,y,linestyle=<span class="string">'-'</span>,marker=<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#日期格式调整,不重叠</span></span><br><span class="line">date_format= mpl.dates.DateFormatter(<span class="string">'%Y-%m'</span>)</span><br><span class="line">ax2.xaxis.set_major_formatter(date_format)</span><br><span class="line">fig2.autofmt_xdate()<span class="comment">#防止重叠</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_35_0.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/output_35_1.png" alt=""></p><h3 id="图中添加新坐标轴"><a href="#图中添加新坐标轴" class="headerlink" title="图中添加新坐标轴"></a>图中添加新坐标轴</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">1</span>,<span class="number">11</span>,<span class="number">0.1</span>)</span><br><span class="line">y1 = x*x</span><br><span class="line">y2 = np.log(x)</span><br><span class="line"></span><br><span class="line">fig1 = plt.figure()</span><br><span class="line">ax1 = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax2 = ax1.twinx()</span><br><span class="line"></span><br><span class="line"><span class="comment">#ax1.set_ylable('Y1')</span></span><br><span class="line"><span class="comment">#ax2.set_ylable('Y2')</span></span><br><span class="line"></span><br><span class="line">ax1.plot(x,y1)</span><br><span class="line">ax2.plot(x,y2,<span class="string">'--r'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_37_0.png" alt=""></p><h3 id="图中画注释符号"><a href="#图中画注释符号" class="headerlink" title="图中画注释符号"></a>图中画注释符号</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">x =np.arange(<span class="number">-10</span>,<span class="number">11</span>,<span class="number">1</span>)</span><br><span class="line">y = x*x</span><br><span class="line"></span><br><span class="line">fig1 = plt.figure()</span><br><span class="line">ax1 = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax1.plot(x,y)</span><br><span class="line">ax1.annotate(<span class="string">'this is bottom'</span>,xy=(<span class="number">0</span>,<span class="number">0</span>),xytext=(<span class="number">-1.25</span>,<span class="number">20</span>),</span><br><span class="line">             arrowprops=dict(facecolor=<span class="string">'r'</span>,frac=<span class="number">0.2</span>))</span><br><span class="line"><span class="comment"># annotate(s, xy, xytext=None, xycoords='data',textcoords='data'arrowprops=None, **kwargs)</span></span><br><span class="line"><span class="comment"># s : 描述的内容</span></span><br><span class="line"><span class="comment"># xy : 加描述的点</span></span><br><span class="line"><span class="comment"># xytext : 标注的位置，xytext=(30,-30),表示从标注点x轴方向上增加30，y轴方减30的位置</span></span><br><span class="line"><span class="comment"># xycoords 、textcoords :这两个参数试了好多次没弄明白，只知道 xycoords='dat给定就行，</span></span><br><span class="line"><span class="comment"># textcoords='offset points' 标注的内容从xy设置的点进行偏移xytext</span></span><br><span class="line"><span class="comment"># textcoords='data' 标注内容为xytext的绝对坐标</span></span><br><span class="line"><span class="comment"># fontsize : 字体大小，这个没什么好说的</span></span><br><span class="line"><span class="comment"># arrowstyle : 箭头样式'-&gt;'指向标注点 '&lt;-'指向标注内容 还有很多'-'</span></span><br><span class="line">               <span class="comment"># '-&gt;'   head_length=0.4,head_width=0.2</span></span><br><span class="line">               <span class="comment"># '-['  widthB=1.0,lengthB=0.2,angleB=None</span></span><br><span class="line">               <span class="comment"># '|-|'     widthA=1.0,widthB=1.0</span></span><br><span class="line">               <span class="comment"># '-|&gt;'  head_length=0.4,head_width=0.2</span></span><br><span class="line">               <span class="comment"># '&lt;-'   head_length=0.4,head_width=0.2</span></span><br><span class="line">               <span class="comment"># '&lt;-&gt;'   head_length=0.4,head_width=0.2</span></span><br><span class="line">               <span class="comment"># '&lt;|-'  head_length=0.4,head_width=0.2</span></span><br><span class="line">               <span class="comment"># '&lt;|-|&gt;'     head_length=0.4,head_width=0.2</span></span><br><span class="line">               <span class="comment"># 'fancy'   head_length=0.4,head_width=0.4,tail_width=0.4</span></span><br><span class="line">               <span class="comment"># 'simple'  head_length=0.5,head_width=0.5,tail_width=0.2</span></span><br><span class="line">               <span class="comment"># 'wedge'   tail_width=0.3,shrink_factor=0.5</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_39_1.png" alt=""></p><h3 id="图形中纯文字标注"><a href="#图形中纯文字标注" class="headerlink" title="图形中纯文字标注"></a>图形中纯文字标注</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x =np.arange(<span class="number">-10</span>,<span class="number">11</span>,<span class="number">1</span>)</span><br><span class="line">y = x*x</span><br><span class="line"></span><br><span class="line">fig1 = plt.figure()</span><br><span class="line">ax1 = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax1.plot(x,y)</span><br><span class="line">ax1.text(<span class="number">-3</span>,<span class="number">40</span>,<span class="string">'function:y=x*x'</span>,family=<span class="string">'fantasy'</span>,size=<span class="number">15</span>,color=<span class="string">'g'</span>,style=<span class="string">'oblique'</span>,weight=<span class="number">20</span>,bbox=dict(facecolor=<span class="string">'r'</span>,alpha=<span class="number">0.2</span>))</span><br><span class="line">ax1.text(<span class="number">-3</span>,<span class="number">30</span>,<span class="string">'function:y=x*x'</span>,family=<span class="string">'serif'</span>,size=<span class="number">15</span>,color=<span class="string">'r'</span>,style=<span class="string">'italic'</span>,weight=<span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_41_0.png" alt=""></p><h3 id="图像中画数学公式"><a href="#图像中画数学公式" class="headerlink" title="图像中画数学公式"></a>图像中画数学公式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig1 = plt.figure()</span><br><span class="line">ax1 = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax1.set_xlim([<span class="number">1</span>,<span class="number">7</span>])</span><br><span class="line">ax1.set_ylim([<span class="number">1</span>,<span class="number">5</span>])</span><br><span class="line">ax1.text(<span class="number">2</span>,<span class="number">4</span>,<span class="string">r"$ \alpha_i \beta_j \pi \lambda \omega $"</span>,size=<span class="number">15</span>)</span><br><span class="line">ax1.text(<span class="number">4</span>,<span class="number">4</span>,<span class="string">r"$ \sin(0)=\cos(\frac&#123;\pi&#125;&#123;2&#125;) $"</span>,size=<span class="number">15</span>)</span><br><span class="line">ax1.text(<span class="number">2</span>,<span class="number">2</span>,<span class="string">r"$ \lim_&#123;x \rightarrow y&#125; \frac&#123;1&#125;&#123;x^3&#125; $"</span>,size=<span class="number">15</span>)</span><br><span class="line">ax1.text(<span class="number">4</span>,<span class="number">2</span>,<span class="string">r"$ \sqrt[4]&#123;x&#125;=\sqrt&#123;y&#125;$"</span>,size=<span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_43_0.png" alt=""></p><h3 id="填充上色"><a href="#填充上色" class="headerlink" title="填充上色"></a>填充上色</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">5</span>*np.pi,<span class="number">1000</span>)</span><br><span class="line">y1 = np.sin(x)</span><br><span class="line">y2 = np.sin(<span class="number">2</span>*x)</span><br><span class="line"></span><br><span class="line">fig1 = plt.figure()</span><br><span class="line">ax1 = fig1.add_subplot(<span class="number">111</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ax1.fill(x,y1,'g',alpha=0.2)</span></span><br><span class="line"><span class="comment"># ax1.fill(x,y2,'r',alpha=0.2)</span></span><br><span class="line">ax1.fill_between(x,y1,y2,where=y1&gt;y2,facecolor=<span class="string">'y'</span>)</span><br><span class="line">ax1.fill_between(x,y1,y2,where=y1&lt;y2,facecolor=<span class="string">'b'</span>)</span><br><span class="line">ax1.grid()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_45_0.png" alt=""></p><h3 id="画填充好的图形"><a href="#画填充好的图形" class="headerlink" title="画填充好的图形"></a>画填充好的图形</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> mpatches</span><br><span class="line">fig,ax = plt.subplots()</span><br><span class="line"></span><br><span class="line">xy1 = np.array([<span class="number">0.2</span>,<span class="number">0.2</span>])</span><br><span class="line">xy2 = np.array([<span class="number">0.2</span>,<span class="number">0.8</span>])</span><br><span class="line">xy3 = np.array([<span class="number">0.8</span>,<span class="number">0.2</span>])</span><br><span class="line">xy4 = np.array([<span class="number">0.8</span>,<span class="number">0.8</span>])</span><br><span class="line"></span><br><span class="line">circle = mpatches.Circle(xy1,<span class="number">0.05</span>) <span class="comment">#xy1 圆心</span></span><br><span class="line">rect = mpatches.Rectangle(xy2,<span class="number">0.2</span>,<span class="number">0.1</span>,color=<span class="string">'r'</span>) <span class="comment">#xy2 左下角对应的点</span></span><br><span class="line">polygen = mpatches.RegularPolygon(xy3,<span class="number">5</span>,<span class="number">0.1</span>,color=<span class="string">'g'</span>) <span class="comment">#xy3 圆心</span></span><br><span class="line">ellipse = mpatches.Ellipse(xy4,<span class="number">0.4</span>,<span class="number">0.2</span>,color=<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line">ax.add_patch(circle)</span><br><span class="line">ax.add_patch(rect)</span><br><span class="line">ax.add_patch(polygen)</span><br><span class="line">ax.add_patch(ellipse)</span><br><span class="line"></span><br><span class="line">plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_47_0.png" alt=""></p><h3 id="美化图形"><a href="#美化图形" class="headerlink" title="美化图形"></a>美化图形</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用ggplot绘画风格</span></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"></span><br><span class="line">fig,axes = plt.subplots(ncols=<span class="number">2</span>,nrows=<span class="number">2</span>)</span><br><span class="line">ax1,ax2,ax3,ax4 = axes.ravel()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x,y = np.random.normal(size=(<span class="number">2</span>,<span class="number">100</span>))</span><br><span class="line">ax1.plot(x,y,<span class="string">'o'</span>)</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line">y = np.arange(<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line">ncolors = len(plt.rcParams[<span class="string">'axes.color_cycle'</span>])</span><br><span class="line">shift = np.linspace(<span class="number">0</span>,<span class="number">10</span>,ncolors)</span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> shift:</span><br><span class="line">    ax2.plot(x,y+s,<span class="string">'-'</span>)</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">5</span>)</span><br><span class="line">y1,y2,y3 = np.random.randint(<span class="number">1</span>,<span class="number">25</span>,size=(<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line">width = <span class="number">0.25</span></span><br><span class="line"></span><br><span class="line">ax3.bar(x,y1,width)</span><br><span class="line">ax3.bar(x+width,y2,width,color=<span class="string">'r'</span>)</span><br><span class="line">ax3.bar(x+<span class="number">2</span>*width,y3,width,color=<span class="string">'g'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i,color <span class="keyword">in</span>  enumerate(plt.rcParams[<span class="string">'axes.color_cycle'</span>]):</span><br><span class="line">    xy = np.random.normal(size=<span class="number">2</span>)</span><br><span class="line">    ax4.add_patch(plt.Circle(xy,radius=<span class="number">0.3</span>,color=color))</span><br><span class="line">ax4.axis(<span class="string">'equal'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_49_1.png" alt=""></p><h3 id="极坐标"><a href="#极坐标" class="headerlink" title="极坐标"></a>极坐标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"></span><br><span class="line">r = np.arange(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line">theta = [<span class="number">0</span>,np.pi/<span class="number">2</span>,np.pi,np.pi*<span class="number">3</span>/<span class="number">2</span>,<span class="number">2</span>*np.pi]</span><br><span class="line"></span><br><span class="line">ax = plt.subplot(<span class="number">111</span>,projection=<span class="string">'polar'</span>)</span><br><span class="line"></span><br><span class="line">ax.plot(theta,r,color=<span class="string">'r'</span>,linewidth=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">ax.grid(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_51_0.png" alt=""></p><h3 id="函数积分图"><a href="#函数积分图" class="headerlink" title="函数积分图"></a>函数积分图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -(x<span class="number">-2</span>)*(x<span class="number">-8</span>)+<span class="number">40</span></span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line">y = func(x)</span><br><span class="line"></span><br><span class="line">fig,axes = plt.subplots()</span><br><span class="line">axes.plot(x,y,<span class="string">'r'</span>,linewidth=<span class="number">2</span>)</span><br><span class="line">a = <span class="number">2</span></span><br><span class="line">b = <span class="number">9</span></span><br><span class="line">axes.set_xticks([a,b])</span><br><span class="line">axes.set_yticks([])</span><br><span class="line">axes.set_xticklabels([<span class="string">r'$a$'</span>,<span class="string">'$b$'</span>])</span><br><span class="line"></span><br><span class="line">ix = np.linspace(a,b)</span><br><span class="line">iy = func(ix)</span><br><span class="line">ixy = zip(ix,iy)</span><br><span class="line"></span><br><span class="line">verts = [(a,<span class="number">0</span>)]+list(ixy)+[(b,<span class="number">0</span>)]</span><br><span class="line">poly = Polygon(verts,facecolor=<span class="string">'0.8'</span>,edgecolor=<span class="string">'0.5'</span>)</span><br><span class="line">axes.add_patch(poly)</span><br><span class="line"></span><br><span class="line">plt.figtext(<span class="number">0.9</span>,<span class="number">0.07</span>,<span class="string">r'$x$'</span>)</span><br><span class="line">plt.figtext(<span class="number">0.1</span>,<span class="number">0.9</span>,<span class="string">r'$y$'</span>)</span><br><span class="line"></span><br><span class="line">x_math = (a+b)*<span class="number">0.5</span></span><br><span class="line">y_math = <span class="number">35</span></span><br><span class="line"></span><br><span class="line">plt.text(x_math,y_math,<span class="string">r'$\int_a^b(-(x-2)*(x-8)+40)dx$'</span>,fontsize=<span class="number">10</span>,horizontalalignment=<span class="string">'center'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_53_0.png" alt=""></p><h3 id="二维散点概率分布图"><a href="#二维散点概率分布图" class="headerlink" title="二维散点概率分布图"></a>二维散点概率分布图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"></span><br><span class="line">x = np.random.randn(<span class="number">200</span>)</span><br><span class="line">y = x + np.random.randn(<span class="number">200</span>)*<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">margin_border = <span class="number">0.1</span></span><br><span class="line">width = <span class="number">0.6</span></span><br><span class="line">margin_between = <span class="number">0.02</span></span><br><span class="line">height = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">left_s = margin_border</span><br><span class="line">bottom_s = margin_border</span><br><span class="line">height_s = width</span><br><span class="line">width_s = width</span><br><span class="line"></span><br><span class="line">left_x = margin_border</span><br><span class="line">bottom_x = margin_border+width+margin_between</span><br><span class="line">height_x = height</span><br><span class="line">width_x = width</span><br><span class="line"></span><br><span class="line">left_y = margin_border+width+margin_between</span><br><span class="line">bottom_y = margin_border</span><br><span class="line">height_y = width</span><br><span class="line">width_y = height</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>,figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">rect_s = [left_s,bottom_s,width_s,height_s]</span><br><span class="line">rect_x = [left_x,bottom_x,width_x,height_x]</span><br><span class="line">rect_y = [left_y,bottom_y,width_y,height_y]</span><br><span class="line"></span><br><span class="line">axScatter = plt.axes(rect_s)</span><br><span class="line">axHisX = plt.axes(rect_x)</span><br><span class="line">axHisY = plt.axes(rect_y)</span><br><span class="line">axHisX.set_xticks([])</span><br><span class="line">axHisY.set_yticks([])</span><br><span class="line"></span><br><span class="line">axScatter.scatter(x,y)</span><br><span class="line"></span><br><span class="line">bin_width = <span class="number">0.25</span></span><br><span class="line">xymax = np.max([np.max(np.fabs(x)),np.max(np.fabs(y))])</span><br><span class="line"></span><br><span class="line">lim =int(xymax/bin_width+<span class="number">1</span>) * bin_width</span><br><span class="line"></span><br><span class="line">axScatter.set_xlim(-lim,lim)</span><br><span class="line">axScatter.set_ylim(-lim,lim)</span><br><span class="line"></span><br><span class="line">bins = np.arange(-lim,lim+bin_width,bin_width)</span><br><span class="line"></span><br><span class="line">axHisX.hist(x,bins=bins)</span><br><span class="line">axHisY.hist(y,bins=bins,orientation=<span class="string">'horizontal'</span>)</span><br><span class="line"></span><br><span class="line">axHisX.set_xlim(axScatter.get_xlim())</span><br><span class="line">axHisY.set_ylim(axScatter.get_ylim())</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Scatter and Hist'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/0056ccc98047db18ae5cb77eba40dd18.png" alt=""></p><h3 id="完整的绘制程序综合"><a href="#完整的绘制程序综合" class="headerlink" title="完整的绘制程序综合"></a>完整的绘制程序综合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据部分</span></span><br><span class="line">x = np.arange(<span class="number">0.</span>, <span class="number">10</span>, <span class="number">0.2</span>)</span><br><span class="line">y1 = np.cos(x)</span><br><span class="line">y2 = np.sin(x)</span><br><span class="line">y3 = np.sqrt(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制 3 条函数曲线</span></span><br><span class="line">plt.plot(x, y1, color=<span class="string">'blue'</span>, linewidth=<span class="number">1.5</span>, linestyle=<span class="string">'-'</span>, marker=<span class="string">'.'</span>, label=<span class="string">r'$y = cos&#123;x&#125;$'</span>)</span><br><span class="line">plt.plot(x, y2, color=<span class="string">'green'</span>, linewidth=<span class="number">1.5</span>, linestyle=<span class="string">'-'</span>, marker=<span class="string">'*'</span>, label=<span class="string">r'$y = sin&#123;x&#125;$'</span>)</span><br><span class="line">plt.plot(x, y3, color=<span class="string">'m'</span>, linewidth=<span class="number">1.5</span>, linestyle=<span class="string">'-'</span>, marker=<span class="string">'x'</span>, label=<span class="string">r'$y = \sqrt&#123;x&#125;$'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 坐标轴上移</span></span><br><span class="line">ax = plt.subplot(<span class="number">111</span>)</span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)     <span class="comment"># 去掉右边的边框线</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)       <span class="comment"># 去掉上边的边框线</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动下边边框线，相当于移动 X 轴</span></span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">'bottom'</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动左边边框线，相当于移动 y 轴</span></span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">'left'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 x, y 轴的取值范围</span></span><br><span class="line">plt.xlim(x.min()*<span class="number">1.1</span>, x.max()*<span class="number">1.1</span>)</span><br><span class="line">plt.ylim(<span class="number">-1.5</span>, <span class="number">4.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 x, y 轴的刻度值</span></span><br><span class="line">plt.xticks([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>], [<span class="string">r'2'</span>, <span class="string">r'4'</span>, <span class="string">r'6'</span>, <span class="string">r'8'</span>, <span class="string">r'10'</span>])</span><br><span class="line">plt.yticks([<span class="number">-1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>],</span><br><span class="line">    [<span class="string">r'-1.0'</span>, <span class="string">r'0.0'</span>, <span class="string">r'1.0'</span>, <span class="string">r'2.0'</span>, <span class="string">r'3.0'</span>, <span class="string">r'4.0'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加文字</span></span><br><span class="line">plt.text(<span class="number">4</span>, <span class="number">1.68</span>, <span class="string">r'$x \in [0.0, \ 10.0]$'</span>, color=<span class="string">'k'</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">plt.text(<span class="number">4</span>, <span class="number">1.38</span>, <span class="string">r'$y \in [-1.0, \ 4.0]$'</span>, color=<span class="string">'k'</span>, fontsize=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特殊点添加注解</span></span><br><span class="line">plt.scatter([<span class="number">8</span>,],[np.sqrt(<span class="number">8</span>),], <span class="number">50</span>, color =<span class="string">'m'</span>)  <span class="comment"># 使用散点图放大当前点</span></span><br><span class="line">plt.annotate(<span class="string">r'$2\sqrt&#123;2&#125;$'</span>, xy=(<span class="number">8</span>, np.sqrt(<span class="number">8</span>)), xytext=(<span class="number">8.5</span>, <span class="number">2.2</span>), fontsize=<span class="number">16</span>, color=<span class="string">'#090909'</span>, arrowprops=dict(arrowstyle=<span class="string">'-&gt;'</span>, connectionstyle=<span class="string">'arc3, rad=0.1'</span>, color=<span class="string">'#090909'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置标题、x轴、y轴</span></span><br><span class="line">plt.title(<span class="string">r'$the \ function \ figure \ of \ cos(), \ sin() \ and \ sqrt()$'</span>, fontsize=<span class="number">19</span>)</span><br><span class="line">plt.xlabel(<span class="string">r'$the \ input \ value \ of \ x$'</span>, fontsize=<span class="number">18</span>, labelpad=<span class="number">88.8</span>)</span><br><span class="line">plt.ylabel(<span class="string">r'$y = f(x)$'</span>, fontsize=<span class="number">18</span>, labelpad=<span class="number">12.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置图例及位置</span></span><br><span class="line">plt.legend(loc=<span class="string">'up right'</span>)</span><br><span class="line"><span class="comment"># plt.legend(['cos(x)', 'sin(x)', 'sqrt(x)'], loc='up right')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示网格线</span></span><br><span class="line">plt.grid(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示绘图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/output_57_1.png" alt=""></p><h3 id="绘制三维图形"><a href="#绘制三维图形" class="headerlink" title="绘制三维图形"></a>绘制三维图形</h3><p>首先补充一下<code>numpy</code>中<code>meshgrid</code>函数的用法。具体含义如下图所示<br><img src="http://xukeqiniu.xukeai.cn/a9be6a5d2f076272741b0a0073892ec8.png" alt="`meshgrid`函数的用法"></p><h4 id="绘制3D曲面图"><a href="#绘制3D曲面图" class="headerlink" title="绘制3D曲面图"></a>绘制3D曲面图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D <span class="comment">#导入三维绘制工具箱</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure() <span class="comment"># 创建一个绘图对象</span></span><br><span class="line">ax = Axes3D(fig) <span class="comment"># #用这个绘图对象创建一个Axes对象(有3D坐标)</span></span><br><span class="line">X = np.arange(<span class="number">-4</span>, <span class="number">4</span>, <span class="number">0.25</span>) <span class="comment">#创建从-4到4，步长为0.25的arange对象</span></span><br><span class="line">Y = np.arange(<span class="number">-4</span>, <span class="number">4</span>, <span class="number">0.25</span>)</span><br><span class="line">X, Y = np.meshgrid(X, Y) <span class="comment">#用这两个arange对象中的可能取值一一映射去扩充为所有可能的取样点</span></span><br><span class="line">R = np.sqrt(X**<span class="number">2</span> + Y**<span class="number">2</span>) <span class="comment">#函数表示</span></span><br><span class="line">Z = np.sin(R)</span><br><span class="line"><span class="comment"># 具体函数方法可用 help(function) 查看，如：help(ax.plot_surface)</span></span><br><span class="line"><span class="comment"># rstride和cstride表示行列隔多少个取样点建一个小面</span></span><br><span class="line"><span class="comment"># cmap表示绘制曲面的颜色</span></span><br><span class="line">ax.plot_surface(X, Y, Z, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>, cmap=<span class="string">'rainbow'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/b086f212707991f00dba6391043e6381.png" alt=""></p><h4 id="绘制三维散点图"><a href="#绘制三维散点图" class="headerlink" title="绘制三维散点图"></a>绘制三维散点图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line">data = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, size=[<span class="number">40</span>, <span class="number">40</span>, <span class="number">40</span>])</span><br><span class="line"><span class="comment">#data = np.random.randint(0, 255, size=[3,40, 40, 40]) #四维</span></span><br><span class="line">x, y, z = data[<span class="number">0</span>], data[<span class="number">1</span>], data[<span class="number">2</span>]</span><br><span class="line"><span class="comment">#x, y, z = data[0,1], data[0,2], data[0,3] #取值时需要[0,index]</span></span><br><span class="line">ax = plt.subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)  <span class="comment"># 创建一个三维的绘图工程</span></span><br><span class="line"><span class="comment">#  将数据点分成三部分画，在颜色上有区分度</span></span><br><span class="line">ax.scatter(x[:<span class="number">10</span>], y[:<span class="number">10</span>], z[:<span class="number">10</span>], c=<span class="string">'y'</span>)  <span class="comment"># 绘制数据点</span></span><br><span class="line">ax.scatter(x[<span class="number">10</span>:<span class="number">20</span>], y[<span class="number">10</span>:<span class="number">20</span>], z[<span class="number">10</span>:<span class="number">20</span>], c=<span class="string">'r'</span>)</span><br><span class="line">ax.scatter(x[<span class="number">30</span>:<span class="number">40</span>], y[<span class="number">30</span>:<span class="number">40</span>], z[<span class="number">30</span>:<span class="number">40</span>], c=<span class="string">'g'</span>)</span><br><span class="line"></span><br><span class="line">ax.set_zlabel(<span class="string">'Z'</span>)  <span class="comment"># 坐标轴</span></span><br><span class="line">ax.set_ylabel(<span class="string">'Y'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'X'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/be7650db3c29cb8c4aab16c8a4b77313.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>matplotlib核心剖析（<a href="http://www.cnblogs.com/vamei/archive/2013/01/30/2879700.html#commentform）" target="_blank" rel="noopener">http://www.cnblogs.com/vamei/archive/2013/01/30/2879700.html#commentform）</a><br>Numpy中Meshgrid函数介绍及2种应用场景(<a href="https://zhuanlan.zhihu.com/p/29663486" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29663486</a>)</p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Matplotlib </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>NumPy学习笔记</title>
      <link href="/2018/01/03/Python/01-numpy/"/>
      <url>/2018/01/03/Python/01-numpy/</url>
      <content type="html"><![CDATA[<p><img src="http://xukeqiniu.xukeai.cn/0c22d321140e5e4bb6a6704609678ce8.png" alt=""></p><h2 id="Numpy基础"><a href="#Numpy基础" class="headerlink" title="Numpy基础"></a>Numpy基础</h2><p><img src="http://xukeqiniu.xukeai.cn/43fd2d12d2c800b8077a567c7fff0b9e.png" alt=""><br>Numpy 是 Python 的一个科学计算包，包含了多维数组以及多维数组的操作</p><p>关于Numpy需要知道的几点：</p><ul><li><p>NumPy 数组在创建时有<strong>固定的大小</strong>，不同于Python列表（可以<strong>动态增长</strong>）。<strong>更改ndarray的大小将创建一个新的数组并删除原始数据</strong>。</p></li><li><p>NumPy 数组中的元素都需要具有<strong>相同的数据类型</strong>，因此在存储器中将具有相同的大小。数组的元素如果也是数组（可以是 Python 的原生 array，也可以是 ndarray）的情况下，则构成了多维数组。</p></li><li><p>NumPy 数组便于对大量数据进行高级数学和其他类型的操作。<strong>通常，这样的操作比使用Python的内置序列可能更有效和更少的代码执行</strong>。</p></li></ul><a id="more"></a><h3 id="ndarray的内存结构"><a href="#ndarray的内存结构" class="headerlink" title="ndarray的内存结构"></a>ndarray的内存结构</h3><p>Numpy 的核心是<code>ndarray</code>对象，这个对象封装了同质数据类型的n维数组。起名 <code>ndarray</code> 的原因就是因为是 <code>n-dimension-array</code> 的简写。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]], dtype=np.float32)</span><br></pre></td></tr></table></figure><p>我们来看一下<code>ndarray</code>如何在内存中储存的：关于数组的描述信息保存在一个数据结构中，这个结构引用两个对象，一块用于保存数据的存储区域和一个用于描述元素类型的<code>dtype</code>对象。<br><img src="http://xukeqiniu.xukeai.cn/d41ca12db14db33b463ae99edc68d80a.png" alt=""><br>数据存储区域保存着数组中所有元素的二进制数据，<code>dtype</code>对象则知道如何将元素的二进制数据转换为可用的值。数组的维数、大小等信息都保存在<code>ndarray</code>数组对象的数据结构中。<br><code>strides</code>中保存的是当每个轴的下标增加1时，数据存储区中的指针所增加的字节数。例如图中的<code>strides</code>为12,4，即<strong>第0轴的下标增加1时，数据的地址增加12个字节</strong>：即a[1,0]的地址比a[0,0]的地址要高12个字节，正好是3个单精度浮点数的总字节数；<strong>第1轴下标增加1时，数据的地址增加4个字节</strong>，正好是单精度浮点数的字节数。</p><h3 id="Numpy数组初始化"><a href="#Numpy数组初始化" class="headerlink" title="Numpy数组初始化"></a>Numpy数组初始化</h3><h4 id="可以调用np-array去从list初始化一个数组"><a href="#可以调用np-array去从list初始化一个数组" class="headerlink" title="可以调用np.array去从list初始化一个数组:"></a>可以调用np.array去从list初始化一个数组:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># 1维数组</span></span><br><span class="line">print(type(a), a.shape, a[<span class="number">0</span>], a[<span class="number">1</span>], a[<span class="number">2</span>])</span><br><span class="line">a[<span class="number">0</span>] = <span class="number">5</span>                 <span class="comment"># 重新赋值</span></span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;numpy.ndarray&apos;&gt; (3,) 1 2 3[5 2 3]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])   <span class="comment"># 2维数组</span></span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><pre><code>[[1 2 3] [4 5 6]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(b.shape)  <span class="comment">#可以看形状（非常常用！）</span></span><br><span class="line">print(b[<span class="number">0</span>, <span class="number">0</span>], b[<span class="number">0</span>, <span class="number">1</span>], b[<span class="number">1</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>(2, 3)1 2 4</code></pre><h4 id="一些内置的创建数组的函数"><a href="#一些内置的创建数组的函数" class="headerlink" title="一些内置的创建数组的函数"></a>一些内置的创建数组的函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.zeros((<span class="number">2</span>,<span class="number">2</span>))  <span class="comment"># 创建2x2的全0数组</span></span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><pre><code>[[ 0.  0.] [ 0.  0.]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.ones((<span class="number">1</span>,<span class="number">2</span>))   <span class="comment"># 创建1x2的全1数组</span></span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><pre><code>[[ 1.  1.]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = np.full((<span class="number">2</span>,<span class="number">2</span>), <span class="number">7</span>) <span class="comment"># 定值数组</span></span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure><pre><code>[[7 7] [7 7]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = np.eye(<span class="number">2</span>)        <span class="comment"># 对角矩阵（对角元素为1）</span></span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure><pre><code>[[ 1.  0.] [ 0.  1.]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">e = np.random.random((<span class="number">2</span>,<span class="number">2</span>)) <span class="comment"># 2x2的随机数组(矩阵)</span></span><br><span class="line">print(e)</span><br></pre></td></tr></table></figure><pre><code>[[ 0.72776966  0.94164821] [ 0.04652655  0.2316599 ]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = np.empty((<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)) <span class="comment"># empty是未初始化的数据，默认为0</span></span><br><span class="line">print(f)</span><br><span class="line">print(f.shape)</span><br></pre></td></tr></table></figure><pre><code>[[[ 0.  0.]  [ 0.  0.]  [ 0.  0.]] [[ 0.  0.]  [ 0.  0.]  [ 0.  0.]]](2, 3, 2)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g = np.arange(<span class="number">15</span>) <span class="comment"># 用arange可以生成连续的一串元素</span></span><br><span class="line">print(g)</span><br><span class="line">print(g.shape)</span><br></pre></td></tr></table></figure><pre><code>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14](15,)</code></pre><h3 id="Numpy数组数据类型"><a href="#Numpy数组数据类型" class="headerlink" title="Numpy数组数据类型"></a>Numpy数组数据类型</h3><p>我们可以用dtype来看numpy数组中元素的类型:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>])  <span class="comment"># numpy构建数组的时候自己会确定类型</span></span><br><span class="line">y = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">z = np.array([<span class="number">1</span>, <span class="number">2</span>], dtype=np.int64)<span class="comment"># 指定用int64构建</span></span><br><span class="line"></span><br><span class="line">print(x.dtype, y.dtype, z.dtype)</span><br></pre></td></tr></table></figure><pre><code>int64 float64 int64</code></pre><h4 id="使用astype复制数组并转换数据类型"><a href="#使用astype复制数组并转换数据类型" class="headerlink" title="使用astype复制数组并转换数据类型"></a>使用astype复制数组并转换数据类型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int_arr = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">float_arr = int_arr.astype(np.float)</span><br><span class="line">print(int_arr.dtype)</span><br><span class="line">print(float_arr.dtype)</span><br></pre></td></tr></table></figure><pre><code>int32float64</code></pre><h4 id="使用astype将float转换为int时小数部分被舍弃"><a href="#使用astype将float转换为int时小数部分被舍弃" class="headerlink" title="使用astype将float转换为int时小数部分被舍弃"></a>使用astype将float转换为int时小数部分被舍弃</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">float_arr = np.array([<span class="number">3.7</span>, <span class="number">-1.2</span>, <span class="number">-2.6</span>, <span class="number">0.5</span>, <span class="number">12.9</span>, <span class="number">10.1</span>])</span><br><span class="line">int_arr = float_arr.astype(dtype = np.int)</span><br><span class="line">print(int_arr)</span><br></pre></td></tr></table></figure><pre><code>[ 3 -1 -2  0 12 10]</code></pre><h4 id="使用astype把字符串转换为数组，如果失败抛出异常。"><a href="#使用astype把字符串转换为数组，如果失败抛出异常。" class="headerlink" title="使用astype把字符串转换为数组，如果失败抛出异常。"></a>使用astype把字符串转换为数组，如果失败抛出异常。</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">str_arr = np.array([<span class="string">'1.25'</span>, <span class="string">'-9.6'</span>, <span class="string">'42'</span>], dtype = np.string_)</span><br><span class="line">float_arr = str_arr.astype(dtype = np.float)</span><br><span class="line">print(float_arr)</span><br></pre></td></tr></table></figure><pre><code>[  1.25  -9.6   42.  ]</code></pre><h4 id="astype使用其它数组的数据类型作为参数"><a href="#astype使用其它数组的数据类型作为参数" class="headerlink" title="astype使用其它数组的数据类型作为参数"></a>astype使用其它数组的数据类型作为参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int_arr = np.arange(<span class="number">10</span>)</span><br><span class="line">float_arr = np.array([<span class="number">.23</span>, <span class="number">0.270</span>, <span class="number">.357</span>, <span class="number">0.44</span>, <span class="number">0.5</span>], dtype = np.float64)</span><br><span class="line">print(int_arr.astype(float_arr.dtype)) <span class="comment">#转换为浮点类型 [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]</span></span><br><span class="line">print(int_arr[<span class="number">0</span>], int_arr[<span class="number">1</span>]) <span class="comment"># 0 1</span></span><br></pre></td></tr></table></figure><pre><code>[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]0 1</code></pre><h3 id="Numpy的复制和视图"><a href="#Numpy的复制和视图" class="headerlink" title="Numpy的复制和视图"></a>Numpy的复制和视图</h3><p>当计算和操作数组时，它们的数据有时被复制到新的数组中，有时不复制。这里我们做个区分。</p><h4 id="完全不复制"><a href="#完全不复制" class="headerlink" title="完全不复制"></a>完全不复制</h4><p>简单赋值不会创建数组对象或其数据的拷贝。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)</span><br><span class="line">print(a)</span><br><span class="line">b = a</span><br><span class="line">print(id(a))</span><br><span class="line">print(id(b)) <span class="comment"># id(a)和id(b)结果相同</span></span><br><span class="line">b.shape =  <span class="number">3</span>,<span class="number">2</span></span><br><span class="line">print(a.shape) <span class="comment"># 修改b形状，结果a的形状也变了</span></span><br></pre></td></tr></table></figure><pre><code>[0 1 2 3 4 5]31696697978083169669797808(3, 2)</code></pre><h4 id="视图或浅复制"><a href="#视图或浅复制" class="headerlink" title="视图或浅复制"></a>视图或浅复制</h4><p>不同的数组对象可以共享相同的数据。<code>view</code>方法创建一个新数组对象，该对象看到相同的数据。与前一种情况不同，<strong>新数组的维数更改不会更改原始数据的维数</strong>，但是<strong>新数组数据更改后，也会影响原始数据</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)</span><br><span class="line">c = a.view()</span><br><span class="line">print(c <span class="keyword">is</span> a) <span class="comment">#False</span></span><br><span class="line">print(c.base <span class="keyword">is</span> a) <span class="comment">#True</span></span><br><span class="line">print(c.flags.owndata) <span class="comment">#False</span></span><br><span class="line">c.shape =(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">print(a.shape) <span class="comment">#(6,)</span></span><br><span class="line">c[<span class="number">0</span>,<span class="number">1</span>] = <span class="number">1234</span></span><br><span class="line">print(a) <span class="comment"># [   0 1234    2    3    4    5]</span></span><br></pre></td></tr></table></figure><pre><code>FalseTrueFalse(6,)[   0 1234    2    3    4    5]</code></pre><h4 id="深复制"><a href="#深复制" class="headerlink" title="深复制"></a>深复制</h4><p><code>copy</code>方法生成数组及其数据的完整拷贝。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">6</span>)</span><br><span class="line">d = a.copy() <span class="comment"># 一个完整的新的数组</span></span><br><span class="line">print(d <span class="keyword">is</span> a)</span><br><span class="line">print(d.base <span class="keyword">is</span> a )</span><br><span class="line">d[<span class="number">0</span>] = <span class="number">9999</span></span><br><span class="line">print(a) <span class="comment"># 修改数组 d 的值，a不会受影响</span></span><br></pre></td></tr></table></figure><pre><code>FalseFalse[0 1 2 3 4 5]</code></pre><h3 id="Numpy数组取值和赋值"><a href="#Numpy数组取值和赋值" class="headerlink" title="Numpy数组取值和赋值"></a>Numpy数组取值和赋值</h3><h4 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个如下格式的3x4数组</span></span><br><span class="line"><span class="comment"># [[ 1  2  3  4]</span></span><br><span class="line"><span class="comment">#  [ 5  6  7  8]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11 12]]</span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>], [<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在两个维度上分别按照[:2]和[1:3]进行切片，取需要的部分</span></span><br><span class="line"><span class="comment"># [[2 3]</span></span><br><span class="line"><span class="comment">#  [6 7]]</span></span><br><span class="line">b = a[:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><pre><code>[[2 3] [6 7]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建3x4的2维数组/矩阵</span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>], [<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><pre><code>[[ 1  2  3  4] [ 5  6  7  8] [ 9 10 11 12]]</code></pre><p>多维数组可以从各个维度同时切片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">row_r1 = a[<span class="number">1</span>, :]    <span class="comment"># 第2行，但是得到的是1维输出（列向量）</span></span><br><span class="line">row_r2 = a[<span class="number">1</span>:<span class="number">2</span>, :]  <span class="comment"># 1x2的2维输出</span></span><br><span class="line">row_r3 = a[[<span class="number">1</span>], :]  <span class="comment"># 同上</span></span><br><span class="line">print(row_r1, row_r1.shape) <span class="comment">#[5 6 7 8] (4,)</span></span><br><span class="line">print(row_r2, row_r2.shape) <span class="comment">#[[5 6 7 8]] (1, 4)</span></span><br><span class="line">print(row_r3, row_r3.shape) <span class="comment">#[[5 6 7 8]] (1, 4)</span></span><br></pre></td></tr></table></figure><pre><code>[5 6 7 8] (4,)[[5 6 7 8]] (1, 4)[[5 6 7 8]] (1, 4)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 试试在第2个维度上切片也一样的:</span></span><br><span class="line">col_r1 = a[:, <span class="number">1</span>]</span><br><span class="line">col_r2 = a[:, <span class="number">1</span>:<span class="number">2</span>]</span><br><span class="line">print(col_r1, col_r1.shape) <span class="comment">#[ 2  6 10] (3,)</span></span><br><span class="line">print()</span><br><span class="line">print(col_r2, col_r2.shape)</span><br></pre></td></tr></table></figure><pre><code>[ 2  6 10] (3,)[[ 2] [ 6] [10]] (3, 1)</code></pre><h4 id="自由地取值和组合"><a href="#自由地取值和组合" class="headerlink" title="自由地取值和组合"></a>自由地取值和组合</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其实意思就是取(0,0),(1,1),(2,0)的元素组起来</span></span><br><span class="line">print(a[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面这个比较直白啦</span></span><br><span class="line">print(np.array([a[<span class="number">0</span>, <span class="number">0</span>], a[<span class="number">1</span>, <span class="number">1</span>], a[<span class="number">2</span>, <span class="number">0</span>]]))</span><br></pre></td></tr></table></figure><pre><code>[1 4 5][1 4 5]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 再来熟悉一下</span></span><br><span class="line"><span class="comment"># 先创建一个2维数组</span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><pre><code>[[ 1  2  3] [ 4  5  6] [ 7  8  9] [10 11 12]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用下标生成一个向量</span></span><br><span class="line">b = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">print(a[np.arange(<span class="number">4</span>), b])  <span class="comment"># Print "[ 1  6  7 11]"</span></span><br></pre></td></tr></table></figure><pre><code>[ 1  6  7 11]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 既然可以取出来，我们当然可以对这些元素操作</span></span><br><span class="line">a[np.arange(<span class="number">4</span>), b] += <span class="number">10</span></span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><pre><code>[[11  2  3] [ 4  5 16] [17  8  9] [10 21 12]]</code></pre><h4 id="用条件判定去取值"><a href="#用条件判定去取值" class="headerlink" title="用条件判定去取值"></a>用条件判定去取值</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">bool_idx = (a &gt; <span class="number">2</span>)  <span class="comment"># 就是判定一下是否大于2</span></span><br><span class="line"></span><br><span class="line">print(bool_idx)  <span class="comment"># 返回一个布尔型的3x2数组</span></span><br></pre></td></tr></table></figure><pre><code>[[False False] [ True  True] [ True  True]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用刚才的布尔型数组作为下标就可以去除符合条件的元素啦</span></span><br><span class="line">print(a[bool_idx])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其实一句话也可以完成是不是？</span></span><br><span class="line">print(a[a &gt; <span class="number">2</span>])</span><br></pre></td></tr></table></figure><pre><code>[3 4 5 6][3 4 5 6]</code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="http://old.sebug.net/paper/books/scipydoc/_images/numpy_intro_02.png" alt=""><br><img src="http://old.sebug.net/paper/books/scipydoc/_images/numpy_intro_03.png" alt=""></p><h3 id="Numpy的基本数学运算"><a href="#Numpy的基本数学运算" class="headerlink" title="Numpy的基本数学运算"></a>Numpy的基本数学运算</h3><h4 id="逐元素运算"><a href="#逐元素运算" class="headerlink" title="逐元素运算"></a>逐元素运算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"><span class="comment"># 逐元素求和</span></span><br><span class="line">print(x + y) <span class="comment">#直接向量相加</span></span><br><span class="line">print(np.add(x, y)) <span class="comment">#调用函数</span></span><br><span class="line"><span class="comment"># 逐元素作差</span></span><br><span class="line">print(x - y) <span class="comment">#直接向量相减</span></span><br><span class="line">print(np.subtract(x, y)) <span class="comment">#调用函数</span></span><br><span class="line"><span class="comment"># 逐元素相乘</span></span><br><span class="line">print(x * y)</span><br><span class="line">print(np.multiply(x, y))</span><br><span class="line"><span class="comment"># 逐元素相除</span></span><br><span class="line">print(x / y)</span><br><span class="line">print(np.divide(x, y))</span><br><span class="line"><span class="comment"># 逐元素求平方根</span></span><br><span class="line">print(np.sqrt(x))</span><br></pre></td></tr></table></figure><pre><code>[[  6.   8.] [ 10.  12.]][[  6.   8.] [ 10.  12.]][[-4. -4.] [-4. -4.]][[-4. -4.] [-4. -4.]][[  5.  12.] [ 21.  32.]][[  5.  12.] [ 21.  32.]][[ 0.2         0.33333333] [ 0.42857143  0.5       ]][[ 0.2         0.33333333] [ 0.42857143  0.5       ]][[ 1.          1.41421356] [ 1.73205081  2.        ]]</code></pre><h4 id="整体运算"><a href="#整体运算" class="headerlink" title="整体运算"></a>整体运算</h4><p>NumPy计算乘积的函数:dot,inner,outer</p><ul><li><strong>dot</strong> : 对于两个一维的数组，计算的是这两个数组对应下标元素的乘积和(数学上称之为内积)；对于二维数组，计算的是两个数组的矩阵乘积；对于多维数组，它的通用计算公式如下，即结果数组中的每个元素都是：数组a的最后一维上的所有元素与数组b的倒数第二位上的所有元素的乘积和</li><li><strong>inner</strong> : 和dot乘积一样，对于两个一维数组，计算的是这两个数组对应下标元素的乘积和；对于多维数组，它计算的结果数组中的每个元素都是：数组a和b的最后一维的内积，因此数组a和b的最后一维的长度必须相同</li><li><strong>outer</strong> : 只按照一维数组进行计算，如果传入参数是多维数组，则先将此数组展平为一维数组之后再进行运算。outer乘积计算的列向量和行向量的矩阵乘积：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a = np.arange(<span class="number">12</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">b = np.arange(<span class="number">12</span>,<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">print(c.shape) <span class="comment"># (2, 3, 2, 3)</span></span><br><span class="line">print(np.alltrue( c[<span class="number">0</span>,:,<span class="number">0</span>,:] == np.dot(a[<span class="number">0</span>],b[<span class="number">0</span>]) )) <span class="comment"># True</span></span><br><span class="line">print(np.alltrue( c[<span class="number">1</span>,:,<span class="number">0</span>,:] == np.dot(a[<span class="number">1</span>],b[<span class="number">0</span>]) )) <span class="comment"># True</span></span><br><span class="line">print(np.alltrue( c[<span class="number">0</span>,:,<span class="number">1</span>,:] == np.dot(a[<span class="number">0</span>],b[<span class="number">1</span>]) )) <span class="comment"># True</span></span><br><span class="line">print(np.alltrue( c[<span class="number">1</span>,:,<span class="number">1</span>,:] == np.dot(a[<span class="number">1</span>],b[<span class="number">1</span>]) )) <span class="comment"># True</span></span><br><span class="line">a = np.arange(<span class="number">12</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">b = np.arange(<span class="number">12</span>,<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">d = np.inner(a,b)</span><br><span class="line">print(d.shape) <span class="comment"># (2, 3, 2, 3)</span></span><br><span class="line">print(d[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>] == np.inner(a[<span class="number">0</span>,<span class="number">0</span>],b[<span class="number">0</span>,<span class="number">0</span>])) <span class="comment"># True</span></span><br><span class="line">print(d[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>] == np.inner(a[<span class="number">0</span>,<span class="number">1</span>],b[<span class="number">1</span>,<span class="number">0</span>])) <span class="comment"># True</span></span><br><span class="line">print(d[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>] == np.inner(a[<span class="number">1</span>,<span class="number">2</span>],b[<span class="number">1</span>,<span class="number">2</span>])) <span class="comment"># True</span></span><br><span class="line">print(np.outer([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]))</span><br></pre></td></tr></table></figure><pre><code>(2, 3, 2, 3)TrueTrueTrueTrue(2, 3, 2, 3)TrueTrueTrue[[ 4  5  6  7] [ 8 10 12 14] [12 15 18 21]]</code></pre><h5 id="求向量内积"><a href="#求向量内积" class="headerlink" title="求向量内积"></a>求向量内积</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = np.array([<span class="number">9</span>,<span class="number">10</span>])</span><br><span class="line">w = np.array([<span class="number">11</span>, <span class="number">12</span>])</span><br><span class="line">print(v.dot(w))</span><br><span class="line">print(np.dot(v, w))</span><br></pre></td></tr></table></figure><pre><code>219219</code></pre><h5 id="矩阵的乘法"><a href="#矩阵的乘法" class="headerlink" title="矩阵的乘法"></a>矩阵的乘法</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = np.array([<span class="number">9</span>,<span class="number">10</span>])</span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">print(np.dot(x, v))</span><br><span class="line">print(np.matmul(x,v))</span><br></pre></td></tr></table></figure><pre><code>[ 29.  67.][ 29.  67.][ 29.  67.]</code></pre><h5 id="转置"><a href="#转置" class="headerlink" title="转置"></a>转置</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">print(x)</span><br><span class="line">print(x.T)</span><br><span class="line">v = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">print(v)</span><br><span class="line">print(v.T) <span class="comment">#一维数组转置不变</span></span><br><span class="line">w = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">print(w)</span><br><span class="line">print(w.T)</span><br></pre></td></tr></table></figure><pre><code>[[ 1.  2.] [ 3.  4.]][[ 1.  3.] [ 2.  4.]][1 2 3][1 2 3][[1 2 3]][[1] [2] [3]]</code></pre><h5 id="高维tensor转置"><a href="#高维tensor转置" class="headerlink" title="高维tensor转置"></a>高维tensor转置</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">16</span>).reshape((<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">print(arr)</span><br></pre></td></tr></table></figure><pre><code>[[[ 0  1  2  3]  [ 4  5  6  7]] [[ 8  9 10 11]  [12 13 14 15]]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(arr.transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>)))</span><br></pre></td></tr></table></figure><pre><code>[[[ 0  1  2  3]  [ 8  9 10 11]] [[ 4  5  6  7]  [12 13 14 15]]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(arr.swapaxes(<span class="number">1</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure><pre><code>[[[ 0  4]  [ 1  5]  [ 2  6]  [ 3  7]] [[ 8 12]  [ 9 13]  [10 14]  [11 15]]]</code></pre><h3 id="Numpy的统计特性"><a href="#Numpy的统计特性" class="headerlink" title="Numpy的统计特性"></a>Numpy的统计特性</h3><ul><li>np.sum()，返回求和</li><li>np.mean()，返回均值</li><li>np.max()，返回最大值</li><li>np.min()，返回最小值</li><li>np.ptp()，数组沿指定轴返回最大值减去最小值，即（max-min）</li><li>np.std()，返回标准偏差（standard deviation）</li><li>np.var()，返回方差（variance）</li><li>np.cumsum()，返回累加值</li><li>np.cumprod()，返回累乘积值</li></ul><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">print(np.sum(x))          <span class="comment"># 数组/矩阵中所有元素求和; prints "10"</span></span><br><span class="line">print(np.sum(x, axis=<span class="number">0</span>))  <span class="comment"># 按行去求和; prints "[4 6]"</span></span><br><span class="line">print(np.sum(x, axis=<span class="number">1</span>))  <span class="comment"># 按列去求和; prints "[3 7]"</span></span><br><span class="line">print(np.mean(x))         <span class="comment"># 数组/矩阵中所有元素求均值; prints "2.5"</span></span><br><span class="line">print(np.mean(x, axis=<span class="number">0</span>)) <span class="comment"># 按行去求均值; prints "[ 2.  3.]"</span></span><br><span class="line">print(np.mean(x, axis=<span class="number">1</span>)) <span class="comment"># 按列去求均值; prints "[ 1.5  3.5]"</span></span><br><span class="line">print(np.max(x))          <span class="comment"># 数组/矩阵中所有元素求最大值; prints "4"</span></span><br><span class="line">print(np.min(x))          <span class="comment"># 数组/矩阵中所有元素求最小值; prints "1"</span></span><br><span class="line">print(np.std(x,axis=<span class="number">0</span>))   <span class="comment">#按行去求标准差; prints "[ 1.  1.]"</span></span><br><span class="line">print(np.var(x,axis=<span class="number">1</span>))   <span class="comment">#按列去求方差; prints "[ 0.25  0.25]"</span></span><br><span class="line">print(x.cumsum(axis=<span class="number">0</span>))   <span class="comment"># 按行去累加; prints "[[1 2][4 6]]"</span></span><br><span class="line">print(x.cumprod(axis=<span class="number">1</span>))  <span class="comment"># 按列去累乘;prints "[[ 1  2][ 3 12]]"</span></span><br></pre></td></tr></table></figure><pre><code>[[1 2] [3 4]]10[4 6][3 7]2.5[ 2.  3.][ 1.5  3.5]41[ 1.  1.][ 0.25  0.25][[1 2] [4 6]][[ 1  2] [ 3 12]]</code></pre><h3 id="Numpy数组排序"><a href="#Numpy数组排序" class="headerlink" title="Numpy数组排序"></a>Numpy数组排序</h3><h4 id="一维数组的排序"><a href="#一维数组的排序" class="headerlink" title="一维数组的排序"></a>一维数组的排序</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr = np.random.randn(<span class="number">8</span>)</span><br><span class="line">print(arr)</span><br><span class="line">arr.sort()</span><br><span class="line">print(arr)</span><br></pre></td></tr></table></figure><pre><code>[ 0.70150419 -0.88493701  0.37449618 -0.42676191  1.52654468 -1.79515205  0.05635219  0.80712566][-1.79515205 -0.88493701 -0.42676191  0.05635219  0.37449618  0.70150419  0.80712566  1.52654468]</code></pre><h4 id="二维数组也可以在某些维度上排序"><a href="#二维数组也可以在某些维度上排序" class="headerlink" title="二维数组也可以在某些维度上排序"></a>二维数组也可以在某些维度上排序</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">print(arr)</span><br><span class="line">arr.sort(<span class="number">1</span>)</span><br><span class="line">print(arr)</span><br></pre></td></tr></table></figure><pre><code>[[-0.51135747 -0.0355637   0.38398028] [-1.44309081  1.31425286  0.16295143] [-0.54112556 -1.07293118  0.55690543] [ 0.55382507  0.79843566 -1.29064181] [ 0.69978121  0.24467205  0.13107927]][[-0.51135747 -0.0355637   0.38398028] [-1.44309081  0.16295143  1.31425286] [-1.07293118 -0.54112556  0.55690543] [-1.29064181  0.55382507  0.79843566] [ 0.13107927  0.24467205  0.69978121]]</code></pre><p>找出排序后位置在5%的数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">large_arr = np.random.randn(<span class="number">1000</span>)</span><br><span class="line">large_arr.sort()</span><br><span class="line">print(large_arr[int(<span class="number">0.05</span>*len(large_arr))])</span><br></pre></td></tr></table></figure><pre><code>-1.65535730932</code></pre><h3 id="Broadcasting（广播）"><a href="#Broadcasting（广播）" class="headerlink" title="Broadcasting（广播）"></a>Broadcasting（广播）</h3><p>应用场景：要用小的矩阵去和大的矩阵做一些操作，但是希望小矩阵能<strong>循环</strong>和大矩阵的那些块做一样的操作。</p><h4 id="举例-1"><a href="#举例-1" class="headerlink" title="举例"></a>举例</h4><ul><li>一个矩阵的每一行都加上一个向量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">v = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">print(x + v)</span><br></pre></td></tr></table></figure><pre><code>[[2 4 6] [5 7 9]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]) <span class="comment"># 2x3的</span></span><br><span class="line">w = np.array([<span class="number">4</span>,<span class="number">5</span>])    <span class="comment"># w 形状是 (2,)</span></span><br><span class="line"></span><br><span class="line">print((x.T + w).T) <span class="comment">#通过转置完成广播运算</span></span><br></pre></td></tr></table></figure><pre><code>[[ 5  6  7] [ 9 10 11]]</code></pre><ul><li>逐元素运算</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">print(x * <span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>[[ 2  4  6] [ 8 10 12]]</code></pre><h4 id="可广播条件"><a href="#可广播条件" class="headerlink" title="可广播条件"></a>可广播条件</h4><ul><li>数组拥有相同形状。</li><li>数组拥有相同的维数，每个维度拥有相同长度，或者长度为 1。</li><li>数组拥有极少的维度，可以在其前面追加长度为 1 的维度，使上述条件成立。</li></ul><h4 id="广播规则"><a href="#广播规则" class="headerlink" title="广播规则"></a>广播规则</h4><ul><li>让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分都通过在前面加1补齐</li><li>输出数组的shape是输入数组shape的各个轴上的最大值</li><li>如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为1时，这个数组能够用来计算，否则出错</li><li>当输入数组的某个轴的长度为1时，沿着此轴运算时都用此轴上的第一组值</li></ul><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p><img src="http://xukeqiniu.xukeai.cn/84505c47106fab5f35883fb849e69f2e.png" alt=""></p><h3 id="一些更高级的ndarray处理"><a href="#一些更高级的ndarray处理" class="headerlink" title="一些更高级的ndarray处理"></a>一些更高级的ndarray处理</h3><h4 id="where和一些其他的逻辑运算"><a href="#where和一些其他的逻辑运算" class="headerlink" title="where和一些其他的逻辑运算"></a>where和一些其他的逻辑运算</h4><p>np.where(cond,x,y)：满足条件（cond）输出x，不满足输出y</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_arr = np.array([<span class="number">1.1</span>, <span class="number">1.2</span>, <span class="number">1.3</span>, <span class="number">1.4</span>, <span class="number">1.5</span>])</span><br><span class="line">y_arr = np.array([<span class="number">2.1</span>, <span class="number">2.2</span>, <span class="number">2.3</span>, <span class="number">2.4</span>, <span class="number">2.5</span>])</span><br><span class="line">cond = np.array([<span class="keyword">True</span>, <span class="keyword">False</span>, <span class="keyword">True</span>, <span class="keyword">True</span>, <span class="keyword">False</span>])</span><br><span class="line">print(np.where(cond, x_arr, y_arr))</span><br></pre></td></tr></table></figure><pre><code>[ 1.1  2.2  1.3  1.4  2.5]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr = np.random.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">print(arr)</span><br><span class="line">print(np.where(arr &gt; <span class="number">0</span>, <span class="number">2</span>, <span class="number">-2</span>))</span><br><span class="line">print(np.where(arr &gt; <span class="number">0</span>, <span class="number">2</span>, arr))</span><br></pre></td></tr></table></figure><pre><code>[[ -1.10484247e+00  -3.82422727e-01  -3.24361549e-01   1.21286234e+00] [  1.54499855e-01  -4.77728163e-04   1.44621074e+00  -2.64241611e-03] [  1.36394862e+00   6.96638259e-02  -2.75237740e-01  -3.32892881e-01] [ -1.37165175e+00   1.79997993e-01  -1.13509664e-01   1.88373639e+00]][[-2 -2 -2  2] [ 2 -2  2 -2] [ 2  2 -2 -2] [-2  2 -2  2]][[ -1.10484247e+00  -3.82422727e-01  -3.24361549e-01   2.00000000e+00] [  2.00000000e+00  -4.77728163e-04   2.00000000e+00  -2.64241611e-03] [  2.00000000e+00   2.00000000e+00  -2.75237740e-01  -3.32892881e-01] [ -1.37165175e+00   2.00000000e+00  -1.13509664e-01   2.00000000e+00]]</code></pre><p>np.where可以嵌套使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cond_1 = np.array([<span class="keyword">True</span>, <span class="keyword">False</span>, <span class="keyword">True</span>, <span class="keyword">True</span>, <span class="keyword">False</span>])</span><br><span class="line">cond_2 = np.array([<span class="keyword">False</span>, <span class="keyword">True</span>, <span class="keyword">False</span>, <span class="keyword">True</span>, <span class="keyword">False</span>])</span><br><span class="line">result = np.where(cond_1 &amp; cond_2, <span class="number">0</span>, \</span><br><span class="line">          np.where(cond_1, <span class="number">1</span>, np.where(cond_2, <span class="number">2</span>, <span class="number">3</span>)))</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><pre><code>[1 2 1 0 3]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr = np.random.randn(<span class="number">10</span>)</span><br><span class="line">print(arr)</span><br><span class="line">print((arr &gt; <span class="number">0</span>).sum()) <span class="comment">#数组中大于0的数相加</span></span><br></pre></td></tr></table></figure><pre><code>[ 0.27350655 -1.51093462  0.26835915 -0.45991855  1.34450904 -1.86871203  0.04308971  1.69640444 -0.02191351 -0.43875275]5</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bools = np.array([<span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">True</span>, <span class="keyword">False</span>])</span><br><span class="line">print(bools.any()) <span class="comment"># 有一个为True则返回True</span></span><br><span class="line">print(bools.all()) <span class="comment"># 有一个为False则返回False</span></span><br></pre></td></tr></table></figure><pre><code>TrueFalse</code></pre><h4 id="reshape（数组变形）"><a href="#reshape（数组变形）" class="headerlink" title="reshape（数组变形）"></a>reshape（数组变形）</h4><p>numpy可以很容易地把一维数组转成二维数组，三维数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">arr = np.arange(<span class="number">8</span>)</span><br><span class="line">print(<span class="string">"(4,2):\n"</span>, arr.reshape((<span class="number">4</span>,<span class="number">2</span>)))</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"(2,2,2):\n"</span>, arr.reshape((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)))</span><br></pre></td></tr></table></figure><pre><code>(4,2): [[0 1] [2 3] [4 5] [6 7]](2,2,2): [[[0 1]  [2 3]] [[4 5]  [6 7]]]</code></pre><h4 id="1（-维度自动推算）"><a href="#1（-维度自动推算）" class="headerlink" title="-1（ 维度自动推算）"></a>-1（ 维度自动推算）</h4><p>如果我们在某一个维度上写上-1，numpy会帮我们自动推导出正确的维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">15</span>)</span><br><span class="line">print(arr.reshape((<span class="number">5</span>,<span class="number">-1</span>)))</span><br><span class="line">print(arr.reshape((<span class="number">5</span>,<span class="number">-1</span>)).shape)</span><br></pre></td></tr></table></figure><pre><code>[[ 0  1  2] [ 3  4  5] [ 6  7  8] [ 9 10 11] [12 13 14]](5, 3)</code></pre><h4 id="ravel（拉平数组）"><a href="#ravel（拉平数组）" class="headerlink" title="ravel（拉平数组）"></a>ravel（拉平数组）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 高维数组用ravel来拉平成为一维数组</span></span><br><span class="line">arr = np.arange(<span class="number">15</span>)</span><br><span class="line">print(arr.ravel())</span><br></pre></td></tr></table></figure><pre><code>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]</code></pre><h4 id="concatenate（连接数组）"><a href="#concatenate（连接数组）" class="headerlink" title="concatenate（连接数组）"></a>concatenate（连接数组）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr1 = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">arr2 = np.array([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">print(np.concatenate([arr1, arr2], axis = <span class="number">0</span>))  <span class="comment"># 按行连接</span></span><br><span class="line">print(np.concatenate([arr1, arr2], axis = <span class="number">1</span>))  <span class="comment"># 按列连接</span></span><br></pre></td></tr></table></figure><pre><code>[[ 1  2  3] [ 4  5  6] [ 7  8  9] [10 11 12]][[ 1  2  3  7  8  9] [ 4  5  6 10 11 12]]</code></pre><p>连接的另一种表述垂直<code>stack</code>与水平<code>stack</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(np.vstack((arr1, arr2))) <span class="comment"># 垂直堆叠</span></span><br><span class="line">print(np.hstack((arr1, arr2))) <span class="comment"># 水平堆叠</span></span><br></pre></td></tr></table></figure><pre><code>[[ 1  2  3] [ 4  5  6] [ 7  8  9] [10 11 12]][[ 1  2  3  7  8  9] [ 4  5  6 10 11 12]]</code></pre><h4 id="split（拆分数组）"><a href="#split（拆分数组）" class="headerlink" title="split（拆分数组）"></a>split（拆分数组）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.random.rand(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">print(arr)</span><br></pre></td></tr></table></figure><pre><code>[[ 0.08218151  0.25291976  0.990262    0.74980044  0.92433676] [ 0.57215647  0.88759783  0.67939949  0.18618301  0.64810013] [ 0.21424794  0.5812622   0.33170632  0.40780156  0.00946797] [ 0.46223634  0.53574553  0.25289433  0.33226224  0.26110024] [ 0.81823359  0.98863697  0.13713923  0.3520669   0.38301044]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">first, second, third = np.split(arr, [<span class="number">1</span>,<span class="number">3</span>], axis = <span class="number">0</span>) <span class="comment"># 按行拆分</span></span><br><span class="line">print(first)</span><br><span class="line">print()</span><br><span class="line">print(second)</span><br><span class="line">print()</span><br><span class="line">print(third)</span><br></pre></td></tr></table></figure><pre><code>[[ 0.08218151  0.25291976  0.990262    0.74980044  0.92433676]][[ 0.57215647  0.88759783  0.67939949  0.18618301  0.64810013] [ 0.21424794  0.5812622   0.33170632  0.40780156  0.00946797]][[ 0.46223634  0.53574553  0.25289433  0.33226224  0.26110024] [ 0.81823359  0.98863697  0.13713923  0.3520669   0.38301044]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">first, second, third = np.split(arr, [<span class="number">1</span>, <span class="number">3</span>], axis = <span class="number">1</span>) <span class="comment"># 按列拆分</span></span><br><span class="line">print(first)</span><br><span class="line">print()</span><br><span class="line">print(second)</span><br><span class="line">print()</span><br><span class="line">print(third)</span><br></pre></td></tr></table></figure><pre><code>[[ 0.08218151] [ 0.57215647] [ 0.21424794] [ 0.46223634] [ 0.81823359]][[ 0.25291976  0.990262  ] [ 0.88759783  0.67939949] [ 0.5812622   0.33170632] [ 0.53574553  0.25289433] [ 0.98863697  0.13713923]][[ 0.74980044  0.92433676] [ 0.18618301  0.64810013] [ 0.40780156  0.00946797] [ 0.33226224  0.26110024] [ 0.3520669   0.38301044]]</code></pre><h5 id="堆叠辅助"><a href="#堆叠辅助" class="headerlink" title="堆叠辅助"></a>堆叠辅助</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">6</span>)</span><br><span class="line">arr1 = arr.reshape((<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">arr2 = np.random.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment">#r_用于按行堆叠</span></span><br><span class="line">print(np.r_[arr1, arr2])</span><br><span class="line">print()</span><br><span class="line"><span class="comment">#c_用于按列堆叠</span></span><br><span class="line">print(np.c_[np.r_[arr1, arr2], arr])</span><br><span class="line">print()</span><br><span class="line"><span class="comment">#切片直接转为数组</span></span><br><span class="line">print(np.c_[<span class="number">1</span>:<span class="number">6</span>, <span class="number">-10</span>:<span class="number">-5</span>])</span><br><span class="line">print()</span><br></pre></td></tr></table></figure><pre><code>[[ 0.          1.        ] [ 2.          3.        ] [ 4.          5.        ] [ 0.04811148 -1.93674347] [ 1.19646481  0.17346639] [-1.4388562  -1.41584843]][[ 0.          1.          0.        ] [ 2.          3.          1.        ] [ 4.          5.          2.        ] [ 0.04811148 -1.93674347  3.        ] [ 1.19646481  0.17346639  4.        ] [-1.4388562  -1.41584843  5.        ]][[  1 -10] [  2  -9] [  3  -8] [  4  -7] [  5  -6]]</code></pre><h4 id="repeat（数组重复）"><a href="#repeat（数组重复）" class="headerlink" title="repeat（数组重复）"></a>repeat（数组重复）</h4><p>repeat(a,repeats, axis=None)</p><ul><li>按元素重复</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">3</span>)</span><br><span class="line">print(arr.repeat(<span class="number">3</span>))</span><br><span class="line">print(arr.repeat([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]))</span><br><span class="line">print()</span><br></pre></td></tr></table></figure><pre><code>[0 0 0 1 1 1 2 2 2][0 0 1 1 1 2 2 2 2]</code></pre><ul><li>指定axis来重复</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">4</span>)</span><br><span class="line">print(arr)</span><br></pre></td></tr></table></figure><pre><code>[[ 0.468845    0.43227877] [ 0.13822954  0.14501615]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(arr.repeat(<span class="number">2</span>, axis=<span class="number">0</span>))</span><br><span class="line">print(arr.repeat(<span class="number">2</span>, axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>[[ 0.468845    0.43227877] [ 0.468845    0.43227877] [ 0.13822954  0.14501615] [ 0.13822954  0.14501615]][[ 0.468845    0.468845    0.43227877  0.43227877] [ 0.13822954  0.13822954  0.14501615  0.14501615]]</code></pre><h4 id="tile-按规则重复数组"><a href="#tile-按规则重复数组" class="headerlink" title="tile(按规则重复数组)"></a>tile(按规则重复数组)</h4><p>tile通过重复给定的次数来构造数组。tile(A, reps)：初始数组是A，重复规则是reps。reps表示数组A需要重复的次数、结果的行数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">4</span>).reshape((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">print(np.tile(arr, <span class="number">2</span>))</span><br><span class="line">print(np.tile(arr, (<span class="number">2</span>,<span class="number">3</span>)))</span><br></pre></td></tr></table></figure><pre><code>[[0 1 0 1] [2 3 2 3]][[0 1 0 1 0 1] [2 3 2 3 2 3] [0 1 0 1 0 1] [2 3 2 3 2 3]]</code></pre><h3 id="numpy的文件输入输出"><a href="#numpy的文件输入输出" class="headerlink" title="numpy的文件输入输出"></a>numpy的文件输入输出</h3><h4 id="读取csv文件作为数组"><a href="#读取csv文件作为数组" class="headerlink" title="读取csv文件作为数组"></a>读取csv文件作为数组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">arr = np.loadtxt(<span class="string">'array_ex.txt'</span>, delimiter=<span class="string">','</span>)</span><br><span class="line">print(arr)</span><br></pre></td></tr></table></figure><pre><code>[[ 0.580052  0.18673   1.040717  1.134411] [ 0.194163 -0.636917 -0.938659  0.124094] [-0.12641   0.268607 -0.695724  0.047428] [-1.484413  0.004176 -0.744203  0.005487] [ 2.302869  0.200131  1.670238 -1.88109 ] [-0.19323   1.047233  0.482803  0.960334]]</code></pre><h4 id="数组文件读写"><a href="#数组文件读写" class="headerlink" title="数组文件读写"></a>数组文件读写</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr = np.arange(<span class="number">10</span>)</span><br><span class="line">np.save(<span class="string">'some_array'</span>, arr)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(np.load(<span class="string">'some_array.npy'</span>))</span><br></pre></td></tr></table></figure><pre><code>[0 1 2 3 4 5 6 7 8 9]</code></pre><h4 id="多个数组可以一起压缩存储"><a href="#多个数组可以一起压缩存储" class="headerlink" title="多个数组可以一起压缩存储"></a>多个数组可以一起压缩存储</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">arr2 = np.arange(<span class="number">15</span>).reshape(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line">np.savez(<span class="string">'array_archive.npz'</span>, a=arr, b=arr2)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arch = np.load(<span class="string">'array_archive.npz'</span>)</span><br><span class="line">print(arch[<span class="string">'a'</span>])</span><br><span class="line">print(arch[<span class="string">'b'</span>])</span><br></pre></td></tr></table></figure><pre><code>[0 1 2 3 4 5 6 7 8 9][[ 0  1  2  3  4] [ 5  6  7  8  9] [10 11 12 13 14]]</code></pre><h3 id="用numpy写一个softmax"><a href="#用numpy写一个softmax" class="headerlink" title="用numpy写一个softmax"></a>用numpy写一个softmax</h3><p>步骤：</p><ul><li>数据预处理</li><li>计算exponential</li><li>每行求和</li><li>每一行除以计算的和</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 产生（10，10）随机数</span></span><br><span class="line">m = np.random.rand(<span class="number">10</span>, <span class="number">10</span>) * <span class="number">10</span> + <span class="number">1000</span></span><br><span class="line">print(m)</span><br></pre></td></tr></table></figure><pre><code>[[ 1002.4195769   1000.59428635  1004.19947044  1009.17641327   1004.89329928  1001.02496808  1007.79619575  1005.61568017   1009.28511386  1000.11608716] [ 1002.9870141   1005.59523328  1001.99337934  1008.79319814   1004.78921679  1003.91814186  1009.38777432  1005.20436416   1009.27099589  1008.69823987] [ 1006.68713949  1009.02893339  1008.2656608   1002.27620211  1009.2256124   1004.14144532  1007.09728075  1006.21626467  1004.60860132   1004.51547132] [ 1005.57757481  1001.6026775   1004.79229078  1004.28025577   1008.68219699  1005.6379599   1008.07958879  1006.35060616   1009.03418483  1003.50279599] [ 1003.22924339  1006.62272977  1008.5591972   1009.72498967   1004.49414198  1004.21450523  1008.32652935  1000.90418303   1009.24606203  1001.27113066] [ 1006.84865072  1005.24619541  1000.04356362  1003.38870582   1008.59759772  1008.80052236  1007.92905671  1006.16987466  1002.3761379   1001.55941284] [ 1006.80724007  1004.46597582  1003.25453387  1008.55713243   1009.19618236  1002.06897172  1004.69874948  1006.51535711   1005.23735087  1006.85265988] [ 1002.22993628  1000.59475018  1007.52711923  1000.36311206   1008.22254861  1003.94553055  1004.23517969  1005.26438502   1006.39421888  1005.22133756] [ 1006.92863693  1003.23688304  1007.11513614  1003.28880837   1009.11093137  1006.35136574  1002.04684923  1001.13114541   1008.50487627  1008.67481458] [ 1002.65347387  1001.90472796  1004.02149562  1009.63548587   1009.16220671  1006.39781332  1008.1526219   1003.57220839   1008.60930803  1004.41645034]]</code></pre><p>直接对m进行e指数运算会产生<strong>上溢</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(np.exp(m))</span><br></pre></td></tr></table></figure><pre><code>[[ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf] [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf] [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf] [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf] [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf] [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf] [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf] [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf] [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf] [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf]]G:\Anaconda3\lib\site-packages\ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp  &quot;&quot;&quot;Entry point for launching an IPython kernel.</code></pre><p>寻找每一行的最大值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按列取最大值（即取每一行的最大值）</span></span><br><span class="line">m_row_max = m.max(axis=<span class="number">1</span>).reshape(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">print(m_row_max, m_row_max.shape)</span><br></pre></td></tr></table></figure><pre><code>[[ 1009.28511386] [ 1009.38777432] [ 1009.2256124 ] [ 1009.03418483] [ 1009.72498967] [ 1008.80052236] [ 1009.19618236] [ 1008.22254861] [ 1009.11093137] [ 1009.63548587]] (10, 1)</code></pre><p>通过广播的方式将每行数据减去对应行的最大值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 采用广播的方式进行减法操作</span></span><br><span class="line">m = m - m_row_max</span><br><span class="line">print(m)</span><br></pre></td></tr></table></figure><pre><code>[[-6.86553696 -8.69082751 -5.08564343 -0.1087006  -4.39181458 -8.26014579  -1.48891811 -3.66943369  0.         -9.16902671] [-6.40076022 -3.79254104 -7.39439498 -0.59457618 -4.59855753 -5.46963247   0.         -4.18341016 -0.11677843 -0.68953445] [-2.5384729  -0.19667901 -0.95995159 -6.94941029  0.         -5.08416708  -2.12833165 -3.00934773 -4.61701107 -4.71014107] [-3.45661002 -7.43150733 -4.24189405 -4.75392907 -0.35198784 -3.39622493  -0.95459604 -2.68357867  0.         -5.53138884] [-6.49574628 -3.1022599  -1.16579247  0.         -5.23084769 -5.51048445  -1.39846033 -8.82080664 -0.47892764 -8.45385902] [-1.95187164 -3.55432696 -8.75695874 -5.41181655 -0.20292464  0.  -0.87146565 -2.63064771 -6.42438446 -7.24110952] [-2.3889423  -4.73020655 -5.94164849 -0.63904993  0.         -7.12721064  -4.49743288 -2.68082526 -3.95883149 -2.34352249] [-5.99261232 -7.62779843 -0.69542937 -7.85943655  0.         -4.27701805  -3.98736891 -2.95816359 -1.82832972 -3.00121104] [-2.18229443 -5.87404833 -1.99579523 -5.82212299  0.         -2.75956563  -7.06408214 -7.97978595 -0.6060551  -0.43611679] [-6.982012   -7.73075791 -5.61399025  0.         -0.47327916 -3.23767255  -1.48286397 -6.06327748 -1.02617783 -5.21903553]]</code></pre><p>求预处理后的e指数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#求预处理后的e指数</span></span><br><span class="line">m_exp = np.exp(m)</span><br><span class="line">print(m_exp, m_exp.shape)</span><br></pre></td></tr></table></figure><pre><code>[[  1.04312218e-03   1.68120847e-04   6.18490628e-03   8.96998943e-01    1.23782475e-02   2.58621284e-04   2.25616615e-01   2.54909015e-02    1.00000000e+00   1.04217895e-04] [  1.66029460e-03   2.25382585e-02   6.14688467e-04   5.51796380e-01    1.00663457e-02   4.21278021e-03   1.00000000e+00   1.52464260e-02    8.89782323e-01   5.01809632e-01] [  7.89869284e-02   8.21454272e-01   3.82911421e-01   9.59200640e-04    1.00000000e+00   6.19404411e-03   1.19035722e-01   4.93238409e-02    9.88228942e-03   9.00350735e-03] [  3.15364890e-02   5.92294057e-04   1.43803289e-02   8.61776882e-03    7.03288672e-01   3.34994945e-02   3.84967625e-01   6.83182276e-02    1.00000000e+00   3.96048477e-03] [  1.50984802e-03   4.49475108e-02   3.11675571e-01   1.00000000e+00    5.34898908e-03   4.04414773e-03   2.46976935e-01   1.47629228e-04    6.19447308e-01   2.13076561e-04] [  1.42008035e-01   2.86006179e-02   1.57362462e-04   4.46352464e-03    8.16339758e-01   1.00000000e+00   4.18337963e-01   7.20317916e-02    1.62153108e-03   7.16516327e-04] [  9.17266523e-02   8.82464816e-03   2.62769434e-03   5.27793627e-01    1.00000000e+00   8.02955997e-04   1.11375513e-02   6.85065952e-02    1.90854027e-02   9.59889224e-02] [  2.49713221e-03   4.86731255e-04   4.98860204e-01   3.86091355e-04    1.00000000e+00   1.38840018e-02   1.85484526e-02   5.19141655e-02    1.60681727e-01   4.97268106e-02] [  1.12782462e-01   2.81146852e-03   1.35905535e-01   2.96131163e-03    1.00000000e+00   6.33192663e-02   8.55279590e-04   3.42312686e-04    5.45498570e-01   6.46542214e-01] [  9.28433319e-04   4.39111184e-04   3.64648989e-03   1.00000000e+00    6.22956140e-01   3.92551533e-02   2.26986674e-01   2.32676246e-03    3.58374111e-01   5.41254683e-03]] (10, 10)</code></pre><p>将求指数后的数据按列加和（每行求和），然后将一维数据(10,)reshape成（10,1）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m_exp_row_sum = m_exp.sum(axis = <span class="number">1</span>).reshape(<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line">print(m_exp_row_sum, m_exp_row_sum.shape)</span><br></pre></td></tr></table></figure><pre><code>[[ 2.1682437 ] [ 2.99772713] [ 2.47775123] [ 2.24916138] [ 2.23431102] [ 2.4842771 ] [ 1.82649405] [ 1.79698532] [ 2.51101842] [ 2.26032542]] (10, 1)</code></pre><p>每行的数据除以对应行e指数求和</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m_softmax = m_exp / m_exp_row_sum</span><br><span class="line">print(m_softmax)</span><br></pre></td></tr></table></figure><pre><code>[[  4.81090841e-04   7.75378004e-05   2.85249591e-03   4.13698398e-01    5.70888203e-03   1.19276853e-04   1.04055008e-01   1.17564744e-02    4.61202771e-01   4.80655820e-05] [  5.53851145e-04   7.51844898e-03   2.05051507e-04   1.84071584e-01    3.35799265e-03   1.40532478e-03   3.33586066e-01   5.08599528e-03    2.96818985e-01   1.67396701e-01] [  3.18784741e-02   3.31532183e-01   1.54539898e-01   3.87125483e-04    4.03591769e-01   2.49986522e-03   4.80418376e-02   1.99066962e-02    3.98841067e-03   3.63374146e-03] [  1.40214434e-02   2.63339955e-04   6.39364033e-03   3.83154756e-03    3.12689288e-01   1.48942156e-02   1.71160517e-01   3.03749780e-02    4.44610159e-01   1.76087176e-03] [  6.75755530e-04   2.01169445e-02   1.39495159e-01   4.47565264e-01    2.39402171e-03   1.81002005e-03   1.10538297e-01   6.60737144e-05    2.77243098e-01   9.53656673e-05] [  5.71627193e-02   1.15126521e-02   6.33433613e-05   1.79670965e-03    3.28602537e-01   4.02531586e-01   1.68394243e-01   2.89950713e-02    6.52717479e-04   2.88420453e-04] [  5.02200663e-02   4.83146833e-03   1.43865475e-03   2.88965424e-01    5.47496993e-01   4.39615994e-04   6.09777585e-03   3.75071549e-02    1.04492006e-02   5.25536464e-02] [  1.38962304e-03   2.70859896e-04   2.77609505e-01   2.14855041e-04    5.56487574e-01   7.72627449e-03   1.03219834e-02   2.88895880e-02    8.94173844e-02   2.76723522e-02] [  4.49150276e-02   1.11965269e-03   5.41236712e-02   1.17932692e-03    3.98244789e-01   2.52165679e-02   3.40610640e-04   1.36324243e-04    2.17241963e-01   2.57482067e-01] [  4.10752058e-04   1.94269011e-04   1.61325881e-03   4.42414172e-01    2.75604625e-01   1.73670361e-02   1.00422121e-01   1.02939269e-03    1.58549786e-01   2.39458743e-03]]</code></pre><p>验证一下，对输出值进行按列求和，每行结果应该均为1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(m_softmax.sum(axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://docs.scipy.org/doc/numpy/reference/" target="_blank" rel="noopener">numpy指南</a></p><p><a href="https://danzhuibing.github.io/py_numpy_ndarray.html" target="_blank" rel="noopener">numpy ndarray详解</a></p><p><a href="http://old.sebug.net/paper/books/scipydoc/numpy_intro.html" target="_blank" rel="noopener">NumPy-快速处理数据</a></p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> NumPy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python基础知识</title>
      <link href="/2018/01/01/Python/00-Python%E5%9F%BA%E7%A1%80/"/>
      <url>/2018/01/01/Python/00-Python%E5%9F%BA%E7%A1%80/</url>
      <content type="html"><![CDATA[<h2 id="Python基础知识"><a href="#Python基础知识" class="headerlink" title="Python基础知识"></a>Python基础知识</h2><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>计算机顾名思义就是可以做数学计算的机器，因此，计算机程序理所当然地可以处理各种数值。但是，计算机能处理的远不止数值，还可以处理文本、图形、音频、视频、网页等各种各样的数据，不同的数据，需要定义不同的数据类型。在Python中，能够直接处理的数据类型有以下几种：</p><ul><li>整数</li></ul><p>Python可以处理任意大小的整数，当然包括负整数，在程序中的表示方法和数学上的写法一模一样，例如：1，100，-8080，0，等等。</p><p>计算机由于使用二进制，所以，有时候用十六进制表示整数比较方便，十六进制用0x前缀和0-9，a-f表示，例如：0xff00，0xa5b4c3d2，等等。</p><ul><li>浮点数</li></ul><p>浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，比如，$1.23 \times 10^9$和$12.3 \times 10^8$是完全相等的。浮点数可以用数学写法，如1.23，3.14，-9.01，等等。但是对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，$1.23 \times 10^9$就是$1.23\times e^9$，或者$12.3\times e^8$，0.000012可以写成$1.2\times e^{-5}$，等等。</p><p>整数和浮点数在计算机内部存储的方式是不同的，整数运算永远是精确的（除法难道也是精确的？是的！），而浮点数运算则可能会有四舍五入的误差。</p><ul><li>字符串</li></ul><p>字符串是以单引号’或双引号”括起来的任意文本，比如’abc’，”xyz”等等。请注意，’’或””本身只是一种表示方式，不是字符串的一部分，因此，字符串’abc’只有a，b，c这3个字符。如果’本身也是一个字符，那就可以用””括起来，比如”I’m OK”包含的字符是I，’，m，空格，O，K这6个字符。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">a =<span class="number">3</span></span><br><span class="line">b =<span class="number">4</span></span><br><span class="line">c = <span class="number">1.09</span></span><br><span class="line">d = <span class="number">2.35</span></span><br><span class="line">e = complex(c,d)</span><br><span class="line">f = complex(float(a),float(b))</span><br><span class="line">g = <span class="string">"hello python"</span></span><br><span class="line">print(<span class="string">"a is type "</span>,type(a))</span><br><span class="line">print(<span class="string">"c is type "</span>,type(c))</span><br><span class="line">print(<span class="string">"e is type "</span>,type(e))</span><br><span class="line">print(<span class="string">"g is type "</span>,type(g))</span><br></pre></td></tr></table></figure><hr><pre><code>a is type  &lt;class &apos;int&apos;&gt;c is type  &lt;class &apos;float&apos;&gt;e is type  &lt;class &apos;complex&apos;&gt;g is type  &lt;class &apos;str&apos;&gt;</code></pre><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><p>Python语言支持以下类型的运算符:</p><ul><li>算术运算符</li><li>比较（关系）运算符</li><li>赋值运算符</li><li>逻辑运算符</li><li>位运算符</li><li>成员运算符</li><li>身份运算符</li></ul><h4 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h4><p><img src="http://xukeqiniu.xukeai.cn/1156040a45a188c8043ccdf0dea294c1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment">#两个数字相加</span></span><br><span class="line">sumNumber=<span class="number">1</span>+<span class="number">2</span></span><br><span class="line">print(sumNumber)      <span class="comment">#输出结果：3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#两个字符串相加</span></span><br><span class="line">sumString=<span class="string">"Nice"</span> + <span class="string">"work"</span></span><br><span class="line">print(sumString)      <span class="comment">#输出结果：Nicework</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#两个数字相减</span></span><br><span class="line">subNumber=<span class="number">2</span><span class="number">-1</span></span><br><span class="line">print(subNumber)      <span class="comment">#输出结果：1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#两个数字相乘或者字符串重复</span></span><br><span class="line">multiplicationNumber=<span class="number">2</span>*<span class="number">3</span></span><br><span class="line">print(multiplicationNumber)      <span class="comment">#输出结果：6</span></span><br><span class="line">multiplicationString=<span class="string">"hello"</span>*<span class="number">2</span></span><br><span class="line">print(multiplicationString)      <span class="comment">#输出结果：hellohello</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#两个数相除</span></span><br><span class="line">divisionNumber=<span class="number">9</span>/<span class="number">2</span></span><br><span class="line">print(divisionNumber)      <span class="comment">#输出结果：4</span></span><br><span class="line">divisionNumber=<span class="number">9.0</span>/<span class="number">2</span></span><br><span class="line">print(divisionNumber)      <span class="comment">#输出结果：4.5</span></span><br><span class="line">divisionNumber=<span class="number">9</span>/<span class="number">2.0</span></span><br><span class="line">print(divisionNumber)      <span class="comment">#输出结果：4.5</span></span><br><span class="line"><span class="comment">#/---除数或被除数中有任意一个是小数的话，商也会保留小数，反之取整---/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#除法运算// 返回商的整数部分，抛弃余数</span></span><br><span class="line">divisorNumber=<span class="number">10</span>//<span class="number">3</span></span><br><span class="line">print(divisorNumber)        <span class="comment">#输出结果：3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#除法运算% 返回商的余数部分，抛弃商</span></span><br><span class="line">divisorNumber=<span class="number">10</span>%<span class="number">3</span></span><br><span class="line">print(divisorNumber)        <span class="comment">#输出结果：1</span></span><br><span class="line">divisorNumber=<span class="number">10</span>%<span class="number">1</span></span><br><span class="line">print(divisorNumber)        <span class="comment">#输出结果：0 /--没有余数则返回0--/</span></span><br><span class="line">divisorNumberx=<span class="number">10</span>//<span class="number">3</span>         <span class="comment">#divisorNumberx是商的整数部分</span></span><br><span class="line">divisorNumbery=<span class="number">10</span>%<span class="number">3</span>         <span class="comment">#divisorNumbery是余数</span></span><br><span class="line">divisorNumberz=<span class="number">3</span>*divisorNumberx+divisorNumbery <span class="comment">#divisorNumberz是除数乘以商的整数部分加上余数，得到的divisorNumberz的值就是被除数</span></span><br><span class="line">print(divisorNumberz)        <span class="comment">#输出结果：10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#求幂运算</span></span><br><span class="line">powerNumber=<span class="number">2</span>**<span class="number">3</span> <span class="comment">#相当于2的3次幂，就是2*2*2 关于幂运算大家应该在数学里都很熟悉了</span></span><br><span class="line">print(powerNumber)       <span class="comment">#输出结果：8</span></span><br></pre></td></tr></table></figure><hr><pre><code>3Nicework16hellohello4.54.54.5310108</code></pre><h4 id="比较（关系）运算符"><a href="#比较（关系）运算符" class="headerlink" title="比较（关系）运算符"></a>比较（关系）运算符</h4><p><img src="http://xukeqiniu.xukeai.cn/bb421a45346f25d80b217f8ede56c0f8.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#小于符号，返回值是bool值</span></span><br><span class="line">lessThan=<span class="number">1</span>&lt;<span class="number">2</span></span><br><span class="line">print(lessThan)        <span class="comment">#输出结果：True</span></span><br><span class="line">lessThan=<span class="number">1</span>&lt;<span class="number">1</span></span><br><span class="line">print(lessThan)        <span class="comment">#输出结果：False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#大于符号，返回值是bool值</span></span><br><span class="line">moreThan=<span class="number">2</span>&gt;<span class="number">1</span></span><br><span class="line">print(moreThan)        <span class="comment">#输出结果：True</span></span><br><span class="line">moreThan=<span class="number">2</span>&gt;<span class="number">2</span></span><br><span class="line">print(moreThan)        <span class="comment">#输出结果：False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#不等于符号 返回值是Bool值</span></span><br><span class="line">notEqual=<span class="number">1</span>!=<span class="number">2</span></span><br><span class="line">print(notEqual)        <span class="comment">#输出结果：True</span></span><br><span class="line">notEqual=<span class="number">1</span>!=<span class="number">1</span></span><br><span class="line">print(notEqual)        <span class="comment">#输出结果：False</span></span><br></pre></td></tr></table></figure><hr><pre><code>TrueFalseTrueFalseTrueFalse</code></pre><h4 id="赋值运算符"><a href="#赋值运算符" class="headerlink" title="赋值运算符"></a>赋值运算符</h4><p><img src="http://xukeqiniu.xukeai.cn/3d838a2e1d0552017d2b55598f9ba9fb.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line">a = <span class="number">21</span></span><br><span class="line">b = <span class="number">10</span></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">c = a + b</span><br><span class="line">print(<span class="string">"1 - c 的值为："</span>, c)</span><br><span class="line"></span><br><span class="line">c += a</span><br><span class="line">print(<span class="string">"2 - c 的值为："</span>, c)</span><br><span class="line"></span><br><span class="line">c *= a</span><br><span class="line">print(<span class="string">"3 - c 的值为："</span>, c)</span><br><span class="line"></span><br><span class="line">c /= a</span><br><span class="line">print(<span class="string">"4 - c 的值为："</span>, c)</span><br><span class="line"></span><br><span class="line">c = <span class="number">2</span></span><br><span class="line">c %= a</span><br><span class="line">print(<span class="string">"5 - c 的值为："</span>, c)</span><br><span class="line"></span><br><span class="line">c **= a</span><br><span class="line">print(<span class="string">"6 - c 的值为："</span>, c)</span><br><span class="line"></span><br><span class="line">c //= a</span><br><span class="line">print(<span class="string">"7 - c 的值为："</span>, c)</span><br></pre></td></tr></table></figure><hr><pre><code>1 - c 的值为： 312 - c 的值为： 523 - c 的值为： 10924 - c 的值为： 52.05 - c 的值为： 26 - c 的值为： 20971527 - c 的值为： 99864</code></pre><h4 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h4><p><img src="http://xukeqiniu.xukeai.cn/7d9dd177653643ff50226641dbd08d5e.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#逻辑非 not</span></span><br><span class="line">operationx=<span class="keyword">True</span></span><br><span class="line">operationy=<span class="keyword">not</span> operationx</span><br><span class="line">print(operationy)        <span class="comment">#输出结果：False</span></span><br><span class="line">operationz=<span class="keyword">False</span></span><br><span class="line">print(<span class="keyword">not</span> operationz)        <span class="comment">#输出结果：True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#逻辑与 and</span></span><br><span class="line">print(<span class="keyword">True</span> <span class="keyword">and</span> <span class="keyword">True</span>)        <span class="comment">#输出结果：True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#逻辑或 or</span></span><br><span class="line">print(<span class="keyword">False</span> <span class="keyword">or</span> <span class="keyword">False</span>)        <span class="comment">#输出结果：False</span></span><br></pre></td></tr></table></figure><hr><pre><code>FalseTrueTrueFalse</code></pre><h4 id="位运算符"><a href="#位运算符" class="headerlink" title="位运算符"></a>位运算符</h4><p><img src="http://xukeqiniu.xukeai.cn/64bc95828a25c7b3eb824338c01077b5.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按位与运算&amp;， 按位与是指一个数字转化为二进制，然后这些二进制的数按位来进行与运算</span></span><br><span class="line">operationNumber=<span class="number">7</span>&amp;<span class="number">18</span></span><br><span class="line">print(operationNumber)        <span class="comment">#输出结果：2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#按位或运算|， 按位或是指一个数字转化为二进制，然后这些二进制的数按位来进行或运算</span></span><br><span class="line">operationNumber=<span class="number">7</span>|<span class="number">18</span></span><br><span class="line">print(operationNumber)        <span class="comment">#输出结果：23   #结题思路和按位与运算的一样，可以参考按位与运算</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#按位异或</span></span><br><span class="line">operationNumber=<span class="number">7</span>^<span class="number">18</span></span><br><span class="line">print(operationNumber)        <span class="comment">#输出结果：21   #结题思路和按位与运算的一样，可以参考按位与运算</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#按位翻转 ~   按位翻转公式: ~x= - (x+1)</span></span><br><span class="line">operationNumber=~<span class="number">12</span>  <span class="comment">#~12=- (12+1) = -13</span></span><br><span class="line">print(operationNumber)        <span class="comment">#输出结果：-13   #结题思路和按位与运算的一样，可以参考按位与运算</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#左移&lt;&lt;</span></span><br><span class="line">operationNumber=<span class="number">12</span>&lt;&lt;<span class="number">1</span></span><br><span class="line">print(operationNumber)        <span class="comment">#输出结果：24</span></span><br><span class="line">operationNumber=<span class="number">3</span>&lt;&lt;<span class="number">3</span></span><br><span class="line">print(operationNumber)        <span class="comment">#输出结果：24</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#右移&gt;&gt;</span></span><br><span class="line">operationNumber=<span class="number">12</span>&gt;&gt;<span class="number">1</span></span><br><span class="line">print(operationNumber)        <span class="comment">#输出结果：6</span></span><br><span class="line">operationNumber=<span class="number">12</span>&gt;&gt;<span class="number">2</span></span><br><span class="line">print(operationNumber)        <span class="comment">#输出结果：3</span></span><br></pre></td></tr></table></figure><hr><pre><code>22321-13242463</code></pre><h4 id="成员运算符"><a href="#成员运算符" class="headerlink" title="成员运算符"></a>成员运算符</h4><p><img src="http://xukeqiniu.xukeai.cn/30d6e065b75d1b7d288fab02b065813c.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line">a = <span class="number">10</span></span><br><span class="line">b = <span class="number">20</span></span><br><span class="line">list = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> ];</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ( a <span class="keyword">in</span> list ):</span><br><span class="line">    print(<span class="string">"1 - 变量 a 在给定的列表中 list 中"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"1 - 变量 a 不在给定的列表中 list 中"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ( b <span class="keyword">not</span> <span class="keyword">in</span> list ):</span><br><span class="line">    print(<span class="string">"2 - 变量 b 不在给定的列表中 list 中"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"2 - 变量 b 在给定的列表中 list 中"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改变量 a 的值</span></span><br><span class="line">a = <span class="number">2</span></span><br><span class="line"><span class="keyword">if</span> ( a <span class="keyword">in</span> list ):</span><br><span class="line">    print(<span class="string">"3 - 变量 a 在给定的列表中 list 中"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"3 - 变量 a 不在给定的列表中 list 中"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>1 - 变量 a 不在给定的列表中 list 中2 - 变量 b 不在给定的列表中 list 中3 - 变量 a 在给定的列表中 list 中</code></pre><h4 id="身份运算符"><a href="#身份运算符" class="headerlink" title="身份运算符"></a>身份运算符</h4><p><img src="http://xukeqiniu.xukeai.cn/abaa0b517b8eb5bcf68f41ca098a8427.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"></span><br><span class="line">a = <span class="number">20</span></span><br><span class="line">b = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ( a <span class="keyword">is</span> b ):</span><br><span class="line">    print(<span class="string">"1 - a 和 b 有相同的标识"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"1 - a 和 b 没有相同的标识"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>( a <span class="keyword">is</span> <span class="keyword">not</span> b ):</span><br><span class="line">    print(<span class="string">"2 - a 和 b 没有相同的标识"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"2 - a 和 b 有相同的标识"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改变量 b 的值</span></span><br><span class="line">b = <span class="number">30</span></span><br><span class="line"><span class="keyword">if</span> ( a <span class="keyword">is</span> b ):</span><br><span class="line">    print(<span class="string">"3 - a 和 b 有相同的标识"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"3 - a 和 b 没有相同的标识"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ( a <span class="keyword">is</span> <span class="keyword">not</span> b ):</span><br><span class="line">    print(<span class="string">"4 - a 和 b 没有相同的标识"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"4 - a 和 b 有相同的标识"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>1 - a 和 b 有相同的标识2 - a 和 b 有相同的标识3 - a 和 b 没有相同的标识4 - a 和 b 没有相同的标识</code></pre><h4 id="运算符优先级"><a href="#运算符优先级" class="headerlink" title="运算符优先级"></a>运算符优先级</h4><p><img src="http://xukeqiniu.xukeai.cn/652fb727619f18fba95899e27f1db4d2.png" alt=""></p><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><p>本文整理几种基本容器：</p><ul><li>列表（list）</li><li>元组（tuple）</li><li>字典（dict）</li><li>集合（set）</li></ul><h4 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h4><p><code>list</code>是一种有序的集合，<strong>可以随时添加和删除其中的元素</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化列表</span></span><br><span class="line">li = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">'abc'</span>, <span class="number">4.5</span>, [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], &#123;<span class="number">1</span>:<span class="string">'one'</span>&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取长度</span></span><br><span class="line">print(len(li))  <span class="comment"># 结果为：7</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据索引读写</span></span><br><span class="line">print(li[<span class="number">0</span>])  <span class="comment">#索引从零开始，结果为：1</span></span><br><span class="line">print(li[<span class="number">3</span>])  <span class="comment">#结果为：'abc'</span></span><br><span class="line">print(li[<span class="number">-1</span>]) <span class="comment">#索引从后往前，结果为：&#123;1: 'one'&#125;</span></span><br><span class="line">print(li[<span class="number">-3</span>]) <span class="comment">#结果为：4.5</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加元素</span></span><br><span class="line">li = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">li.append(<span class="string">'a'</span>)</span><br><span class="line">li.append(<span class="string">'b'</span>)</span><br><span class="line">print(li)  <span class="comment">#结果为：[1, 2, 3, 'a', 'b']</span></span><br><span class="line">li.append([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]) <span class="comment">#添加一个列表[4,5,6]</span></span><br><span class="line">print(li)  <span class="comment">#结果为：[1, 2, 3, 'a', 'b', [4, 5, 6]]</span></span><br><span class="line">li = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">li.extend([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]) <span class="comment">#往列表中添加数字4，5，6</span></span><br><span class="line">print(li)  <span class="comment">#结果为：[1, 2, 3, 4, 5, 6]</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除元素</span></span><br><span class="line">li = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">li.pop()   <span class="comment">#采用pop的方法,按照栈的方式弹出，结果为：[1, 2, 3, 4]</span></span><br><span class="line">print(li)</span><br><span class="line"><span class="keyword">del</span>(li[<span class="number">0</span>]) <span class="comment">#删掉第一个数据，现在li的数据排列为[2,3,4,5]</span></span><br><span class="line"><span class="keyword">del</span>(li[<span class="number">1</span>]) <span class="comment">#删掉[2,3,4,5]中的第二个数据</span></span><br><span class="line">print(li)  <span class="comment">#结果为：[2, 4]</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 元素是否存在</span></span><br><span class="line">li = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">print(<span class="number">1</span> <span class="keyword">in</span> li) <span class="comment">#结果为True</span></span><br><span class="line">print(<span class="number">6</span> <span class="keyword">in</span> li) <span class="comment">#结果为False</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列表是否为空</span></span><br><span class="line">li = []</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> li:</span><br><span class="line">    print(<span class="string">'Empty'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Not empty'</span>)</span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 字符串</span></span><br><span class="line">ss = <span class="string">'abcdefg'</span></span><br><span class="line">li = list(ss)</span><br><span class="line">li[<span class="number">4</span>] = <span class="string">'E'</span> <span class="comment">#取第五个字符并置为'E'</span></span><br><span class="line">li[<span class="number">5</span>] = <span class="string">'F'</span> <span class="comment">#取第六个字符并置为'F'</span></span><br><span class="line">print(li)   <span class="comment">#结果为：['a', 'b', 'c', 'd', 'E', 'F', 'g']</span></span><br><span class="line">ss = <span class="string">''</span>.join(li) <span class="comment">#去掉引号</span></span><br><span class="line">print(ss)    <span class="comment">#结果为：abcdEFg</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历</span></span><br><span class="line">li = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> li:</span><br><span class="line">    print(i)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(li)):</span><br><span class="line">    print(li[i])</span><br></pre></td></tr></table></figure><hr><pre><code>71abc{1: &apos;one&apos;}4.5[1, 2, 3, &apos;a&apos;, &apos;b&apos;][1, 2, 3, &apos;a&apos;, &apos;b&apos;, [4, 5, 6]][1, 2, 3, 4, 5, 6][1, 2, 3, 4][2, 4]TrueFalseEmpty[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;E&apos;, &apos;F&apos;, &apos;g&apos;]abcdEFg123123</code></pre><h4 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h4><p>另一种有序列表叫元组：tuple。tuple和list非常类似，但是<strong>tuple一旦初始化就不能修改</strong>。<br>其中需要注意的是：tuple所谓的“不变”是说，tuple的每个元素，<strong>指向永远不变</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建 tuple</span></span><br><span class="line">number_tuple = (<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>) <span class="comment">#数字</span></span><br><span class="line">print(<span class="string">"number_tuple: "</span> + str(number_tuple))</span><br><span class="line">print(number_tuple[<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">string_tuple = (<span class="string">"adc"</span>,<span class="string">"sdf"</span>,<span class="string">"python"</span>) <span class="comment">#字符串</span></span><br><span class="line">print(<span class="string">"string_tuple: "</span> + str(string_tuple))</span><br><span class="line">print(string_tuple[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">mixed_tuple  = (<span class="string">"python"</span>,<span class="number">1</span>,<span class="number">5</span>) <span class="comment">#数字+字符串</span></span><br><span class="line">print(<span class="string">"mixed_tuple: "</span> + str(mixed_tuple))</span><br><span class="line">print(mixed_tuple[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#访问tuple元素</span></span><br><span class="line">a = number_tuple[<span class="number">2</span>]</span><br><span class="line">b = string_tuple[<span class="number">1</span>]</span><br><span class="line">c = mixed_tuple [<span class="number">0</span>]</span><br><span class="line">print(<span class="string">"a:&#123;0&#125;\nb:&#123;1&#125;\nc:&#123;2&#125;\n"</span>.format(a,b,c))</span><br><span class="line"></span><br><span class="line"><span class="comment">#tuple的截取</span></span><br><span class="line">abcd_tuple = (<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>)</span><br><span class="line">print(abcd_tuple[<span class="number">1</span>])</span><br><span class="line">print(abcd_tuple[<span class="number">-2</span>])</span><br><span class="line">print(abcd_tuple[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment">#tuple 中嵌入list</span></span><br><span class="line">mix_tuple = (<span class="number">1</span>,<span class="number">2</span>,[<span class="string">'a'</span>,<span class="string">'b'</span>])</span><br><span class="line">print(<span class="string">"mix_tuple: "</span> + str(mix_tuple))</span><br><span class="line">mix_tuple[<span class="number">2</span>][<span class="number">0</span>] = <span class="string">'c'</span></span><br><span class="line">mix_tuple[<span class="number">2</span>][<span class="number">1</span>] = <span class="string">'d'</span></span><br><span class="line">print(<span class="string">"mix_tuple: "</span> + str(mix_tuple))</span><br><span class="line"></span><br><span class="line"><span class="comment">#list 中嵌入tuple</span></span><br><span class="line">mix_list = [<span class="number">1</span>,<span class="number">2</span>,(<span class="string">'a'</span>,<span class="string">'b'</span>)]</span><br><span class="line">print(<span class="string">"mix_list: "</span> + str(mix_list))</span><br><span class="line">mix_list[<span class="number">2</span>] = (<span class="string">'c'</span>,<span class="string">'d'</span>)</span><br><span class="line">print(<span class="string">"mix_list: "</span> + str(mix_list))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历</span></span><br><span class="line">tup = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tup:</span><br><span class="line">    print(i)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(tup)):</span><br><span class="line">    print(tup[i])</span><br></pre></td></tr></table></figure><hr><pre><code>number_tuple: (1, 3, 5, 7, 9)9string_tuple: (&apos;adc&apos;, &apos;sdf&apos;, &apos;python&apos;)sdfmixed_tuple: (&apos;python&apos;, 1, 5)pythona:5b:sdfc:pythonbc(&apos;b&apos;, &apos;c&apos;, &apos;d&apos;)mix_tuple: (1, 2, [&apos;a&apos;, &apos;b&apos;])mix_tuple: (1, 2, [&apos;c&apos;, &apos;d&apos;])mix_list: [1, 2, (&apos;a&apos;, &apos;b&apos;)]mix_list: [1, 2, (&apos;c&apos;, &apos;d&apos;)]123123</code></pre><h4 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h4><p>字典由<code>keys</code>（键）和<code>values</code>（值）组成。字典的每个键值(key=&gt;value)对,用冒号(:)分割，每个对之间用逗号(,)分割，整个字典包括在花括号({})中.<strong>键必须是唯一的，但值则不必</strong>。<strong>值可以取任何数据类型，但键必须是不可变的，如字符串，数字或元组</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">d = &#123;<span class="string">'a'</span>:<span class="number">1</span>, <span class="number">2</span>:<span class="string">'b'</span>, <span class="string">'c'</span>:<span class="number">3</span>, <span class="number">4</span>:<span class="string">'d'</span>&#125;</span><br><span class="line">print(d)  <span class="comment">#结果为：&#123;'a': 1, 2: 'b', 'c': 3, 4: 'd'&#125;</span></span><br><span class="line">print(d.keys()) <span class="comment">#结果为：dict_keys(['a', 2, 'c', 4])</span></span><br><span class="line">print(d.values()) <span class="comment">#结果为：dict_values([1, 'b', 3, 'd'])</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取长度</span></span><br><span class="line">print(len(d)) <span class="comment">#结果为：4</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据key读写</span></span><br><span class="line">d[<span class="string">'a'</span>] = <span class="number">100</span></span><br><span class="line">d[<span class="number">4</span>] = <span class="string">'dd'</span></span><br><span class="line">print(d) <span class="comment">#结果为：&#123;'a': 100, 2: 'b', 'c': 3, 4: 'dd'&#125;</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加元素</span></span><br><span class="line">d[<span class="string">'e'</span>] = <span class="number">5</span></span><br><span class="line">d[<span class="number">6</span>] = <span class="string">'f'</span></span><br><span class="line">print(d) <span class="comment">#结果为：&#123;'a': 100, 2: 'b', 'c': 3, 4: 'dd', 'e': 5, 6: 'f'&#125;</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除元素</span></span><br><span class="line">d = &#123;<span class="string">'a'</span>:<span class="number">1</span>, <span class="number">2</span>:<span class="string">'b'</span>, <span class="string">'c'</span>:<span class="number">3</span>, <span class="number">4</span>:<span class="string">'d'</span>&#125;</span><br><span class="line"><span class="keyword">del</span>(d[<span class="string">'a'</span>])</span><br><span class="line"><span class="keyword">del</span>(d[<span class="number">2</span>])</span><br><span class="line">print(d) <span class="comment">#结果为：&#123;'c': 3, 4: 'd'&#125;</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断key是否存在</span></span><br><span class="line">d = &#123;<span class="string">'a'</span>:<span class="number">1</span>, <span class="number">2</span>:<span class="string">'b'</span>, <span class="string">'c'</span>:<span class="number">3</span>, <span class="number">4</span>:<span class="string">'d'</span>&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="string">'a'</span> <span class="keyword">in</span> d:</span><br><span class="line">    print(<span class="string">'a in d'</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="number">2</span> <span class="keyword">in</span> d:</span><br><span class="line">    print(<span class="string">'2 in d'</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> (<span class="string">'x'</span> <span class="keyword">in</span> d):</span><br><span class="line">    print(<span class="string">'x not in d'</span>)</span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断字典是否为空</span></span><br><span class="line">d = &#123;&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> d:</span><br><span class="line">    print(<span class="string">'d is empty'</span>)</span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历</span></span><br><span class="line">d = &#123;<span class="string">'a'</span>:<span class="number">1</span>, <span class="number">2</span>:<span class="string">'b'</span>, <span class="string">'c'</span>:<span class="number">3</span>, <span class="number">4</span>:<span class="string">'d'</span>&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> d.keys():</span><br><span class="line">    print(str(k) + <span class="string">': '</span> + str(d[k]))</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">    print(str(k) + <span class="string">': '</span> + str(v))</span><br></pre></td></tr></table></figure><hr><pre><code>{&apos;a&apos;: 1, 2: &apos;b&apos;, &apos;c&apos;: 3, 4: &apos;d&apos;}dict_keys([&apos;a&apos;, 2, &apos;c&apos;, 4])dict_values([1, &apos;b&apos;, 3, &apos;d&apos;])4{&apos;a&apos;: 100, 2: &apos;b&apos;, &apos;c&apos;: 3, 4: &apos;dd&apos;}{&apos;a&apos;: 100, 2: &apos;b&apos;, &apos;c&apos;: 3, 4: &apos;dd&apos;, &apos;e&apos;: 5, 6: &apos;f&apos;}{&apos;c&apos;: 3, 4: &apos;d&apos;}a in d2 in dx not in dd is emptya: 12: bc: 34: da: 12: bc: 34: d</code></pre><h4 id="集合（set）"><a href="#集合（set）" class="headerlink" title="集合（set）"></a>集合（set）</h4><p><code>set</code>是基本数据类型的一种集合类型，它有可变集合(<code>set()</code>)和不可变集合(<code>frozenset</code>)两种。创建集合<code>set</code>、集合<code>set</code>添加、集合删除、交集、并集、差集的操作都是非常实用的方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">s_a = set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">s_b = set([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">print(s_a) <span class="comment"># 结果为：&#123;1, 2, 3, 4, 5&#125;</span></span><br><span class="line">print(s_b) <span class="comment"># 结果为：&#123;1, 2, 3, 4, 5&#125;</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取长度</span></span><br><span class="line">print(len(s_a)) <span class="comment"># 结果为：5</span></span><br><span class="line">print(len(s_b)) <span class="comment"># 结果为：5</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加元素</span></span><br><span class="line">s_a.add(<span class="number">6</span>)</span><br><span class="line">s_a.add(<span class="number">6</span>)</span><br><span class="line">s_a.update([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">print(s_a) <span class="comment"># 结果为：&#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125;</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除元素</span></span><br><span class="line">s_a.remove(<span class="number">8</span>)</span><br><span class="line">s_a.remove(<span class="number">9</span>)</span><br><span class="line">print(s_a) <span class="comment"># 结果为：&#123;1, 2, 3, 4, 5, 6, 7&#125;</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断元素是否存在</span></span><br><span class="line">print(<span class="number">1</span> <span class="keyword">in</span> s_a)  <span class="comment"># 结果为：True</span></span><br><span class="line">print(<span class="number">10</span> <span class="keyword">in</span> s_a) <span class="comment"># 结果为：False</span></span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断集合是否为空</span></span><br><span class="line">s_a = set([])</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> s_a:</span><br><span class="line">    print(<span class="string">'set is empty'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'set is not empty'</span>)</span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历</span></span><br><span class="line">s_a = set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> s_a:</span><br><span class="line">    print(i)</span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 集合操作</span></span><br><span class="line">s_a = set([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">s_b = set([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"><span class="comment"># 并集</span></span><br><span class="line">print(s_a | s_b) <span class="comment"># 结果为：&#123;1, 2, 3, 4, 5, 6, 7, 8&#125;</span></span><br><span class="line">print(s_a.union(s_b)) <span class="comment"># 结果为：&#123;1, 2, 3, 4, 5, 6, 7, 8&#125;</span></span><br><span class="line"><span class="comment"># 交集</span></span><br><span class="line">print(s_a &amp; s_b) <span class="comment"># 结果为：&#123;4, 5&#125;</span></span><br><span class="line">print(s_a.intersection(s_b)) <span class="comment"># 结果为：&#123;4, 5&#125;</span></span><br><span class="line"><span class="comment"># 差集 s_a - (s_a and s_b)</span></span><br><span class="line">print(s_a - s_b) <span class="comment"># 结果为：&#123;1, 2, 3&#125;</span></span><br><span class="line">print(s_a.difference(s_b)) <span class="comment"># 结果为：&#123;1, 2, 3&#125;</span></span><br><span class="line"><span class="comment"># 对称差</span></span><br><span class="line">print(s_a ^ s_b) <span class="comment"># 结果为：&#123;1, 2, 3, 6, 7, 8&#125;</span></span><br><span class="line">print((s_a | s_b) - (s_a &amp; s_b)) <span class="comment"># 结果为：&#123;1, 2, 3, 6, 7, 8&#125;</span></span><br><span class="line">print(s_a.symmetric_difference(s_b)) <span class="comment"># 结果为：&#123;1, 2, 3, 6, 7, 8&#125;</span></span><br></pre></td></tr></table></figure><hr><pre><code>{1, 2, 3, 4, 5}{1, 2, 3, 4, 5}55{1, 2, 3, 4, 5, 6, 7, 8, 9}{1, 2, 3, 4, 5, 6, 7}TrueFalseset is empty12345{1, 2, 3, 4, 5, 6, 7, 8}{1, 2, 3, 4, 5, 6, 7, 8}{4, 5}{4, 5}{1, 2, 3}{1, 2, 3}{1, 2, 3, 6, 7, 8}{1, 2, 3, 6, 7, 8}{1, 2, 3, 6, 7, 8}</code></pre><h3 id="判断"><a href="#判断" class="headerlink" title="判断"></a>判断</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">number = <span class="number">59</span></span><br><span class="line">guess = int(input(<span class="string">"Enter an integer:"</span>))</span><br><span class="line">print(<span class="string">"guess is "</span> + str(guess))</span><br><span class="line"><span class="keyword">if</span> guess == number:</span><br><span class="line">    print(<span class="string">"you are right!"</span>)</span><br><span class="line"><span class="keyword">elif</span> guess &lt; number:</span><br><span class="line">    print(<span class="string">"the number is higher than that"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"the number is lower than that"</span>)</span><br><span class="line">print(<span class="string">"Done!"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Enter an integer:58guess is 58the number is higher than thatDone!</code></pre><h3 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h3><h4 id="for"><a href="#for" class="headerlink" title="for"></a>for</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">    print(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"the for loop is over"</span>)</span><br><span class="line"></span><br><span class="line">a_list  = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a_list:</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line">a_tuple = (<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a_tuple:</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line">a_dict = &#123;<span class="string">'Tom'</span>:<span class="number">111</span>,<span class="string">'Jerry'</span>:<span class="number">222</span>,<span class="string">'Cathy'</span>:<span class="number">333</span>&#125;</span><br><span class="line"><span class="keyword">for</span> ele <span class="keyword">in</span> a_dict:</span><br><span class="line">    print(ele)</span><br><span class="line">    print(a_dict[ele])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key,elem <span class="keyword">in</span> a_dict.items():</span><br><span class="line">    print(key,elem)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 例子</span></span><br><span class="line">number = <span class="number">59</span></span><br><span class="line">guess_flag = <span class="keyword">False</span></span><br><span class="line">num_chances = <span class="number">3</span></span><br><span class="line">print(<span class="string">"you have only 3 chances to guess"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,num_chances + <span class="number">1</span>):</span><br><span class="line">    print(<span class="string">"chace"</span> + str(i))</span><br><span class="line">    guess = int(input(<span class="string">"Enter an integer:"</span>))</span><br><span class="line">    <span class="keyword">if</span> guess == number:</span><br><span class="line">        print(<span class="string">"you are right!"</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">elif</span> guess &lt; number:</span><br><span class="line">        print(<span class="string">"the number is higher than that,you have only "</span> + str(num_chances-i) + <span class="string">"chance!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"the number is lower than that,you have only "</span> + str(num_chances-i) + <span class="string">"chance!"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>1234the for loop is over1357913579Tom111Jerry222Cathy333Tom 111Jerry 222Cathy 333you have only 3 chances to guesschace1Enter an integer:57the number is higher than that,you have only 2chance!chace2Enter an integer:58the number is higher than that,you have only 1chance!chace3Enter an integer:59you are right!</code></pre><h4 id="while"><a href="#while" class="headerlink" title="while"></a>while</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">number = <span class="number">59</span></span><br><span class="line">guess_flag = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> guess_flag == <span class="keyword">False</span>:</span><br><span class="line">    guess = int(input(<span class="string">"Enter an integer:"</span>))</span><br><span class="line">    print(<span class="string">"guess is: "</span> + str(guess))</span><br><span class="line">    <span class="keyword">if</span> guess == number:</span><br><span class="line">        print(<span class="string">"you are right!"</span>)</span><br><span class="line">        guess_flag = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">elif</span> guess &lt; number:</span><br><span class="line">        print(<span class="string">"the number is higher than that"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"the number is lower than that"</span>)</span><br><span class="line">print(<span class="string">"Done!"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Enter an integer:1guess is: 1the number is higher than thatEnter an integer:2guess is: 2the number is higher than thatEnter an integer:3guess is: 3the number is higher than thatEnter an integer:59guess is: 59you are right!Done!</code></pre><h3 id="break-amp-amp-continue-amp-amp-pass"><a href="#break-amp-amp-continue-amp-amp-pass" class="headerlink" title="break &amp;&amp; continue &amp;&amp; pass"></a>break &amp;&amp; continue &amp;&amp; pass</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#break &amp;&amp; continue example</span></span><br><span class="line">number = <span class="number">59</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    guess = int(input(<span class="string">"Enter an integer:"</span>))</span><br><span class="line">    print(<span class="string">"guess is: "</span> + str(guess))</span><br><span class="line">    <span class="keyword">if</span> guess == number:</span><br><span class="line">        print(<span class="string">"you are right!"</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">elif</span> guess &lt; number:</span><br><span class="line">        print(<span class="string">"the number is higher than that"</span>)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"the number is lower than that"</span>)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    print(<span class="string">"Done!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#continue &amp;&amp; pass example</span></span><br><span class="line">a_list = [<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">print(<span class="string">"using continue:"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a_list:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> i:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    print(i)</span><br><span class="line">print(<span class="string">"using pass:"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a_list:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> i:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#默认参数(放在函数参数末尾)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repeat_str</span><span class="params">(s,times = <span class="number">1</span>)</span>:</span></span><br><span class="line">    repeat_strs = s * times</span><br><span class="line">    <span class="keyword">return</span> repeat_strs</span><br><span class="line"></span><br><span class="line">repeat_strings = repeat_str(<span class="string">'Happy Birthday!'</span>)</span><br><span class="line">print(repeat_strings)</span><br><span class="line"></span><br><span class="line">repeat_strings = repeat_str(<span class="string">'Happy Birthday!'</span>,<span class="number">3</span>)</span><br><span class="line">print(repeat_strings)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#关键字参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(a,b = <span class="number">4</span>,c = <span class="number">8</span>)</span>:</span></span><br><span class="line">    print(<span class="string">'a is '</span>,a,<span class="string">'and b is'</span>,b,<span class="string">'and c is'</span>,c)</span><br><span class="line"></span><br><span class="line">func(<span class="number">13</span>,<span class="number">17</span>)</span><br><span class="line">func(<span class="number">125</span>,c = <span class="number">24</span>)</span><br><span class="line">func(c = <span class="number">40</span>,a = <span class="number">80</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#varargs 参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_paras</span><span class="params">(fpara,*num,**words)</span>:</span></span><br><span class="line">    print(<span class="string">"fpara: "</span> + str(fpara))</span><br><span class="line">    print(<span class="string">"num: "</span> + str(num))</span><br><span class="line">    print(<span class="string">"words: "</span> + str(words))</span><br><span class="line"></span><br><span class="line">print_paras(<span class="string">"Hello"</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,word = <span class="string">'python'</span>,nexword = <span class="string">'Java'</span>)</span><br></pre></td></tr></table></figure><h3 id="面型对象（初级篇）"><a href="#面型对象（初级篇）" class="headerlink" title="面型对象（初级篇）"></a>面型对象（初级篇）</h3><p>面向对象的三大特性是指：封装、继承和多态。</p><h4 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h4><p>封装，顾名思义就是将内容封装到某个地方，以后再去调用被封装在某处的内容。<br>所以，在使用面向对象的封装特性时，需要：</p><ul><li>将内容封装到某处</li><li>从某处调用被封装的内容</li></ul><p><strong>第一步：将内容封装到某处</strong><br><img src="http://xukeqiniu.xukeai.cn/32a8a026f60894faeaf56c282b194c78.png" alt=""></p><p>self 是一个形式参数，<br>当执行 <code>obj1 = Foo(&#39;wupeiqi&#39;, 18 )</code> 时，<code>self</code> 等于 <code>obj1</code><br>当执行 <code>obj2 = Foo(&#39;alex&#39;, 78 )</code> 时，<code>self</code> 等于 <code>obj2</code></p><p>所以，内容其实被封装到了对象<code>obj1</code>和<code>obj2</code>中，每个对象中都有<code>name</code>和<code>age</code>属性，在内存里类似于下图来保存。<br><img src="http://xukeqiniu.xukeai.cn/7636163c691431849ecf2c273c26810d.png" alt=""></p><p><strong>第二步：从某处调用被封装的内容</strong></p><p>调用被封装的内容时，有两种情况：</p><ul><li>通过对象直接调用</li><li>通过self间接调用</li></ul><p>1、通过对象直接调用被封装的内容<br>上图展示了对象<code>obj1</code>和<code>obj2</code>在内存中保存的方式，根据保存格式可以如此调用被封装的内容：对象.属性名</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line"></span><br><span class="line">obj1 = Foo(<span class="string">'wupeiqi'</span>, <span class="number">18</span>)</span><br><span class="line">print(obj1.name)    <span class="comment"># 直接调用obj1对象的name属性</span></span><br><span class="line">print(obj1.age)     <span class="comment"># 直接调用obj1对象的age属性</span></span><br><span class="line"></span><br><span class="line">obj2 = Foo(<span class="string">'alex'</span>, <span class="number">73</span>)</span><br><span class="line">print(obj2.name)    <span class="comment"># 直接调用obj2对象的name属性</span></span><br><span class="line">print(obj2.age)     <span class="comment"># 直接调用obj2对象的age属性</span></span><br></pre></td></tr></table></figure><hr><pre><code>wupeiqi18alex73</code></pre><p>2、通过self间接调用被封装的内容</p><p>执行类中的方法时，需要通过self间接调用被封装的内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.name)</span><br><span class="line">        print(self.age)</span><br><span class="line"></span><br><span class="line">obj1 = Foo(<span class="string">'wupeiqi'</span>, <span class="number">18</span>)</span><br><span class="line">obj1.detail()  <span class="comment"># Python默认会将obj1传给self参数，即：obj1.detail(obj1)，所以，此时方法内部的 self ＝ obj1，即：self.name 是 wupeiqi ；self.age 是 18</span></span><br><span class="line"></span><br><span class="line">obj2 = Foo(<span class="string">'alex'</span>, <span class="number">73</span>)</span><br><span class="line">obj2.detail()  <span class="comment"># Python默认会将obj2传给self参数，即：obj1.detail(obj2)，所以，此时方法内部的 self ＝ obj2，即：self.name 是 alex ； self.age 是 78</span></span><br></pre></td></tr></table></figure><hr><pre><code>wupeiqi18alex73</code></pre><p><strong>综上所述，对于面向对象的封装来说，其实就是使用构造方法将内容封装到 对象 中，然后通过对象直接或者self间接获取被封装的内容。</strong></p><h5 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h5><p>继承，面向对象中的继承和现实生活中的继承相同，即：子可以继承父的内容。</p><p>例如：</p><p>　　猫可以：喵喵叫、吃、喝、拉、撒</p><p>　　狗可以：汪汪叫、吃、喝、拉、撒</p><p>如果我们要分别为猫和狗创建一个类，那么就需要为 猫 和 狗 实现他们所有的功能，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> 猫：</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">def</span> 喵喵叫<span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'喵喵叫'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 吃<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 喝<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 拉<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 撒<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> 狗：</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">def</span> 汪汪叫<span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'喵喵叫'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 吃<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 喝<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 拉<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 撒<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br></pre></td></tr></table></figure><p>上述代码不难看出，吃、喝、拉、撒是猫和狗都具有的功能，而我们却分别的猫和狗的类中编写了两次。如果使用 继承 的思想，如下实现：</p><p>　　动物：吃、喝、拉、撒</p><p>　　   猫：喵喵叫（猫继承动物的功能）</p><p>　　   狗：汪汪叫（狗继承动物的功能）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> 动物:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 吃<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 喝<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 拉<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> 撒<span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># do something</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在类后面括号中写入另外一个类名，表示当前类继承另外一个类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> 猫<span class="params">(动物)</span>：</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">def</span> 喵喵叫<span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'喵喵叫'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在类后面括号中写入另外一个类名，表示当前类继承另外一个类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> 狗<span class="params">(动物)</span>：</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">def</span> 汪汪叫<span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'喵喵叫'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animal</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eat</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"%s 吃 "</span> %self.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">drink</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"%s 喝 "</span> %self.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shit</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"%s 拉 "</span> %self.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pee</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"%s 撒 "</span> %self.name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cat</span><span class="params">(Animal)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.breed = <span class="string">'猫'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cry</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'喵喵叫'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(Animal)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.breed ＝ <span class="string">'狗'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cry</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'汪汪叫'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ######### 执行 #########</span></span><br><span class="line"></span><br><span class="line">c1 = Cat(<span class="string">'小白家的小黑猫'</span>)</span><br><span class="line">c1.eat()</span><br><span class="line"></span><br><span class="line">c2 = Cat(<span class="string">'小黑的小白猫'</span>)</span><br><span class="line">c2.drink()</span><br><span class="line"></span><br><span class="line">d1 = Dog(<span class="string">'胖子家的小瘦狗'</span>)</span><br><span class="line">d1.eat()</span><br></pre></td></tr></table></figure><p><strong>所以，对于面向对象的继承来说，其实就是将多个类共有的方法提取到父类中，子类仅需继承父类而不必一一实现每个方法。</strong></p><p><em>注：除了子类和父类的称谓，你可能看到过 派生类 和 基类 ，他们与子类和父类只是叫法不同而已。</em></p><p><img src="http://xukeqiniu.xukeai.cn/0b7977ad0389e510df25f70dde9cd991.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animal</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eat</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"%s 吃 "</span> %self.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">drink</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"%s 喝 "</span> %self.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shit</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"%s 拉 "</span> %self.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pee</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"%s 撒 "</span> %self.name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cat</span><span class="params">(Animal)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.breed ＝ <span class="string">'猫'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cry</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'喵喵叫'</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(Animal)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.breed ＝ <span class="string">'狗'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cry</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'汪汪叫'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ######### 执行 #########</span></span><br><span class="line"></span><br><span class="line">c1 = Cat(<span class="string">'小白家的小黑猫'</span>)</span><br><span class="line">c1.eat()</span><br><span class="line"></span><br><span class="line">c2 = Cat(<span class="string">'小黑的小白猫'</span>)</span><br><span class="line">c2.drink()</span><br><span class="line"></span><br><span class="line">d1 = Dog(<span class="string">'胖子家的小瘦狗'</span>)</span><br><span class="line">d1.eat()</span><br></pre></td></tr></table></figure><p>那么问题又来了，<strong>多继承</strong>呢？</p><ul><li>是否可以继承多个类</li><li>如果继承的多个类每个类中都定了相同的函数，那么那一个会被使用呢？</li></ul><p>1、Python的类可以继承多个类，Java和C#中则只能继承一个类</p><p>2、Python的类如果继承了多个类，那么其寻找方法的方式有两种，分别是：<strong>深度优先</strong>和<strong>广度优先</strong><br><img src="http://xukeqiniu.xukeai.cn/5c411f32d39db0477a7775edcd126d4f.png" alt=""><br>当类是<strong>经典类</strong>时，多继承情况下，会按照<strong>深度优先</strong>方式查找<br>当类是<strong>新式类</strong>时，多继承情况下，会按照<strong>广度优先</strong>方式查找<br>经典类和新式类，从字面上可以看出一个老一个新，新的必然包含了跟多的功能，也是之后推荐的写法，从写法上区分的话，如果<strong>当前类</strong>或者<strong>父类</strong>继承了<strong>object类</strong>，那么该类便是<strong>新式类</strong>，否则便是经典类。<br><img src="http://xukeqiniu.xukeai.cn/0c34a6e12b9dbdf58ac555244927d4ff.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 经典类多继承</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'D.bar'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span><span class="params">(D)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'C.bar'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span><span class="params">(D)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'B.bar'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span><span class="params">(B, C)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'A.bar'</span>)</span><br><span class="line"></span><br><span class="line">a = A()</span><br><span class="line"><span class="comment"># 执行bar方法时</span></span><br><span class="line"><span class="comment"># 首先去A类中查找，如果A类中没有，则继续去B类中找，如果B类中么有，则继续去D类中找，如果D类中么有，则继续去C类中找，如果还是未找到，则报错</span></span><br><span class="line"><span class="comment"># 所以，查找顺序：A --&gt; B --&gt; D --&gt; C</span></span><br><span class="line"><span class="comment"># 在上述查找bar方法的过程中，一旦找到，则寻找过程立即中断，便不会再继续找了</span></span><br><span class="line">a.bar()</span><br></pre></td></tr></table></figure><hr><pre><code>A.bar</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新式类多继承</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'D.bar'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span><span class="params">(D)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'C.bar'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span><span class="params">(D)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'B.bar'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span><span class="params">(B, C)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'A.bar'</span>)</span><br><span class="line"></span><br><span class="line">a = A()</span><br><span class="line"><span class="comment"># 执行bar方法时</span></span><br><span class="line"><span class="comment"># 首先去A类中查找，如果A类中没有，则继续去B类中找，如果B类中么有，则继续去C类中找，如果C类中么有，则继续去D类中找，如果还是未找到，则报错</span></span><br><span class="line"><span class="comment"># 所以，查找顺序：A --&gt; B --&gt; C --&gt; D</span></span><br><span class="line"><span class="comment"># 在上述查找bar方法的过程中，一旦找到，则寻找过程立即中断，便不会再继续找了</span></span><br><span class="line">a.bar()</span><br></pre></td></tr></table></figure><hr><pre><code>A.bar</code></pre><p>经典类：首先去<strong>A类</strong>中查找，如果A类中没有，则继续去<strong>B类</strong>中找，如果B类中么有，则继续去<strong>D类</strong>中找，如果D类中么有，则继续去<strong>C类</strong>中找，如果还是未找到，则报错</p><p>新式类：首先去<strong>A类</strong>中查找，如果A类中没有，则继续去<strong>B类</strong>中找，如果B类中么有，则继续去<strong>C类</strong>中找，如果C类中么有，则继续去<strong>D类</strong>中找，如果还是未找到，则报错</p><p>注意：在上述查找过程中，一旦找到，则寻找过程立即中断，便不会再继续找了</p><h4 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h4><p> Pyhon不支持Java和C#这一类强类型语言中多态的写法，但是原生多态，其Python崇尚<strong>“鸭子类型”</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python伪代码实现Java或C#的多态</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">F1</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">S1</span><span class="params">(F1)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'S1.show'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">S2</span><span class="params">(F1)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'S2.show'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于在Java或C#中定义函数参数时，必须指定参数的类型</span></span><br><span class="line"><span class="comment"># 为了让Func函数既可以执行S1对象的show方法，又可以执行S2对象的show方法，所以，定义了一个S1和S2类的父类</span></span><br><span class="line"><span class="comment"># 而实际传入的参数是：S1对象和S2对象</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Func</span><span class="params">(F1 obj)</span>:</span></span><br><span class="line">    <span class="string">"""Func函数需要接收一个F1类型或者F1子类的类型"""</span></span><br><span class="line"></span><br><span class="line">    print(obj.show())</span><br><span class="line"></span><br><span class="line">s1_obj = S1()</span><br><span class="line">Func(s1_obj) <span class="comment"># 在Func函数中传入S1类的对象 s1_obj，执行 S1 的show方法，结果：S1.show</span></span><br><span class="line"></span><br><span class="line">s2_obj = S2()</span><br><span class="line">Func(s2_obj) <span class="comment"># 在Func函数中传入Ss类的对象 ss_obj，执行 Ss 的show方法，结果：S2.show</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python “鸭子类型”</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">F1</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">S1</span><span class="params">(F1)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'S1.show'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">S2</span><span class="params">(F1)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'S2.show'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Func</span><span class="params">(obj)</span>:</span></span><br><span class="line">    print(obj.show())</span><br><span class="line"></span><br><span class="line">s1_obj = S1()</span><br><span class="line">Func(s1_obj)</span><br><span class="line"></span><br><span class="line">s2_obj = S2()</span><br><span class="line">Func(s2_obj)</span><br></pre></td></tr></table></figure><p>以上就是本节对于面向对象初级知识的介绍，总结如下：<br>面向对象是一种编程方式，此编程方式的实现是基于对类和对象的使用<br>类是一个模板，模板中包装了多个“函数”供使用<br>对象，根据模板创建的实例（即：对象），实例用于调用被包装在类中的函数<br>面向对象三大特性：封装、继承和多态</p><h4 id="面型对象（高级篇）"><a href="#面型对象（高级篇）" class="headerlink" title="面型对象（高级篇）"></a>面型对象（高级篇）</h4><p>将详细介绍Python类的成员、成员修饰符、类的特殊成员。</p><h5 id="类的成员"><a href="#类的成员" class="headerlink" title="类的成员"></a>类的成员</h5><p>类的成员可以分为三大类：字段、方法和属性<br><img src="http://xukeqiniu.xukeai.cn/a5476e956e03d65ad44c40de134cb4be.png" alt=""><br>注：<strong>所有成员中，只有普通字段的内容保存对象中</strong>，即：根据此类创建了多少对象，在内存中就有多少个普通字段。而<strong>其他的成员，则都是保存在类中</strong>，即：无论对象的多少，在内存中只创建一份。</p><h6 id="字段"><a href="#字段" class="headerlink" title="字段"></a>字段</h6><p>字段包括：普通字段和静态字段，他们在定义和使用中有所区别，而最本质的区别是内存中保存的位置不同，</p><ul><li><strong>普通字段属于对象</strong></li><li><strong>静态字段属于类</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#字段的定义和访问</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Province</span>:</span></span><br><span class="line">    <span class="comment">#静态字段</span></span><br><span class="line">    country=<span class="string">'中国'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="comment">#普通字段</span></span><br><span class="line">        self.name = name</span><br><span class="line"><span class="comment"># 直接访问普通字段</span></span><br><span class="line">obj = Province(<span class="string">'河北省'</span>)</span><br><span class="line">print(obj.name)</span><br><span class="line"><span class="comment"># 直接访问静态字段</span></span><br><span class="line">Province.country</span><br></pre></td></tr></table></figure><hr><pre><code>河北省&apos;中国&apos;</code></pre><hr><p>由上述代码可以看出<strong>普通字段需要通过对象来访问，静态字段通过类访问</strong>，在使用上可以看出普通字段和静态字段的归属是不同的。其在内容的存储方式类似如下图：<br><img src="http://xukeqiniu.xukeai.cn/633badb35cc78d73c8837af82a5eb5c4.png" alt=""><br>由上图表示：</p><ul><li>静态字段在内存中只保存一份</li><li>普通字段在每个对象中都要保存一份</li></ul><p>应用场景：通过类创建对象时，如果每个对象都具有相同的字段，那么就使用静态字段</p><h6 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h6><p>方法包括：普通方法、类方法、静态方法，三种方法在内存中都归属于类，区别在于调用方式不同。</p><ul><li>普通方法：由<strong>对象</strong>调用；至少一个<strong>self</strong>参数；执行普通方法时，自动将调用该方法的<strong>对象</strong>赋值给<strong>self</strong>；</li><li>类方法：由<strong>类</strong>调用； 至少一个<strong>cls</strong>参数；执行类方法时，自动将调用该方法的<strong>类</strong>复制给<strong>cls</strong>；</li><li>静态方法：由<strong>类</strong>调用；无默认参数；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方法的定义和使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ord_func</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">""" 定义普通方法，至少有一个self参数 """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print self.name</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'普通方法'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">class_func</span><span class="params">(cls)</span>:</span></span><br><span class="line">        <span class="string">""" 定义类方法，至少有一个cls参数 """</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'类方法'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">static_func</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="string">""" 定义静态方法 ，无默认参数"""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'静态方法'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用普通方法</span></span><br><span class="line">f = Foo(<span class="string">'test'</span>)</span><br><span class="line">f.ord_func()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用类方法</span></span><br><span class="line">Foo.class_func()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用静态方法</span></span><br><span class="line">Foo.static_func()</span><br></pre></td></tr></table></figure><hr><pre><code>普通方法类方法静态方法</code></pre><hr><p><img src="http://xukeqiniu.xukeai.cn/e498490ac348434b13bf9034df4ffcce.png" alt=""></p><p><strong>相同点</strong>：对于所有的方法而言，均属于类（非对象）中，所以，在内存中也只保存一份。</p><p><strong>不同点</strong>：方法调用者不同、调用方法时自动传入的参数不同。</p><h6 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h6><p>如果你已经了解Python类中的方法，那么属性就非常简单了，因为<strong>Python中的属性其实是普通方法的变种</strong>。</p><p>对于属性，有以下三个知识点：</p><ul><li>属性的基本使用</li><li>属性的两种定义方式</li></ul><p>1、属性的基本使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 属性的定义和使用</span></span><br><span class="line"><span class="comment"># ############### 定义 ###############</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义属性</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prop</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="comment"># ############### 调用 ###############</span></span><br><span class="line">foo_obj = Foo()</span><br><span class="line"></span><br><span class="line">foo_obj.func()</span><br><span class="line">foo_obj.prop   <span class="comment">#调用属性</span></span><br></pre></td></tr></table></figure><p>由属性的定义和调用要注意一下几点：</p><ul><li>定义时，在普通方法的基础上添加 @property装饰器；</li><li><p>定义时，属性<strong>仅有一个</strong>self参数<br>调用时，无需<strong>括号</strong></p><pre><code>方法：foo_obj.func()属性：foo_obj.prop</code></pre><p>注意：属性存在意义是：访问属性时可以制造出和访问字段完全相同的假象</p><p>  属性由方法变种而来，如果Python中没有属性，方法完全可以代替其功能。</p></li></ul><p>实例：对于主机列表页面，每次请求不可能把数据库中的所有内容都显示到页面上，而是通过分页的功能局部显示，所以在向数据库中请求数据时就要显示的指定获取从第m条到第n条的所有数据（即：limit m,n），这个分页的功能包括：</p><ul><li>根据用户请求的当前页和总数据条数计算出 m 和 n</li><li>根据m 和 n 去数据库中请求数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ############### 定义 ###############</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pager</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, current_page)</span>:</span></span><br><span class="line">        <span class="comment"># 用户当前请求的页码（第一页、第二页...）</span></span><br><span class="line">        self.current_page = current_page</span><br><span class="line">        <span class="comment"># 每页默认显示10条数据</span></span><br><span class="line">        self.per_items = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start</span><span class="params">(self)</span>:</span></span><br><span class="line">        val = (self.current_page - <span class="number">1</span>) * self.per_items</span><br><span class="line">        <span class="keyword">return</span> val</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">end</span><span class="params">(self)</span>:</span></span><br><span class="line">        val = self.current_page * self.per_items</span><br><span class="line">        <span class="keyword">return</span> val</span><br><span class="line"></span><br><span class="line"><span class="comment"># ############### 调用 ###############</span></span><br><span class="line"></span><br><span class="line">p = Pager(<span class="number">1</span>)</span><br><span class="line">print(p.start) <span class="comment">#就是起始值，即：m</span></span><br><span class="line">print(p.end)   <span class="comment">#就是结束值，即：n</span></span><br></pre></td></tr></table></figure><hr><pre><code>010</code></pre><hr><p>从上述可见，Python的属性的功能是：<strong>属性内部进行一系列的逻辑计算，最终将计算结果返回</strong>。</p><p>2、属性的两种定义方式</p><p>属性的定义有两种方式：</p><ul><li>装饰器 即：在方法上应用装饰器</li><li>静态字段 即：在类中定义值为property对象的静态字段</li></ul><p><strong>装饰器方式</strong>：在类的普通方法上应用@property装饰器</p><p>我们知道Python中的类有经典类和新式类，新式类的属性比经典类的属性丰富。（ 如果类继object，那么该类是新式类 ）</p><ul><li><strong>经典类</strong>，具有一种@property装饰器（如上一步实例）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ############### 定义 ###############</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Goods</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"xuke"</span></span><br><span class="line"><span class="comment"># ############### 调用 ###############</span></span><br><span class="line">obj = Goods()</span><br><span class="line">result = obj.price  <span class="comment"># 自动执行 @property 修饰的 price 方法，并获取方法的返回值</span></span><br><span class="line">result</span><br></pre></td></tr></table></figure><hr><pre><code>&apos;xuke&apos;</code></pre><hr><ul><li><strong>新式类</strong>，具有三种@property装饰器</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ############### 定义 ###############</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Goods</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'@property'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @price.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        print(<span class="string">'@price.setter'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @price.deleter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'@price.deleter'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ############### 调用 ###############</span></span><br><span class="line">obj = Goods()</span><br><span class="line"></span><br><span class="line">obj.price          <span class="comment"># 自动执行 @property 修饰的 price 方法，并获取方法的返回值</span></span><br><span class="line"></span><br><span class="line">obj.price = <span class="number">123</span>    <span class="comment"># 自动执行 @price.setter 修饰的 price 方法，并将  123 赋值给方法的参数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">del</span> obj.price      <span class="comment"># 自动执行 @price.deleter 修饰的 price 方法</span></span><br></pre></td></tr></table></figure><hr><pre><code>@property@price.setter@price.deleter</code></pre><hr><p>注：经典类中的属性只有一种访问方式，其对应被 @property 修饰的方法</p><p>   新式类中的属性有三种访问方式，并分别对应了三个被@property、@方法名.setter、@方法名.deleter修饰的方法</p><p>由于新式类中具有三种访问方式，我们可以根据他们几个属性的访问特点，分别将三个方法定义为对同一个属性：获取、修改、删除</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Goods</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 原价</span></span><br><span class="line">        self.original_price = <span class="number">100</span></span><br><span class="line">        <span class="comment"># 折扣</span></span><br><span class="line">        self.discount = <span class="number">0.8</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 实际价格 = 原价 * 折扣</span></span><br><span class="line">        new_price = self.original_price * self.discount</span><br><span class="line">        <span class="keyword">return</span> new_price</span><br><span class="line"></span><br><span class="line"><span class="meta">    @price.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self.original_price = value</span><br><span class="line"></span><br><span class="line"><span class="meta">    @price.deleter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">price</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">del</span> self.original_price</span><br><span class="line"></span><br><span class="line">obj = Goods()</span><br><span class="line">obj.price         <span class="comment"># 获取商品价格</span></span><br><span class="line">obj.price = <span class="number">200</span>   <span class="comment"># 修改商品原价</span></span><br><span class="line"><span class="keyword">del</span> obj.price     <span class="comment"># 删除商品原价</span></span><br></pre></td></tr></table></figure><p><strong>静态字段方式</strong>，创建值为property对象的静态字段</p><p>当使用静态字段的方式创建属性时，经典类和新式类无区别</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'xuke'</span></span><br><span class="line"></span><br><span class="line">    BAR = property(get_bar)</span><br><span class="line"></span><br><span class="line">obj = Foo()</span><br><span class="line">reuslt = obj.BAR <span class="comment"># 自动调用get_bar方法，并获取方法的返回值</span></span><br><span class="line">print(reuslt)</span><br></pre></td></tr></table></figure><hr><pre><code>xuke</code></pre><hr><p>property的构造方法中有个四个参数</p><ul><li>第一个参数是方法名，调用 <strong>对象.属性</strong> 时自动触发执行方法</li><li>第二个参数是方法名，调用 <strong>对象.属性 ＝ XXX</strong> 时自动触发执行方法</li><li>第三个参数是方法名，调用 <strong>del 对象.属性</strong> 时自动触发执行方法</li><li>第四个参数是字符串，调用 <strong>对象.属性.__doc__</strong> ，此参数是该属性的描述信息</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'xuke'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 必须两个参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_bar</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'set value'</span> + value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">del_bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'xuke'</span></span><br><span class="line"></span><br><span class="line">    BAR = property(get_bar, set_bar, del_bar, <span class="string">'description...'</span>)</span><br><span class="line"></span><br><span class="line">obj = Foo()</span><br><span class="line"></span><br><span class="line">obj.BAR              <span class="comment"># 自动调用第一个参数中定义的方法：get_bar</span></span><br><span class="line">obj.BAR = <span class="string">"alex"</span>     <span class="comment"># 自动调用第二个参数中定义的方法：set_bar方法，并将“alex”当作参数传入</span></span><br><span class="line"><span class="keyword">del</span> obj.BAR          <span class="comment"># 自动调用第三个参数中定义的方法：del_bar方法</span></span><br><span class="line">obj.BAR.__doc__      <span class="comment"># 自动获取第四个参数中设置的值：description...</span></span><br></pre></td></tr></table></figure><p>由于静态字段方式创建属性具有三种访问方式，我们可以根据他们几个属性的访问特点，分别将三个方法定义为对同一个属性：获取、修改、删除</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Goods</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 原价</span></span><br><span class="line">        self.original_price = <span class="number">100</span></span><br><span class="line">        <span class="comment"># 折扣</span></span><br><span class="line">        self.discount = <span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_price</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 实际价格 = 原价 * 折扣</span></span><br><span class="line">        new_price = self.original_price * self.discount</span><br><span class="line">        <span class="keyword">return</span> new_price</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_price</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self.original_price = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">del_price</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">del</span> self.original_price</span><br><span class="line"></span><br><span class="line">    PRICE = property(get_price, set_price, del_price, <span class="string">'价格属性描述...'</span>)</span><br><span class="line"></span><br><span class="line">obj = Goods()</span><br><span class="line">obj.PRICE         <span class="comment"># 获取商品价格</span></span><br><span class="line">obj.PRICE = <span class="number">200</span>   <span class="comment"># 修改商品原价</span></span><br><span class="line"><span class="keyword">del</span> obj.PRICE     <span class="comment"># 删除商品原价</span></span><br></pre></td></tr></table></figure><p>所以，定义属性共有两种方式，分别是【装饰器】和【静态字段】，而【装饰器】方式针对经典类和新式类又有所不同。</p><h5 id="成员修饰符"><a href="#成员修饰符" class="headerlink" title="成员修饰符"></a>成员修饰符</h5><p>类的所有成员在上一步骤中已经做了详细的介绍，对于每一个类的成员而言都有两种形式：</p><ul><li>公有成员，在任何地方都能访问</li><li>私有成员，只有在类的内部才能方法</li></ul><p>私有成员和公有成员的定义不同：<strong>私有成员命名时，前两个字符是下划线</strong>。（特殊成员除外，例如：__init__、__call__、__dict__等）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.name = <span class="string">'公有字段'</span></span><br><span class="line">        self.__foo = <span class="string">"私有字段"</span></span><br></pre></td></tr></table></figure><p>私有成员和公有成员的访问限制不同：</p><p>静态字段</p><ul><li>公有静态字段：类可以访问；类内部可以访问；派生类中可以访问</li><li>私有静态字段：仅类内部可以访问；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 公有静态字段</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>:</span></span><br><span class="line"></span><br><span class="line">    name = <span class="string">"公有静态字段"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(C.name)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span><span class="params">(C)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(C.name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">C.name         <span class="comment"># 类访问</span></span><br><span class="line"></span><br><span class="line">obj = C()</span><br><span class="line">obj.func()     <span class="comment"># 类内部可以访问</span></span><br><span class="line"></span><br><span class="line">obj_son = D()</span><br><span class="line">obj_son.show() <span class="comment"># 派生类中可以访问</span></span><br></pre></td></tr></table></figure><hr><pre><code>公有静态字段公有静态字段</code></pre><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">私有静态字段</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>:</span></span><br><span class="line"></span><br><span class="line">    __name = <span class="string">"公有静态字段"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(C.__name)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span><span class="params">(C)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(C.__name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">C.__name       <span class="comment"># 类访问            ==&gt; 错误</span></span><br><span class="line"></span><br><span class="line">obj = C()</span><br><span class="line">obj.func()     <span class="comment"># 类内部可以访问     ==&gt; 正确</span></span><br><span class="line"></span><br><span class="line">obj_son = D()</span><br><span class="line">obj_son.show() <span class="comment"># 派生类中可以访问   ==&gt; 错误</span></span><br></pre></td></tr></table></figure><p>普通字段</p><ul><li>公有普通字段：对象可以访问；类内部可以访问；派生类中可以访问</li><li>私有普通字段：仅类内部可以访问；</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 公有字段</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.foo = <span class="string">"公有字段"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.foo) 　<span class="comment">#　类内部访问</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span><span class="params">(C)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.foo)  <span class="comment">#  派生类中访问</span></span><br><span class="line"></span><br><span class="line">obj = C()</span><br><span class="line"></span><br><span class="line">obj.foo     <span class="comment"># 通过对象访问</span></span><br><span class="line">obj.func()  <span class="comment"># 类内部访问</span></span><br><span class="line"></span><br><span class="line">obj_son = D();</span><br><span class="line">obj_son.show()  <span class="comment"># 派生类中访问</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 私有字段</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.__foo = <span class="string">"私有字段"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.foo) 　<span class="comment">#　类内部访问</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span><span class="params">(C)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.foo)　<span class="comment">#　派生类中访问</span></span><br><span class="line"></span><br><span class="line">obj = C()</span><br><span class="line"></span><br><span class="line">obj.__foo     <span class="comment"># 通过对象访问    ==&gt; 错误</span></span><br><span class="line">obj.func()    <span class="comment"># 类内部访问        ==&gt; 正确</span></span><br><span class="line"></span><br><span class="line">obj_son = D();</span><br><span class="line">obj_son.show()  <span class="comment"># 派生类中访问  ==&gt; 错误</span></span><br></pre></td></tr></table></figure><p>方法、属性的访问于上述方式相似，即：私有成员只能在类内部使用</p><h5 id="类的特殊成员"><a href="#类的特殊成员" class="headerlink" title="类的特殊成员"></a>类的特殊成员</h5><p>上文介绍了Python的类成员以及成员修饰符，从而了解到类中有字段、方法和属性三大类成员，并且成员名前如果有两个下划线，则表示该成员是私有成员，私有成员只能由类内部调用。无论人或事物往往都有不按套路出牌的情况，Python的类成员也是如此，存在着一些具有特殊含义的成员，详情如下：</p><p>1、 __doc__</p><p>　表示类的描述信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line">    <span class="string">""" 类的描述信息 """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (Foo.__doc__)</span><br><span class="line"><span class="comment">#输出：类的描述信息</span></span><br></pre></td></tr></table></figure><hr><pre><code>类的描述信息</code></pre><hr><p>2、 __module__ 和  __class__</p><p>　　__module__ 表示当前操作的对象在那个模块</p><p>　　__class__     表示当前操作的对象的类是什么</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.name = <span class="string">'wupeiqi'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># lib/aa.py</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lib.aa <span class="keyword">import</span> C</span><br><span class="line"></span><br><span class="line">obj = C()</span><br><span class="line"><span class="keyword">print</span> obj.__module__  <span class="comment"># 输出 lib.aa，即：输出模块</span></span><br><span class="line"><span class="keyword">print</span> obj.__class__      <span class="comment"># 输出 lib.aa.C，即：输出类</span></span><br></pre></td></tr></table></figure><p>3、 __init__</p><p>　　构造方法，通过类创建对象时，自动触发执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = <span class="number">18</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">obj = Foo(<span class="string">'xuke'</span>) <span class="comment"># 自动执行类中的 __init__ 方法</span></span><br></pre></td></tr></table></figure><p>4、 __del__</p><p>　　析构方法，当对象在内存中被释放时，自动触发执行。</p><p>注：此方法一般无须定义，因为Python是一门高级语言，程序员在使用时无需关心内存的分配和释放，因为此工作都是交给Python解释器来执行，所以，析构函数的调用是由解释器在进行垃圾回收时自动触发执行的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>5、__call__</p><p>　　对象后面加括号，触发执行。</p><p>注：构造方法的执行是由创建对象触发的，即：对象 = 类名() ；而对于 <strong>call</strong> 方法的执行是由对象后加括号触发的，即：对象() 或者 类()()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'__call__'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">obj = Foo() <span class="comment"># 执行 __init__</span></span><br><span class="line">obj()       <span class="comment"># 执行 __call__</span></span><br></pre></td></tr></table></figure><p>6、__dict__</p><p>　　类或对象中的所有成员</p><p>上文中我们知道：类的普通字段属于对象；类中的静态字段和方法等属于类，即：<br><img src="http://xukeqiniu.xukeai.cn/f100f53013041b925ef57914e7181ccc.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Province</span>:</span></span><br><span class="line"></span><br><span class="line">    country = <span class="string">'China'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, count)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.count = count</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'func'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取类的成员，即：静态字段、方法、</span></span><br><span class="line"><span class="keyword">print</span> (Province.__dict__)</span><br><span class="line"><span class="comment"># 输出：&#123;'country': 'China', '__module__': '__main__', 'func': &lt;function func at 0x10be30f50&gt;, '__init__': &lt;function __init__ at 0x10be30ed8&gt;, '__doc__': None&#125;</span></span><br><span class="line"></span><br><span class="line">obj1 = Province(<span class="string">'HeBei'</span>,<span class="number">10000</span>)</span><br><span class="line"><span class="keyword">print</span> (obj1.__dict__)</span><br><span class="line"><span class="comment"># 获取 对象obj1 的成员</span></span><br><span class="line"><span class="comment"># 输出：&#123;'count': 10000, 'name': 'HeBei'&#125;</span></span><br><span class="line"></span><br><span class="line">obj2 = Province(<span class="string">'HeNan'</span>, <span class="number">3888</span>)</span><br><span class="line"><span class="keyword">print</span> (obj2.__dict__)</span><br><span class="line"><span class="comment"># 获取 对象obj1 的成员</span></span><br><span class="line"><span class="comment"># 输出：&#123;'count': 3888, 'name': 'HeNan'&#125;</span></span><br></pre></td></tr></table></figure><p>7、__str__</p><p>如果一个类中定义了__str__方法，那么在打印 对象 时，默认输出该方法的返回值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'xuke'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">obj = Foo()</span><br><span class="line"><span class="keyword">print</span> (obj)</span><br><span class="line"><span class="comment"># 输出：xuke</span></span><br></pre></td></tr></table></figure><p>8、__getitem__、__setitem__、__delitem__</p><p>用于索引操作，如字典。以上分别表示获取、设置、删除数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'__getitem__'</span>,key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span><span class="params">(self, key, value)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'__setitem__'</span>,key,value)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__delitem__</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'__delitem__'</span>,key)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">obj = Foo()</span><br><span class="line"></span><br><span class="line">result = obj[<span class="string">'k1'</span>]      <span class="comment"># 自动触发执行 __getitem__</span></span><br><span class="line">obj[<span class="string">'k2'</span>] = <span class="string">'wupeiqi'</span>   <span class="comment"># 自动触发执行 __setitem__</span></span><br><span class="line"><span class="keyword">del</span> obj[<span class="string">'k1'</span>]           <span class="comment"># 自动触发执行 __delitem__</span></span><br></pre></td></tr></table></figure><p>9、__getslice__、__setslice__、__delslice__</p><p> 该三个方法用于分片操作，如：列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getslice__</span><span class="params">(self, i, j)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'__getslice__'</span>,i,j)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setslice__</span><span class="params">(self, i, j, sequence)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'__setslice__'</span>,i,j)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__delslice__</span><span class="params">(self, i, j)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'__delslice__'</span>,i,j)</span><br><span class="line"></span><br><span class="line">obj = Foo()</span><br><span class="line"></span><br><span class="line">obj[<span class="number">-1</span>:<span class="number">1</span>]                   <span class="comment"># 自动触发执行 __getslice__</span></span><br><span class="line">obj[<span class="number">0</span>:<span class="number">1</span>] = [<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>]    <span class="comment"># 自动触发执行 __setslice__</span></span><br><span class="line"><span class="keyword">del</span> obj[<span class="number">0</span>:<span class="number">2</span>]                <span class="comment"># 自动触发执行 __delslice__</span></span><br></pre></td></tr></table></figure><p>10、__iter__</p><p>用于迭代器，之所以列表、字典、元组可以进行for循环，是因为类型内部定义了 __iter__</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一步</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">obj = Foo()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> obj:</span><br><span class="line">    <span class="keyword">print</span> (i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报错：TypeError: 'Foo' object is not iterable</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第二步</span></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">obj = Foo()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> obj:</span><br><span class="line">    <span class="keyword">print</span> (i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报错：TypeError: iter() returned non-iterator of type 'NoneType'</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第三步</span></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sq)</span>:</span></span><br><span class="line">        self.sq = sq</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> iter(self.sq)</span><br><span class="line"></span><br><span class="line">obj = Foo([<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> obj:</span><br><span class="line">    <span class="keyword">print</span> (i)</span><br></pre></td></tr></table></figure><p>以上步骤可以看出，for循环迭代的其实是  iter([11,22,33,44]) ，所以执行流程可以变更为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">obj = iter([<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> obj:</span><br><span class="line">    <span class="keyword">print</span> (i)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For循环语法内部</span></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">obj = iter([<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    val = obj.next()</span><br><span class="line">    <span class="keyword">print</span> (val)</span><br></pre></td></tr></table></figure><p>11、__new__ 和 __metaclass__</p><p>阅读以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">obj = Foo()   <span class="comment"># obj是通过Foo类实例化的对象</span></span><br></pre></td></tr></table></figure><p>上述代码中，obj 是通过 Foo 类实例化的对象，其实，不仅 obj 是一个对象，Foo类本身也是一个对象，因为在<strong>Python中一切事物都是对象</strong>。</p><p>如果按照一切事物都是对象的理论：obj对象是通过执行Foo类的构造方法创建，那么Foo类对象应该也是通过执行某个类的 构造方法 创建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (type(obj)) <span class="comment"># 输出：&lt;class '__main__.Foo'&gt;     表示，obj 对象由Foo类创建</span></span><br><span class="line"><span class="keyword">print</span> (type(Foo)) <span class="comment"># 输出：&lt;type 'type'&gt;              表示，Foo类对象由 type 类创建</span></span><br></pre></td></tr></table></figure><p>所以，obj对象是Foo类的一个实例，Foo类对象是 type 类的一个实例，即：Foo类对象 是通过type类的构造方法创建。</p><p>那么，创建类就可以有两种方式：</p><p>a). 普通方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'hello xuke'</span>)</span><br></pre></td></tr></table></figure><p>b).特殊方式（type类的构造函数）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'hello xuke'</span>)</span><br><span class="line"></span><br><span class="line">Foo = type(<span class="string">'Foo'</span>,(object,), &#123;<span class="string">'func'</span>: func&#125;)</span><br><span class="line"><span class="comment">#type第一个参数：类名</span></span><br><span class="line"><span class="comment">#type第二个参数：当前类的基类</span></span><br><span class="line"><span class="comment">#type第三个参数：类的成员</span></span><br></pre></td></tr></table></figure><p>因此 <strong>类是由<code>type类</code>实例化产生</strong></p><p>那么问题来了，类默认是由 type 类实例化产生，type类中如何实现的创建类？类又是如何创建对象？</p><p>答：类中有一个属性 __metaclass__，其用来表示该类由 谁 来实例化创建，所以，我们可以为 __metaclass__ 设置一个type类的派生类，从而查看 类 创建的过程。<br><img src="http://xukeqiniu.xukeai.cn/7a5f4fe74c6a465321e33eb725d67e85.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyType</span><span class="params">(type)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, what, bases=None, dict=None)</span>:</span></span><br><span class="line">        super(MyType, self).__init__(what, bases, dict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        obj = self.__new__(self, *args, **kwargs)</span><br><span class="line"></span><br><span class="line">        self.__init__(obj)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    __metaclass__ = MyType</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> object.__new__(cls, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一阶段：解释器从上到下执行代码创建Foo类</span></span><br><span class="line"><span class="comment"># 第二阶段：通过Foo类创建obj对象</span></span><br><span class="line">obj = Foo()</span><br></pre></td></tr></table></figure><h4 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        x = int(input(<span class="string">"please enter a number"</span>))</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        print(<span class="string">"Not valid input , try again..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    f = open(<span class="string">'myfile.txt'</span>)</span><br><span class="line">    s = f.readline()</span><br><span class="line">    i = int(s.strip())</span><br><span class="line"><span class="keyword">except</span> OSError <span class="keyword">as</span> err:</span><br><span class="line">    print(<span class="string">"OS error :&#123;&#125;"</span>.format(err))</span><br><span class="line"><span class="keyword">except</span> ValueError:</span><br><span class="line">    print(<span class="string">"Could not convert data to an integer."</span>)</span><br></pre></td></tr></table></figure><pre><code>please enter a numberABCNot valid input , try again...please enter a number123OS error :[Errno 2] No such file or directory: &apos;myfile.txt&apos;</code></pre><h4 id="文件读写"><a href="#文件读写" class="headerlink" title="文件读写"></a>文件读写</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">some_sentences = <span class="string">'''\</span></span><br><span class="line"><span class="string">i love learing python</span></span><br><span class="line"><span class="string">because python is fun</span></span><br><span class="line"><span class="string">and also easy to use</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">f = open(<span class="string">'sentences.txt'</span>,<span class="string">'w'</span>)</span><br><span class="line">f.write(some_sentences)</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">f = open(<span class="string">'sentences.txt'</span>,<span class="string">'r'</span>)</span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    line = f.readline()</span><br><span class="line">    <span class="keyword">if</span> len(line) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    print(line)</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><pre><code>i love learing pythonbecause python is funand also easy to use</code></pre><h4 id="json处理"><a href="#json处理" class="headerlink" title="json处理"></a>json处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">d = &#123;<span class="string">'Python'</span>:<span class="number">100</span>, <span class="string">'C++'</span>:<span class="number">70</span>, <span class="string">'Basic'</span>:<span class="number">60</span>, <span class="string">'others'</span>:&#123;<span class="string">'C'</span>:<span class="number">65</span>, <span class="string">'Java'</span>:<span class="number">50</span>&#125;&#125;</span><br><span class="line">jtxt = json.dumps(d)</span><br><span class="line">dd = json.loads(jtxt) <span class="comment">#dd为字典类型</span></span><br><span class="line">print(jtxt)</span><br><span class="line">print(dd)</span><br><span class="line">print(dd[<span class="string">'Python'</span>])</span><br><span class="line">print(<span class="string">''</span>)</span><br></pre></td></tr></table></figure><pre><code>{&quot;Python&quot;: 100, &quot;C++&quot;: 70, &quot;Basic&quot;: 60, &quot;others&quot;: {&quot;C&quot;: 65, &quot;Java&quot;: 50}}{&apos;Python&apos;: 100, &apos;C++&apos;: 70, &apos;Basic&apos;: 60, &apos;others&apos;: {&apos;C&apos;: 65, &apos;Java&apos;: 50}}100</code></pre><p>非dict对象如何用json序列化？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, age, score)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        self.score = score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'%s: %d, %d'</span> % (self.name, self.age, self.score)</span><br><span class="line"></span><br><span class="line">s = Student(<span class="string">'Tom'</span>, <span class="number">15</span>, <span class="number">85</span>)</span><br><span class="line">print(s)</span><br><span class="line">print(s.__dict__)</span><br></pre></td></tr></table></figure><pre><code>Tom: 15, 85{&apos;name&apos;: &apos;Tom&apos;, &apos;age&apos;: 15, &apos;score&apos;: 85}</code></pre><ul><li>方法1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">jtxt = json.dumps(s, default = <span class="keyword">lambda</span> obj: obj.__dict__)</span><br><span class="line">print(jtxt)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">d2s</span><span class="params">(d)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> Student(d[<span class="string">'name'</span>], d[<span class="string">'age'</span>], d[<span class="string">'score'</span>])</span><br><span class="line">print(json.loads(jtxt, object_hook = d2s))</span><br></pre></td></tr></table></figure><pre><code>{&quot;name&quot;: &quot;Tom&quot;, &quot;age&quot;: 15, &quot;score&quot;: 85}Tom: 15, 85</code></pre><ul><li>方法2</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">s2d</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> s.__dict__</span><br><span class="line">jtxt = json.dumps(s, default = s2d)</span><br><span class="line">print(jtxt)</span><br><span class="line">dd = json.loads(jtxt) <span class="comment">#dd为字典类型</span></span><br><span class="line">print(dd[<span class="string">'name'</span>])</span><br></pre></td></tr></table></figure><hr><pre><code>{&quot;name&quot;: &quot;Tom&quot;, &quot;age&quot;: 15, &quot;score&quot;: 85}Tom</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>廖雪峰Python教程（<a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000）" target="_blank" rel="noopener">https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000）</a><br>Python 运算符   (<a href="http://www.runoob.com/python/python-operators.html#ysf6" target="_blank" rel="noopener">http://www.runoob.com/python/python-operators.html#ysf6</a>)</p><p>Python面向对象（初级篇）（<a href="http://www.cnblogs.com/wupeiqi/p/4493506.html）" target="_blank" rel="noopener">http://www.cnblogs.com/wupeiqi/p/4493506.html）</a></p><p>Python面向对象（高级篇）（<a href="http://www.cnblogs.com/wupeiqi/p/4766801.html）" target="_blank" rel="noopener">http://www.cnblogs.com/wupeiqi/p/4766801.html）</a></p>]]></content>
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Xilinx OpenCL的存储模型</title>
      <link href="/2017/12/26/SDAccel/%E5%9F%BA%E7%A1%80/OpenCL%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"/>
      <url>/2017/12/26/SDAccel/%E5%9F%BA%E7%A1%80/OpenCL%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/</url>
      <content type="html"><![CDATA[<blockquote class="blockquote-center"><br><br><img src="http://xukeqiniu.xukeai.cn/ba8a91ba461c7aee85797c3a0d624746.png" alt=""><br><br></blockquote><a id="more"></a><p>具体细节的展示图如下：<br><img src="http://xukeqiniu.xukeai.cn/dd72efd1d54575c5443bb809c32802cc.png" alt=""><br>通过上图可知，在OpenCL中提供的存储模型中，有如下几种Memory类型。</p><h3 id="Host-Memory"><a href="#Host-Memory" class="headerlink" title="Host Memory"></a>Host Memory</h3><p>Host Memory指的是连接在主板上的内存条，仅供host进行数据读取。</p><h3 id="Off-Chip-Global-memory"><a href="#Off-Chip-Global-memory" class="headerlink" title="Off-Chip Global memory"></a>Off-Chip Global memory</h3><p>Off-Chip Global memory 指的是在FPGA板卡上通过硬件与<code>FPGA</code>芯片连接的内存条。数据存取所花费的时间相对较长，但是容量相对较大。<br><img src="http://xukeqiniu.xukeai.cn/0051b0fb402e185c86f45a4fb5fd5e49.png" alt=""></p><ul><li>Off-Chip Global Memory<br>Off-Chip Global Memory Host端可以通过PCIe进行数据读写，Device端同样可以进行数据的读写</li><li>Off-Chip Constant Global Memory<br>Constant Global Memory同样是在FPGA板卡上通过硬件与<code>FPGA</code>芯片连接的内存条。不同的是Host端只能进行写入，Device端只能进行读出。适用于参数数据的传输。<h3 id="On-Chip-Memory"><a href="#On-Chip-Memory" class="headerlink" title="On-Chip Memory"></a>On-Chip Memory</h3>On-Chip Memory 主要是采用FPGA中BRAM资源组成，具有随机存储和低延时的特点，但是资源有限。</li><li>On-Chip Global Memory<br><img src="http://xukeqiniu.xukeai.cn/3d304d01541ddc4d6f277ca75a822790.png" alt=""></li><li>On-Chip Pipes<br><img src="http://xukeqiniu.xukeai.cn/105ed4ec6822b54ef66e2dc578ac140d.png" alt=""></li><li>Local Memory &amp; Private Memory<br>Local Memory可用于所有的<code>work-item</code><br>Private Memory仅用于<code>single work-item</code><br><img src="http://xukeqiniu.xukeai.cn/9e1a687b0b73cbfdd6ba0e07eced7c7b.png" alt=""></li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://china.xilinx.com/video/hardware/opencl-memory-architecture.html" target="_blank" rel="noopener">OpenCL Memory Architecture</a><br>SDAccel Environment  Optimization Guide (UG1207)</p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 基础 </tag>
            
            <tag> 存储模型 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SDAccel Dataflow 那点事儿（二）</title>
      <link href="/2017/11/27/SDAccel/%E4%BC%98%E5%8C%96/dataflow/OpenCL_dataflow_pipe/"/>
      <url>/2017/11/27/SDAccel/%E4%BC%98%E5%8C%96/dataflow/OpenCL_dataflow_pipe/</url>
      <content type="html"><![CDATA[<blockquote class="blockquote-center"><br><br><img src="http://xukeqiniu.xukeai.cn/67be912a7072aad94e097cb54c4f5a47.png" alt=""><br>dataflow可谓是FPGA性能体现的绝佳表现方式，数据在硬件中能流动起来，靠的就是dataflow!<br>下面我们将探究SDAccel OpenCL开发方式下两种dataflow的实现方式：<code>pipe</code>传输方式与<code>function</code>传输方式。<br>以及SDAccel HLS 开发方式下两种dataflow的编程风格：<code>loop</code> 与 <code>stream</code><br><a href="https://xuke225.github.io/2017/11/15/SDAccel/" target="_blank" rel="noopener">SDAccel Dataflow 那点事儿（一）</a><br><a href="">SDAccel Dataflow 那点事儿（二）</a><br><a href="">SDAccel Dataflow 那点事儿（三）</a><br><a href="">SDAccel Dataflow 那点事儿（四）</a><br></blockquote><h3 id="dataflow-pipe-方式简介"><a href="#dataflow-pipe-方式简介" class="headerlink" title="dataflow pipe 方式简介"></a>dataflow pipe 方式简介</h3><p>对于<code>OpenCL</code>来说，<code>Dataflow</code>的对象除了<code>function</code>的方式之外，还有<code>Kernel</code>的方式，因此，<code>Dataflow</code>在<code>OpenCL</code>中还有一种在<code>Kernel</code>与<code>Kernel</code>之间的传输方式,也就是通过<code>pipe</code>传输。<code>pipe</code>的传输机制采用的其实是<code>FIFO</code>的方式，因此在使用<code>pipe</code>传输的过程中需要设置<code>FIFO</code>的深度，也就是在传输过程中有多少的缓存深度。</p><a id="more"></a><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><ul><li>实现示意图</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">PIPE Memory based Adder will be implemented as below:</span><br><span class="line">                 _____________</span><br><span class="line">                |             |&lt;----- Input Vector from Global Memory</span><br><span class="line">                |  read_input |       __</span><br><span class="line">                |_____________|-----&gt;|  |</span><br><span class="line">                 _____________       |  | p0</span><br><span class="line">                |             |&lt;-----|__|</span><br><span class="line">                | compute_add |       __</span><br><span class="line">                |_____________|-----&gt;|  |</span><br><span class="line">                 ______________      |  | p1</span><br><span class="line">                |              |&lt;----|__|</span><br><span class="line">                | write_result |</span><br><span class="line">                |______________|-----&gt; Output result to Global Memory</span><br></pre></td></tr></table></figure><ul><li>host端代码</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/*******************************************************************************</span></span><br><span class="line"><span class="comment">Description: SDx Vector Addition using Blocking Pipes Operation</span></span><br><span class="line"><span class="comment">*******************************************************************************/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INCR_VALUE 10</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//OpenCL utility layer include</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"xcl.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"oclHelper.h"</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> OCL_CHECK(call)                                                        \</span></span><br><span class="line">  <span class="keyword">do</span> &#123;                                                                         \</span><br><span class="line">    cl_int err = call;                                                         \</span><br><span class="line">    <span class="keyword">if</span> (err != CL_SUCCESS) &#123;                                                   \</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"Error calling "</span> #call <span class="string">", error: %s\n"</span>, oclErrorCode(err));       \</span><br><span class="line">      <span class="built_in">exit</span>(EXIT_FAILURE);                                                      \</span><br><span class="line">    &#125;                                                                          \</span><br><span class="line">  &#125; <span class="keyword">while</span> (<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">aligned_allocator</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">using</span> value_type = T;</span><br><span class="line">  <span class="function">T* <span class="title">allocate</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> num)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">void</span>* ptr = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">if</span> (posix_memalign(&amp;ptr,<span class="number">4096</span>,num*<span class="keyword">sizeof</span>(T)))</span><br><span class="line">      <span class="keyword">throw</span> <span class="built_in">std</span>::bad_alloc();</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">reinterpret_cast</span>&lt;T*&gt;(ptr);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">deallocate</span><span class="params">(T* p, <span class="built_in">std</span>::<span class="keyword">size_t</span> num)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="built_in">free</span>(p);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">size_t</span> data_size = <span class="number">1024</span>*<span class="number">1024</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Reducing the data size for emulation mode * /</span></span><br><span class="line"><span class="comment">    char * xcl_mode = getenv("XCL_EMULATION_MODE");</span></span><br><span class="line"><span class="comment">    if (xcl_mode != NULL)&#123;</span></span><br><span class="line"><span class="comment">        data_size = 1024;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    //Allocate Memory in Host Memory</span></span><br><span class="line"><span class="comment">    size_t vector_size_bytes = sizeof(int) * data_size;</span></span><br><span class="line"><span class="comment">    std::vector&lt;int,aligned_allocator&lt;int&gt;&gt; source_input     (data_size);</span></span><br><span class="line"><span class="comment">    std::vector&lt;int,aligned_allocator&lt;int&gt;&gt; source_hw_results(data_size);</span></span><br><span class="line"><span class="comment">    std::vector&lt;int,aligned_allocator&lt;int&gt;&gt; source_sw_results(data_size);</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    // Create the test data and Software Result</span></span><br><span class="line"><span class="comment">    for(size_t i = 0 ; i &lt; data_size; i++)&#123;</span></span><br><span class="line"><span class="comment">        source_input[i] = i;</span></span><br><span class="line"><span class="comment">        source_sw_results[i] = i + INCR_VALUE;</span></span><br><span class="line"><span class="comment">        source_hw_results[i] = 0;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">//OPENCL HOST CODE AREA START</span></span><br><span class="line"><span class="comment">    //Create Program and Kernels.</span></span><br><span class="line"><span class="comment">    xcl_world world = xcl_world_single();</span></span><br><span class="line"><span class="comment">    cl_program program = xcl_import_binary(world,"adder");</span></span><br><span class="line"><span class="comment">    cl_kernel krnl_adder_stage   = xcl_get_kernel(program, "adder_stage");</span></span><br><span class="line"><span class="comment">    //Creating additional Kernels</span></span><br><span class="line"><span class="comment">    cl_kernel krnl_input_stage   = xcl_get_kernel(program, "input_stage");</span></span><br><span class="line"><span class="comment">    cl_kernel krnl_output_stage  = xcl_get_kernel(program, "output_stage");</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    // By-default xcl_world_single create command queues with sequential command.</span></span><br><span class="line"><span class="comment">    // For this example, user to replace command queue with out of order command queue</span></span><br><span class="line"><span class="comment">    clReleaseCommandQueue(world.command_queue);</span></span><br><span class="line"><span class="comment">    int err;</span></span><br><span class="line"><span class="comment">    world.command_queue = clCreateCommandQueue(world.context, world.device_id,</span></span><br><span class="line"><span class="comment">            CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE | CL_QUEUE_PROFILING_ENABLE,</span></span><br><span class="line"><span class="comment">            &amp;err);</span></span><br><span class="line"><span class="comment">    if (err != CL_SUCCESS)&#123;</span></span><br><span class="line"><span class="comment">        std::cout &lt;&lt; "Error: Failed to create a command queue!" &lt;&lt; std::endl;</span></span><br><span class="line"><span class="comment">        std::cout &lt;&lt; "Test failed" &lt;&lt; std::endl;</span></span><br><span class="line"><span class="comment">        return EXIT_FAILURE;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    //Allocate Buffer in Global Memory</span></span><br><span class="line"><span class="comment">    cl_mem buffer_output = xcl_malloc(world, CL_MEM_WRITE_ONLY, vector_size_bytes);</span></span><br><span class="line"><span class="comment">    cl_mem buffer_input = clCreateBuffer(world.context,</span></span><br><span class="line"><span class="comment">            CL_MEM_READ_ONLY | CL_MEM_USE_HOST_PTR,</span></span><br><span class="line"><span class="comment">            vector_size_bytes, source_input.data(), NULL);</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    cl_event write_event;</span></span><br><span class="line"><span class="comment">    // Using clEnqueueMigrateMemObjects() instead of clEnqueueWriteBuffer() to avoid</span></span><br><span class="line"><span class="comment">    // deadlock in real hardware which can be noticed only for large dataset.</span></span><br><span class="line"><span class="comment">    // Rootcause: design leads to a deadlock when host-&gt;DDR and</span></span><br><span class="line"><span class="comment">    // output_stage-&gt;DDR causes a contention and deadlock. In small dataset, the</span></span><br><span class="line"><span class="comment">    // data gets transferred from host-&gt; DDR in 1 burst and hence no deadlock.</span></span><br><span class="line"><span class="comment">    // Solution: Start output_stage when host-&gt;DDR data transfer is completed.</span></span><br><span class="line"><span class="comment">    // clEnqueueMigrateMemObject() event is used for all three kernels to avoid deadlock.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    //Copy input data to device global memory</span></span><br><span class="line"><span class="comment">    OCL_CHECK(clEnqueueMigrateMemObjects(world.command_queue,1, &amp;buffer_input,</span></span><br><span class="line"><span class="comment">                0 /* flags, 0 means from host*/</span>,<span class="number">0</span>, <span class="literal">NULL</span>,&amp;write_event));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Wait</span></span><br><span class="line">    clFinish(world.command_queue);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> inc = INCR_VALUE;</span><br><span class="line">    <span class="keyword">int</span> size = data_size;</span><br><span class="line">    <span class="comment">//Set the Kernel Arguments</span></span><br><span class="line">    xcl_set_kernel_arg(krnl_input_stage,<span class="number">0</span>,<span class="keyword">sizeof</span>(cl_mem),&amp;buffer_input);</span><br><span class="line">    xcl_set_kernel_arg(krnl_input_stage,<span class="number">1</span>,<span class="keyword">sizeof</span>(<span class="keyword">int</span>),&amp;size);</span><br><span class="line">    xcl_set_kernel_arg(krnl_adder_stage,<span class="number">0</span>,<span class="keyword">sizeof</span>(<span class="keyword">int</span>),&amp;inc);</span><br><span class="line">    xcl_set_kernel_arg(krnl_adder_stage,<span class="number">1</span>,<span class="keyword">sizeof</span>(<span class="keyword">int</span>),&amp;size);</span><br><span class="line">    xcl_set_kernel_arg(krnl_output_stage,<span class="number">0</span>,<span class="keyword">sizeof</span>(cl_mem),&amp;buffer_output);</span><br><span class="line">    xcl_set_kernel_arg(krnl_output_stage,<span class="number">1</span>,<span class="keyword">sizeof</span>(<span class="keyword">int</span>),&amp;size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Launch the Kernel</span></span><br><span class="line">    OCL_CHECK(clEnqueueTask(world.command_queue,krnl_input_stage, <span class="number">1</span>, &amp;write_event, <span class="literal">NULL</span>));</span><br><span class="line">    OCL_CHECK(clEnqueueTask(world.command_queue,krnl_adder_stage, <span class="number">1</span>, &amp;write_event, <span class="literal">NULL</span>));</span><br><span class="line">    OCL_CHECK(clEnqueueTask(world.command_queue,krnl_output_stage,<span class="number">1</span>, &amp;write_event, <span class="literal">NULL</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//wait for all kernels to finish their operations</span></span><br><span class="line">    clFinish(world.command_queue);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Copy Result from Device Global Memory to Host Local Memory</span></span><br><span class="line">    xcl_memcpy_from_device(world, source_hw_results.data(), buffer_output,vector_size_bytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Release Device Memories and Kernels</span></span><br><span class="line">    clReleaseMemObject(buffer_input);</span><br><span class="line">    clReleaseMemObject(buffer_output);</span><br><span class="line">    clReleaseKernel(krnl_input_stage);</span><br><span class="line">    clReleaseKernel(krnl_adder_stage);</span><br><span class="line">    clReleaseKernel(krnl_output_stage);</span><br><span class="line">    clReleaseProgram(program);</span><br><span class="line">    xcl_release_world(world);</span><br><span class="line"><span class="comment">//OPENCL HOST CODE AREA END</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compare the results of the Device to the simulation</span></span><br><span class="line">    <span class="keyword">int</span> match = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span> ; i &lt; data_size; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (source_hw_results[i] != source_sw_results[i])&#123;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Error: Result mismatch"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"i = "</span> &lt;&lt; i &lt;&lt; <span class="string">" CPU result = "</span> &lt;&lt; source_sw_results[i]</span><br><span class="line">                &lt;&lt; <span class="string">" Device result = "</span> &lt;&lt; source_hw_results[i] &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">            match = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"TEST "</span> &lt;&lt; (match ? <span class="string">"FAILED"</span> : <span class="string">"PASSED"</span>) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> (match ? EXIT_FAILURE :  EXIT_SUCCESS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>device端代码</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//Declaring PIPE memory with Depth 32</span></span><br><span class="line">pipe <span class="keyword">int</span> p0 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">int</span> p1 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line"><span class="comment">//  In Kernel code, xcl_reqd_pipe_depth attribute is used to define the PIPE</span></span><br><span class="line"><span class="comment">//  Memory depth to 32. Depth 32 means that PIPE memory can hold maximum 32</span></span><br><span class="line"><span class="comment">//  elements at a given time. If PIPE memory is full, any blocking write command</span></span><br><span class="line"><span class="comment">//  will go into wait state until some other kernel reads element from PIPE</span></span><br><span class="line"><span class="comment">//  memory. Similarly if PIPE memory is empty (no element in memory), any</span></span><br><span class="line"><span class="comment">//  blocking read command on this memory will go into wait state until some</span></span><br><span class="line"><span class="comment">//  other kernel writes elements to PIPE Memory.</span></span><br><span class="line"><span class="comment">//  This blocking read and write functionality allow designer to synchronize the</span></span><br><span class="line"><span class="comment">//  data across multiple kernels</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Input Stage Kernel : Read Data from Global Memory and write into Pipe P0</span></span><br><span class="line">kernel __attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">input_stage</span><span class="params">(__global <span class="keyword">int</span> *input, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    mem_rd: <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; size ; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//blocking Write command to pipe P0</span></span><br><span class="line">        write_pipe_block(p0, &amp;input[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Adder Stage Kernel: Read Input data from Pipe P0 and write the result</span></span><br><span class="line"><span class="comment">// into Pipe P1</span></span><br><span class="line">kernel __attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">adder_stage</span><span class="params">(<span class="keyword">int</span> inc, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    execute: <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; size ;  i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> input_data, output_data;</span><br><span class="line">        <span class="comment">//blocking read command to Pipe P0</span></span><br><span class="line">        read_pipe_block(p0, &amp;input_data);</span><br><span class="line">        output_data = input_data + inc;</span><br><span class="line">        <span class="comment">//blocking write command to Pipe P1</span></span><br><span class="line">        write_pipe_block(p1, &amp;output_data);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Output Stage Kernel: Read result from Pipe P1 and write the result to Global</span></span><br><span class="line"><span class="comment">// Memory</span></span><br><span class="line">kernel __attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">output_stage</span><span class="params">(__global <span class="keyword">int</span> *output, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    mem_wr: <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; size ; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//blocking read command to Pipe P1</span></span><br><span class="line">        read_pipe_block(p1, &amp;output[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="关于Pipe不支持结构体的解决办法"><a href="#关于Pipe不支持结构体的解决办法" class="headerlink" title="关于Pipe不支持结构体的解决办法"></a>关于Pipe不支持结构体的解决办法</h3><p>当前针对SDAccel 2017.2 版本测试发现关于Pipe的Dataflow方式是不支持结构体类型的，因此对于借助向量化的数据来说，结构体的数据类型传输会造成一定的困难，基于此问题，我们采用Python脚本加宏定义的方式进行代码生成，进而实现结构体的Pipe传输</p><ul><li>结构体类型的向量加法</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"pipe.cl"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> VEC_SIZE       4</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LANE   4</span></span><br><span class="line"><span class="comment">//Input Stage Kernel : Read Data (channel_vec Type) from Global Memory and write into Pipe</span></span><br><span class="line">kernel __attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">input_stage</span><span class="params">(__global channel_vec *input, <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">__attribute__((xcl_pipeline_loop))</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; size; k++) &#123;</span><br><span class="line">    data_ch0_write_pipe_block(input[k]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">kernel __attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add_stage</span><span class="params">(<span class="keyword">int</span> inc, <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">channel_vec input,output;</span><br><span class="line">__attribute__((xcl_pipeline_loop))</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++)&#123;</span><br><span class="line">data_ch0_read_pipe_block(input);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">char</span> ll=<span class="number">0</span>; ll&lt;LANE; ll++)&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">char</span> vv=<span class="number">0</span>; vv&lt;VEC_SIZE; vv++)&#123;</span><br><span class="line">output.lane[ll].data[vv] = input.lane[ll].data[vv] + inc;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">data_ch2_write_pipe_block(output);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">kernel __attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">output_stage</span><span class="params">(__global channel_vec *output, <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">__attribute__((xcl_pipeline_loop))</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++)&#123;</span><br><span class="line">data_ch2_read_pipe_block(output[i]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>宏定义模式</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> _PIPE_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> _PIPE_H</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line"><span class="keyword">float</span> data[<span class="number">4</span>];</span><br><span class="line">&#125; lane_data;</span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">lane_data lane[<span class="number">4</span>];</span><br><span class="line">&#125; channel_vec;</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_0 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_0 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_0 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_1 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_1 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_1 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_2 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_2 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_2 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_3 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_3 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_3 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_4 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_4 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_4 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_5 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_5 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_5 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_6 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_6 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_6 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_7 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_7 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_7 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_8 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_8 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_8 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_9 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_9 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_9 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_10 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_10 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_10 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_11 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_11 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_11 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_12 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_12 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_12 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_13 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_13 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_13 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_14 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_14 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_14 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch0_15 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch1_15 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line">pipe <span class="keyword">float</span> data_ch2_15 __attribute__((xcl_reqd_pipe_depth(<span class="number">32</span>)));</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> data_ch0_write_pipe_block(input_data)  &#123;float temp;\</span></span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_0, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_1, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_2, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_3, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_4, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_5, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_6, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_7, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_8, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_9, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_10, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_11, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_12, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_13, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_14, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch0_15, &amp;temp);&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> data_ch0_read_pipe_block(input_data)  &#123;float temp;\</span></span><br><span class="line">                                           read_pipe_block(data_ch0_0, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_1, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_2, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_3, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">3</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_4, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_5, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_6, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_7, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">3</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_8, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_9, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_10, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_11, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">3</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_12, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_13, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_14, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch0_15, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">3</span>] = temp;&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> data_ch1_write_pipe_block(input_data)  &#123;float temp;\</span></span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_0, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_1, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_2, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_3, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_4, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_5, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_6, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_7, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_8, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_9, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_10, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_11, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_12, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_13, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_14, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch1_15, &amp;temp);&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> data_ch1_read_pipe_block(input_data)  &#123;float temp;\</span></span><br><span class="line">                                           read_pipe_block(data_ch1_0, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_1, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_2, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_3, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">3</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_4, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_5, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_6, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_7, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">3</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_8, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_9, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_10, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_11, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">3</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_12, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_13, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_14, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch1_15, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">3</span>] = temp;&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> data_ch2_write_pipe_block(input_data)  &#123;float temp;\</span></span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_0, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_1, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_2, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">0</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_3, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_4, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_5, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_6, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">1</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_7, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_8, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_9, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_10, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">2</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_11, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">0</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_12, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">1</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_13, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">2</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_14, &amp;temp);\</span><br><span class="line">                                           temp = input_data.lane[<span class="number">3</span>].data[<span class="number">3</span>]; \</span><br><span class="line">                                           write_pipe_block(data_ch2_15, &amp;temp);&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> data_ch2_read_pipe_block(input_data)  &#123;float temp;\</span></span><br><span class="line">                                           read_pipe_block(data_ch2_0, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_1, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_2, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_3, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">0</span>].data[<span class="number">3</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_4, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_5, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_6, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_7, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">1</span>].data[<span class="number">3</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_8, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_9, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_10, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_11, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">2</span>].data[<span class="number">3</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_12, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">0</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_13, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">1</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_14, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">2</span>] = temp; \</span><br><span class="line">                                           read_pipe_block(data_ch2_15, &amp;temp);\</span><br><span class="line">                                           input_data.lane[<span class="number">3</span>].data[<span class="number">3</span>] = temp;&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><ul><li>Python 脚本</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Usage: python [lane_num] [vec_size]'</span></span><br><span class="line">exit(<span class="number">1</span>)</span><br><span class="line">    lane = int(sys.argv[<span class="number">1</span>])</span><br><span class="line">    vec_num = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">    all = lane*vec_num</span><br><span class="line">    code_str = <span class="string">'#ifndef _PIPE_H\n'</span>+<span class="string">'#define _PIPE_H\n'</span></span><br><span class="line">    code_str += <span class="string">'typedef struct &#123;\n'</span> +\</span><br><span class="line">                <span class="string">'float data['</span>+str(vec_num) +<span class="string">'];\n'</span>  +\</span><br><span class="line">                <span class="string">'&#125; lane_data;\n'</span></span><br><span class="line">    code_str += <span class="string">'typedef struct &#123;\n'</span> +\</span><br><span class="line">                <span class="string">'lane_data lane['</span>+str(lane) +<span class="string">'];\n'</span>  +\</span><br><span class="line">                <span class="string">'&#125; channel_vec;\n'</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,all):</span><br><span class="line">        code_str += <span class="string">'pipe float data_ch0_'</span> + str(i) + <span class="string">' __attribute__((xcl_reqd_pipe_depth(32)));\n'</span> + \</span><br><span class="line">                    <span class="string">'pipe float data_ch1_'</span> + str(i) + <span class="string">' __attribute__((xcl_reqd_pipe_depth(32)));\n'</span> + \</span><br><span class="line">                    <span class="string">'pipe float data_ch2_'</span> + str(i) + <span class="string">' __attribute__((xcl_reqd_pipe_depth(32)));\n'</span></span><br><span class="line">    code_str += <span class="string">'#define data_ch0_write_pipe_block(input_data)  '</span>+\</span><br><span class="line">                <span class="string">'&#123;float temp;\\\n'</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,lane):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,vec_num):</span><br><span class="line">    count = count + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> (count &lt;= (all<span class="number">-1</span>)):</span><br><span class="line">    code_str += <span class="string">'                                           temp = input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">']; \\\n'</span> +\</span><br><span class="line">    <span class="string">'                                           write_pipe_block(data_ch0_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);\\\n'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    code_str += <span class="string">'                                           temp = input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">']; \\\n'</span> +\</span><br><span class="line">    <span class="string">'                                           write_pipe_block(data_ch0_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);&#125;\n'</span></span><br><span class="line"></span><br><span class="line">    code_str += <span class="string">'#define data_ch0_read_pipe_block(input_data)  '</span>+\</span><br><span class="line">                <span class="string">'&#123;float temp;\\\n'</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,lane):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,vec_num):</span><br><span class="line">    count = count + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> (count &lt;= (all<span class="number">-1</span>)):</span><br><span class="line">    code_str += <span class="string">'                                           read_pipe_block(data_ch0_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);\\\n'</span>+\</span><br><span class="line">                <span class="string">'                                           input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">'] = temp; \\\n'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    code_str += <span class="string">'                                           read_pipe_block(data_ch0_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);\\\n'</span>+\</span><br><span class="line">                <span class="string">'                                           input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">'] = temp;&#125; \n'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    code_str += <span class="string">'#define data_ch1_write_pipe_block(input_data)  '</span>+\</span><br><span class="line">                <span class="string">'&#123;float temp;\\\n'</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,lane):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,vec_num):</span><br><span class="line">    count = count + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> (count &lt;= (all<span class="number">-1</span>)):</span><br><span class="line">    code_str += <span class="string">'                                           temp = input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">']; \\\n'</span> +\</span><br><span class="line">    <span class="string">'                                           write_pipe_block(data_ch1_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);\\\n'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    code_str += <span class="string">'                                           temp = input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">']; \\\n'</span> +\</span><br><span class="line">    <span class="string">'                                           write_pipe_block(data_ch1_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);&#125;\n'</span></span><br><span class="line"></span><br><span class="line">    code_str += <span class="string">'#define data_ch1_read_pipe_block(input_data)  '</span>+\</span><br><span class="line">                <span class="string">'&#123;float temp;\\\n'</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,lane):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,vec_num):</span><br><span class="line">    count = count + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> (count &lt;= (all<span class="number">-1</span>)):</span><br><span class="line">    code_str += <span class="string">'                                           read_pipe_block(data_ch1_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);\\\n'</span>+\</span><br><span class="line">                <span class="string">'                                           input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">'] = temp; \\\n'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    code_str += <span class="string">'                                           read_pipe_block(data_ch1_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);\\\n'</span>+\</span><br><span class="line">                <span class="string">'                                           input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">'] = temp;&#125; \n'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    code_str += <span class="string">'#define data_ch2_write_pipe_block(input_data)  '</span>+\</span><br><span class="line">                <span class="string">'&#123;float temp;\\\n'</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,lane):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,vec_num):</span><br><span class="line">    count = count + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> (count &lt;= (all<span class="number">-1</span>)):</span><br><span class="line">    code_str += <span class="string">'                                           temp = input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">']; \\\n'</span> +\</span><br><span class="line">    <span class="string">'                                           write_pipe_block(data_ch2_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);\\\n'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    code_str += <span class="string">'                                           temp = input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">']; \\\n'</span> +\</span><br><span class="line">    <span class="string">'                                           write_pipe_block(data_ch2_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);&#125;\n'</span></span><br><span class="line"></span><br><span class="line">    code_str += <span class="string">'#define data_ch2_read_pipe_block(input_data)  '</span>+\</span><br><span class="line">                <span class="string">'&#123;float temp;\\\n'</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,lane):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,vec_num):</span><br><span class="line">    count = count + <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> (count &lt;= (all<span class="number">-1</span>)):</span><br><span class="line">    code_str += <span class="string">'                                           read_pipe_block(data_ch2_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);\\\n'</span>+\</span><br><span class="line">                <span class="string">'                                           input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">'] = temp; \\\n'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">    code_str += <span class="string">'                                           read_pipe_block(data_ch2_'</span>+str(i*vec_num+j)+<span class="string">', &amp;temp);\\\n'</span>+\</span><br><span class="line">                <span class="string">'                                           input_data.lane['</span> + str(i) + <span class="string">'].data['</span> + str(j) + <span class="string">'] = temp;&#125; \n'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    code_str += <span class="string">'#endif\n'</span></span><br><span class="line">    fd = open(<span class="string">'pipe.cl'</span>, <span class="string">'w'</span>)</span><br><span class="line">    fd.write(code_str)</span><br><span class="line">    fd.close()</span><br></pre></td></tr></table></figure><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul><li>非结构体类型Pipe硬件仿真波形图</li></ul><p><img src="http://xukeqiniu.xukeai.cn/dc1579131dd925ed45eafe33f0bb401e.png" alt=""></p><ul><li>结构体类型Pipe硬件仿真波形图</li></ul><p><img src="http://xukeqiniu.xukeai.cn/6b1620ac10ee6d9393b4fb69ef37f116.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/08ba6a45ed0db177d0d3d990478caf3e.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/42f2d336176fb922a30d30691a77e81f.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/0d1e0e0ba375769beb6cd81797392592.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Xilinx/SDAccel_Examples/tree/master/getting_started/dataflow" target="_blank" rel="noopener">xilinx github SDAccel_Examples/getting_started/dataflow/</a><br>ug1253 SDx Pragma Reference Guide 2017.2<br>ug1207 SDAccel Environment Optmizaton Guide</p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> dataflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> dataflow </tag>
            
            <tag> pipe </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SDAccel Dataflow 那点事儿（一）</title>
      <link href="/2017/11/26/SDAccel/%E4%BC%98%E5%8C%96/dataflow/OpenCL_dataflow_function/"/>
      <url>/2017/11/26/SDAccel/%E4%BC%98%E5%8C%96/dataflow/OpenCL_dataflow_function/</url>
      <content type="html"><![CDATA[<blockquote class="blockquote-center"><br><br><img src="http://xukeqiniu.xukeai.cn/67be912a7072aad94e097cb54c4f5a47.png" alt=""><br>dataflow可谓是FPGA性能体现的绝佳表现方式，数据在硬件中能流动起来，靠的就是dataflow!<br>下面我们将探究SDAccel OpenCL开发方式下两种dataflow的实现方式：<code>pipe</code>传输方式与<code>function</code>传输方式。<br>以及SDAccel HLS 开发方式下两种dataflow的编程风格：<code>loop</code> 与 <code>stream</code><br><a href="https://xuke225.github.io/2017/11/15/SDAccel/" target="_blank" rel="noopener">SDAccel Dataflow 那点事（一）</a><br><a href="">SDAccel Dataflow 那点事（二）</a><br><a href="">SDAccel Dataflow 那点事（三）</a><br><a href="">SDAccel Dataflow 那点事（四）</a><br></blockquote><h3 id="dataflow-function-方式简介"><a href="#dataflow-function-方式简介" class="headerlink" title="dataflow function 方式简介"></a>dataflow function 方式简介</h3><p>下图是<code>Dataflow</code>优化的示意图。在不做<code>Dataflow</code>之前，<code>func_C</code>的输入需要依赖<code>func_A</code>和<code>func_B</code>的计算处理，8个时钟周期才能完成一次<code>fun_C</code>的计算结果输出。在做完<code>Dataflow</code>之后，<code>func_A</code>和<code>func_C</code>的启动间隔加快，使得5个周期就可以完成一次<code>func_C</code>的计算结果输出。从理论上上来说，<code>Dataflow</code>也属于更高层次上的一种<code>Pipeline</code>。<br><img src="http://xukeqiniu.xukeai.cn/630a6ce95be94ecc3f596cc46f969e28.png" alt=""></p><a id="more"></a><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>下面我们通过一个简单的向量乘法的例子来说明dataflow的function层面的实现。</p><ul><li>实现示意图</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Data Flow based Adder will be implemented as below:</span><br><span class="line">                _____________</span><br><span class="line">                |             |&lt;----- Input Vector from Global Memory</span><br><span class="line">                |  read_input |       __</span><br><span class="line">                |_____________|-----&gt;|  |</span><br><span class="line">                 _____________       |  | buffer_in</span><br><span class="line">                |             |&lt;-----|__|</span><br><span class="line">                | compute_add |       __</span><br><span class="line">                |_____________|-----&gt;|  |</span><br><span class="line">                 _____________       |  | buffer_out</span><br><span class="line">                |              |&lt;----|__|</span><br><span class="line">                | write_result |</span><br><span class="line">                |______________|-----&gt; Output result to Global Memory</span><br></pre></td></tr></table></figure><ul><li>host端代码</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//OpenCL utility layer include</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"xcl2.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> DATA_SIZE 4096</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INCR_VALUE 10</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//Allocate Memory in Host Memory</span></span><br><span class="line">    <span class="keyword">size_t</span> vector_size_bytes = <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * DATA_SIZE;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt; source_input     (DATA_SIZE);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt; source_hw_results(DATA_SIZE);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt; source_sw_results(DATA_SIZE);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create the test data and Software Result</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; DATA_SIZE ; i++)&#123;</span><br><span class="line">        source_input[i] = i;</span><br><span class="line">        source_sw_results[i] = i + INCR_VALUE;</span><br><span class="line">        source_hw_results[i] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//OPENCL HOST CODE AREA START</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;cl::Device&gt; devices = xcl::get_xil_devices();</span><br><span class="line">    cl::Device device = devices[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    cl::<span class="function">Context <span class="title">context</span><span class="params">(device)</span></span>;</span><br><span class="line">    cl::<span class="function">CommandQueue <span class="title">q</span><span class="params">(context, device, CL_QUEUE_PROFILING_ENABLE)</span></span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> device_name = device.getInfo&lt;CL_DEVICE_NAME&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Create Program and Kernel</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> binaryFile = xcl::find_binary_file(device_name,<span class="string">"adder"</span>);</span><br><span class="line">    cl::Program::Binaries bins = xcl::import_binary_file(binaryFile);</span><br><span class="line">    devices.resize(<span class="number">1</span>);</span><br><span class="line">    cl::<span class="function">Program <span class="title">program</span><span class="params">(context, devices, bins)</span></span>;</span><br><span class="line">    cl::<span class="function">Kernel <span class="title">krnl_adder</span><span class="params">(program,<span class="string">"adder"</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Allocate Buffer in Global Memory</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;cl::Memory&gt; inBufVec, outBufVec;</span><br><span class="line">    cl::<span class="function">Buffer <span class="title">buffer_input</span> <span class="params">(context, CL_MEM_USE_HOST_PTR | CL_MEM_READ_ONLY,</span></span></span><br><span class="line">            vector_size_bytes,source_input.data());</span><br><span class="line">    cl::<span class="function">Buffer <span class="title">buffer_output</span><span class="params">(context, CL_MEM_USE_HOST_PTR | CL_MEM_WRITE_ONLY,</span></span></span><br><span class="line">            vector_size_bytes,source_hw_results.data());</span><br><span class="line">    inBufVec.push_back(buffer_input);</span><br><span class="line">    outBufVec.push_back(buffer_output);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Copy input data to device global memory</span></span><br><span class="line">    q.enqueueMigrateMemObjects(inBufVec,<span class="number">0</span><span class="comment">/* 0 means from host*/</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> inc = INCR_VALUE;</span><br><span class="line">    <span class="keyword">int</span> size = DATA_SIZE;</span><br><span class="line">    <span class="comment">//Set the Kernel Arguments</span></span><br><span class="line">    <span class="keyword">int</span> narg=<span class="number">0</span>;</span><br><span class="line">    krnl_adder.setArg(narg++,buffer_input);</span><br><span class="line">    krnl_adder.setArg(narg++,buffer_output);</span><br><span class="line">    krnl_adder.setArg(narg++,inc);</span><br><span class="line">    krnl_adder.setArg(narg++,size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Launch the Kernel</span></span><br><span class="line">    q.enqueueTask(krnl_adder);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Copy Result from Device Global Memory to Host Local Memory</span></span><br><span class="line">    q.enqueueMigrateMemObjects(outBufVec,CL_MIGRATE_MEM_OBJECT_HOST);</span><br><span class="line">    q.finish();</span><br><span class="line"></span><br><span class="line"><span class="comment">//OPENCL HOST CODE AREA END</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compare the results of the Device to the simulation</span></span><br><span class="line">    <span class="keyword">int</span> match = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; DATA_SIZE ; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (source_hw_results[i] != source_sw_results[i])&#123;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Error: Result mismatch"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"i = "</span> &lt;&lt; i &lt;&lt; <span class="string">" CPU result = "</span> &lt;&lt; source_sw_results[i]</span><br><span class="line">                &lt;&lt; <span class="string">" Device result = "</span> &lt;&lt; source_hw_results[i] &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">            match = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"TEST "</span> &lt;&lt; (match ? <span class="string">"FAILED"</span> : <span class="string">"PASSED"</span>) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> (match ? EXIT_FAILURE :  EXIT_SUCCESS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>device端代码</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUFFER_SIZE 4096</span></span><br><span class="line"><span class="comment">//Includes</span></span><br><span class="line"><span class="comment">// Read Data from Global Memory and write into buffer_in</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">read_input</span><span class="params">(__global <span class="keyword">int</span> *in, <span class="keyword">int</span> * buffer_in,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; size ; i++)&#123;</span><br><span class="line">        buffer_in[i] =  in[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read Input data from buffer_in and write the result into buffer_out</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compute_add</span><span class="params">(<span class="keyword">int</span> * buffer_in , <span class="keyword">int</span> * buffer_out</span></span></span><br><span class="line"><span class="function"><span class="params">        , <span class="keyword">int</span> inc, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; size ; i++)&#123;</span><br><span class="line">        buffer_out[i] = buffer_in[i] + inc;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read result from buffer_out and write the result to Global Memory</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">write_result</span><span class="params">(__global <span class="keyword">int</span> *out, <span class="keyword">int</span>* buffer_out,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; size ; i++)&#123;</span><br><span class="line">        out[i] = buffer_out[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    Vector Addition Kernel Implementation using dataflow</span></span><br><span class="line"><span class="comment">    Arguments:</span></span><br><span class="line"><span class="comment">        in   (input)  --&gt; Input Vector</span></span><br><span class="line"><span class="comment">        out  (output) --&gt; Output Vector</span></span><br><span class="line"><span class="comment">        inc  (input)  --&gt; Increment</span></span><br><span class="line"><span class="comment">        size (input)  --&gt; Size of Vector in Integer</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">__kernel</span><br><span class="line">__attribute__ ((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">__attribute__ ((xcl_dataflow))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">adder</span><span class="params">(__global <span class="keyword">int</span> *in, __global <span class="keyword">int</span> *out, <span class="keyword">int</span> inc, <span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> buffer_in[BUFFER_SIZE];</span><br><span class="line">    <span class="keyword">int</span> buffer_out[BUFFER_SIZE];</span><br><span class="line"></span><br><span class="line">    read_input(in,buffer_in,size);</span><br><span class="line">    compute_add(buffer_in,buffer_out,inc,size);</span><br><span class="line">    write_result(out,buffer_out,size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><ul><li>硬件仿真波形图</li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Xilinx/SDAccel_Examples/tree/master/getting_started/dataflow" target="_blank" rel="noopener">xilinx github SDAccel_Examples/getting_started/dataflow/</a><br>ug1253 SDx Pragma Reference Guide 2017.2<br>ug1207 SDAccel Environment Optmizaton Guide</p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> dataflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> dataflow </tag>
            
            <tag> function </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SDAccel矩阵乘法优化（四）</title>
      <link href="/2017/11/25/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%964/"/>
      <url>/2017/11/25/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%964/</url>
      <content type="html"><![CDATA[<blockquote class="blockquote-center"><br><img src="http://xukeqiniu.xukeai.cn/78a27d3a0dfbdabcbb72001593a6d2d0.png" alt=""><br>从一个矩阵乘法的例子一步一步进行功能设计与性能优化。<br><a href="https://xuke225.github.io/2017/11/22/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%961/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（一）</a><br><a href="https://xuke225.github.io/2017/11/23/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%962/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（二）</a><br><a href="https://xuke225.github.io/2017/11/24/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%963/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（三）</a><br><a href="https://xuke225.github.io/2017/11/25/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%964/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（四）</a><br></blockquote><h3 id="mmult实现及优化步骤"><a href="#mmult实现及优化步骤" class="headerlink" title="mmult实现及优化步骤"></a>mmult实现及优化步骤</h3><table><thead><tr><th>步骤</th><th>实现功能</th><th>关键概念/ Keywords</th></tr></thead><tbody><tr><td>1、cpu实现</td><td>即在<code>host</code>端实现简单的矩阵乘法，便于比对数据与性能对比</td></tr><tr><td>2、OpenCL实现</td><td>在<code>device</code>端实现基于OpenCL的FPGA矩阵乘法硬件设计.</td><td><strong>Key</strong> <strong>Concepts</strong><br> - OpenCL APIs</td></tr><tr><td>3、加入<code>Local Memory</code></td><td>采用 <code>Local Memory</code> 减少<strong>数据访存</strong>次数</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Kernel Optimization<br> - Local Memory</td></tr><tr><td>4、实现读写的突发传输</td><td>采用突发传输的方式更好的实现<code>DDR</code>与 <code>Local Memory</code>数据的读写访问</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Kernel Optimization<br> - Burst Read/Write</td></tr><tr><td>5、数组分割</td><td>通过循环展开与数组分割的方式，实现更好的计算性能</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Array Partition<br> - Loop Unroll<br><strong>Keywords</strong><br> - xcl_pipeline_loop<br> - xcl_array_partition(complete, dim)<br> - opencl_unroll_hint</td></tr></tbody></table><a id="more"></a><h3 id="方案分析及优化思路三（指令优化）"><a href="#方案分析及优化思路三（指令优化）" class="headerlink" title="方案分析及优化思路三（指令优化）"></a>方案分析及优化思路三（指令优化）</h3><p>现在经过两次优化后，代码的组织结构没有什么问题了，但是关键是对应矩阵运算的嵌套for循环仅仅实现了内层的<code>pipeline</code>，因为外层for循环无法对内部的for循环<code>flatten</code>，所以外面两层的for循环没有实现pipeline，要解决这个问题，最直接的思路就是将最内层的for循环直接进行循环展开，进一步提高计算过程的并行度。但是在进行循环展开的过程中，需要将内层用到的数组进行切割，否则将无法进行<code>unroll</code>。因此，我们将用到的指令有三个内层for循环要进行循环展开，并且计算用到的数组要进行数组切割（<code>array partition</code>），次外层的for循环要进行<code>pipeline</code>。</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_SIZE 64</span></span><br><span class="line"></span><br><span class="line">kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mmult</span><span class="params">( __global <span class="keyword">int</span>* in1,  <span class="comment">//Read-only input matrix1</span></span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span>* in2,  <span class="comment">//Read-only input matrix2</span></span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span>* out,  <span class="comment">//Output matrix</span></span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">int</span> dim             <span class="comment">//One dimension of the matrix</span></span></span></span><br><span class="line"><span class="function"><span class="params">          )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//Local memory to store input matrices</span></span><br><span class="line">    <span class="comment">//Local memory is implemented as BRAM memory blocks</span></span><br><span class="line">    <span class="comment">//Complete partition done on dim 2 for in1, on dim 1 for in2 and on dim 2 for out</span></span><br><span class="line">    __local <span class="keyword">int</span> local_in1[MAX_SIZE][MAX_SIZE] __attribute__((xcl_array_partition(complete, <span class="number">2</span>)));</span><br><span class="line">    __local <span class="keyword">int</span> local_in2[MAX_SIZE][MAX_SIZE] __attribute__((xcl_array_partition(complete, <span class="number">1</span>)));</span><br><span class="line">    __local <span class="keyword">int</span> local_out[MAX_SIZE][MAX_SIZE] __attribute__((xcl_array_partition(complete, <span class="number">2</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Burst reads on input matrices from DDR memory</span></span><br><span class="line">    <span class="comment">//Burst read for matrix local_in1 and local_in2</span></span><br><span class="line">    read_in1: <span class="keyword">for</span>(<span class="keyword">int</span> iter = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; iter &lt; dim * dim; iter++, j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j == dim)&#123; j = <span class="number">0</span>; i++; &#125;</span><br><span class="line">        local_in1[i][j] = in1[iter];</span><br><span class="line">    &#125;</span><br><span class="line">    read_in2: <span class="keyword">for</span>(<span class="keyword">int</span> iter = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; iter &lt; dim * dim; iter++, j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j == dim)&#123; j = <span class="number">0</span>; i++; &#125;</span><br><span class="line">        local_in2[i][j] = in2[iter];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Based on the functionality the number of iterations</span></span><br><span class="line">    <span class="comment">//to be executed for "loop_3" must be "dim" size.</span></span><br><span class="line">    <span class="comment">//But for the pipeline to happen in the "loop_2" the</span></span><br><span class="line">    <span class="comment">//"loop_3" must be unrolled, to unroll the size cannot be dynamic.</span></span><br><span class="line">    <span class="comment">//It gives better throughput with usage of additional resources.</span></span><br><span class="line">    loop_1: <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dim; i++)&#123;</span><br><span class="line">        __attribute__((xcl_pipeline_loop))</span><br><span class="line">        loop_2: <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; dim; j++)&#123;</span><br><span class="line">            local_out[i][j] = <span class="number">0</span>;</span><br><span class="line">            __attribute__((opencl_unroll_hint))</span><br><span class="line">            loop_3: <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; MAX_SIZE; k++)&#123;</span><br><span class="line">                local_out[i][j] += local_in1[i][k] * local_in2[k][ j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Burst write from local_out to DDR memory</span></span><br><span class="line">    write_out: <span class="keyword">for</span>(<span class="keyword">int</span> iter = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; iter &lt; dim * dim; iter++, j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j == dim)&#123; j = <span class="number">0</span>; i++; &#125;</span><br><span class="line">        out[iter] = local_out[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>xcl_pipeline_loop</li></ul><p>循环流水（<code>Loop pipelining</code>）是在一个迭代周期内的多个操作可实现并行处理。下图中的<code>A</code>展示了默认情况下的顺序执行操作，每次读操作之间相差3个时钟周期（<code>II=3</code>），离最后一次写操作相差8个时钟周期。图中的<code>B</code>展示了加入循环流水的示意图，每次读操作之间相差1个周期(<code>II=1</code>)，离最后一次写操作相差4个时钟周期，在使用同样的资源下，提高了流水线的启动间隔和延迟。<br><img src="http://xukeqiniu.xukeai.cn/89f2fca8fde82ccf904243adbb520dc5.png" alt=""></p><ul><li>xcl_array_partition</li></ul><p>下图所示的是数组分隔的三种方式：<code>block</code>、<code>cyclic</code>和<code>complete</code>。<code>block</code>和<code>cyclic</code>是根据<code>factor</code>参数的设置来决定划分多少个独立的<code>RAM</code>存储（如图中的<code>factor=2</code>）。按块划为<code>block</code>是在原数组上按连续存储元素的方式划分成<code>factor</code>个块单元；按轮询<code>cyclic</code>是在原数组上按交叉存储元素的方式划分成<code>factor</code>个块单元；按全部展开<code>complete</code>是把原数组划分为一个个独立的元素（寄存器）。<br><img src="http://xukeqiniu.xukeai.cn/7696f0ee156a3a1de3aebd4bc1631f02.png" alt=""><br>对于多维数组的分割，可按照数组的维度来划分：<br><img src="http://xukeqiniu.xukeai.cn/9323b98639be0a46975f06cea4ee9777.png" alt=""></p><h4 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h4><ul><li>vivado hls log文件分析</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">WARNING: [XFORM 203-104] Completely partitioning array &apos;local_in1&apos; accessed through non-constant indices on dimension 2 (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:170:9), which may result in long runtime and suboptimal QoR due to large multiplexers. Please consider wrapping the array access into a function or using a register file core instead.</span><br><span class="line">INFO: [XFORM 203-101] Partitioning array &apos;local_in1&apos; in dimension 2 completely.</span><br><span class="line">WARNING: [XFORM 203-104] Completely partitioning array &apos;local_in2&apos; accessed through non-constant indices on dimension 1 (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:174:9), which may result in long runtime and suboptimal QoR due to large multiplexers. Please consider wrapping the array access into a function or using a register file core instead.</span><br><span class="line">INFO: [XFORM 203-101] Partitioning array &apos;local_in2&apos; in dimension 1 completely.</span><br><span class="line">WARNING: [XFORM 203-104] Completely partitioning array &apos;local_out&apos; accessed through non-constant indices on dimension 2 (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:196:9), which may result in long runtime and suboptimal QoR due to large multiplexers. Please consider wrapping the array access into a function or using a register file core instead.</span><br><span class="line">INFO: [XFORM 203-101] Partitioning array &apos;local_out&apos; in dimension 2 completely.</span><br><span class="line">INFO: [XFORM 203-11] Balancing expressions in function &apos;mmult&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:44:41)...125 expression(s) balanced.</span><br><span class="line">INFO: [HLS 200-111] Finished Pre-synthesis Time (s): cpu = 00:00:06 ; elapsed = 00:00:06 . Memory (MB): peak = 494.324 ; gain = 156.758 ; free physical = 19781 ; free virtual = 45173</span><br><span class="line">INFO: [XFORM 203-541] Flattening a loop nest &apos;loop_1&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:182:42) in function &apos;mmult&apos;.</span><br><span class="line">INFO: [XFORM 203-811] Inferring bus burst read of variable length on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:170:9).</span><br><span class="line">INFO: [XFORM 203-811] Inferring bus burst read of variable length on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:174:9).</span><br><span class="line">INFO: [XFORM 203-811] Inferring bus burst write of variable length on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:196:9).</span><br><span class="line">INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:07 ; elapsed = 00:00:07 . Memory (MB): peak = 494.324 ; gain = 156.758 ; free physical = 19781 ; free virtual = 45174</span><br><span class="line">INFO: [HLS 200-10] Starting hardware synthesis ...</span><br><span class="line">INFO: [HLS 200-10] Synthesizing &apos;mmult&apos; ...</span><br><span class="line">WARNING: [SYN 201-107] Renaming port name &apos;mmult/out&apos; to &apos;mmult/out_r&apos; to avoid the conflict with HDL keywords or other object names.</span><br><span class="line">INFO: [HLS 200-10] ----------------------------------------------------------------</span><br><span class="line">INFO: [HLS 200-42] -- Implementing module &apos;mmult&apos;</span><br><span class="line">INFO: [HLS 200-10] ----------------------------------------------------------------</span><br><span class="line">INFO: [SCHED 204-11] Starting scheduling ...</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;read_in1&apos;.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 1, Depth: 3.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;read_in2&apos;.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 1, Depth: 3.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;loop_1_loop_2&apos;.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 1, Depth: 12.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;write_out&apos;.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 1, Depth: 5.</span><br><span class="line">INFO: [SCHED 204-11] Finished scheduling.</span><br></pre></td></tr></table></figure><ul><li>HLS Report</li></ul><p><img src="http://xukeqiniu.xukeai.cn/1d973057d6b3609af2256ab9a19e4edf.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/98602da9079cd03cf8ccc7045d21ea04.png" alt=""></p><ul><li>综合结果分析<br>＊ 首先，硬件代码没有优化指令，log文件中首先将三个数组进行了对应维度的切割，然后也成功的对最内层的循环进行了<code>unroll</code>处理。<br>＊ 然后，相比于<code>Burst Read/Write</code>版本的矩阵乘法实现，该版本主要是加上了两个优化指令，实现内层循环的并行化。从<code>Pipleline</code>的角度考虑：第一段for循环<code>pipeline</code>成功；第二段的for循环嵌套中<code>write_data</code>对应的for循环并行展开，接着对其外层的for循环进行<code>flatten</code>,最终，整体实现<code>pipeline</code>；第三段for循环<code>pipeline</code>成功。<br>＊ 从pipeline成功后的II角度考虑:所有for循环<code>pipeline</code>后的<code>II=1</code>。</li></ul><ul><li>硬件仿真结果<br><img src="http://xukeqiniu.xukeai.cn/513370e7eff99b2da9e41d14edfe265d.png" alt=""></li></ul><ul><li>硬件实现结果</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>到此为止，我们已经完整的分析了一个SDAceel的例子。从CPU实现，再到FPGA实现，并一步一步进行优化设计。在FPGA的优化中主要包括两种优化方向：其一是基于带宽（<code>Bandwidth</code>）和数据吞吐率(<code>Throughput</code>)的优化；其二是基于计算性能(<code>Performance</code>)的优化。通常在设计的过程中，新手往往只会考虑到计算性能的优化，不断地提高并行度，但是在提高并行度的过程中，往往会导致带宽严重不足，数据吞吐率低，进一步使得并行的效果受到带宽的限制，以至于无法实现并行的效果。在实现矩阵乘法的例子中，我们首先做了基本的功能实现，紧接着对于<code>Local Memory</code>和<code>Burst Write/Read</code>的优化就是在提高访存效率，进而提高数据吞吐率。最后关于指令的优化，才是对于计算性能上的优化。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Xilinx/SDAccel_Examples/tree/master/getting_started/cpu_to_fpga" target="_blank" rel="noopener">xilinx github Xilinx/SDAccel_Examples/cpu_to_fpga</a><br>ug1253 SDx Pragma Reference Guide 2017.2<br>ug1207 SDAccel Environment Optmizaton Guide</p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> 综合案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> 综合案例 </tag>
            
            <tag> 矩阵乘法 </tag>
            
            <tag> Local Memory </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SDAccel矩阵乘法优化（三）</title>
      <link href="/2017/11/24/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%963/"/>
      <url>/2017/11/24/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%963/</url>
      <content type="html"><![CDATA[<blockquote class="blockquote-center"><br><img src="http://xukeqiniu.xukeai.cn/78a27d3a0dfbdabcbb72001593a6d2d0.png" alt=""><br>从一个矩阵乘法的例子一步一步进行功能设计与性能优化。<br><a href="https://xuke225.github.io/2017/11/22/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%961/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（一）</a><br><a href="https://xuke225.github.io/2017/11/23/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%962/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（二）</a><br><a href="https://xuke225.github.io/2017/11/24/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%963/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（三）</a><br><a href="https://xuke225.github.io/2017/11/25/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%964/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（四）</a><br></blockquote><h3 id="mmult实现及优化步骤"><a href="#mmult实现及优化步骤" class="headerlink" title="mmult实现及优化步骤"></a>mmult实现及优化步骤</h3><table><thead><tr><th>步骤</th><th>实现功能</th><th>关键概念/ Keywords</th></tr></thead><tbody><tr><td>1、cpu实现</td><td>即在<code>host</code>端实现简单的矩阵乘法，便于比对数据与性能对比</td></tr><tr><td>2、OpenCL实现</td><td>在<code>device</code>端实现基于OpenCL的FPGA矩阵乘法硬件设计.</td><td><strong>Key</strong> <strong>Concepts</strong><br> - OpenCL APIs</td></tr><tr><td>3、加入<code>Local Memory</code></td><td>采用 <code>Local Memory</code> 减少<strong>数据访存</strong>次数</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Kernel Optimization<br> - Local Memory</td></tr><tr><td>4、实现读写的突发传输</td><td>采用突发传输的方式更好的实现<code>DDR</code>与 <code>Local Memory</code>数据的读写访问</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Kernel Optimization<br> - Burst Read/Write</td></tr><tr><td>5、数组分割</td><td>通过循环展开与数组分割的方式，实现更好的计算性能</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Array Partition<br> - Loop Unroll<br><strong>Keywords</strong><br> - xcl_pipeline_loop<br> - xcl_array_partition(complete, dim)<br> - opencl_unroll_hint</td></tr></tbody></table><a id="more"></a><h3 id="方案分析及优化思路二（Burst-Read-Write）"><a href="#方案分析及优化思路二（Burst-Read-Write）" class="headerlink" title="方案分析及优化思路二（Burst Read/Write）"></a>方案分析及优化思路二（Burst Read/Write）</h3><p>承接第二篇<code>Local Memory</code>的实现方法进一步进行优化处理，主要解决<code>gmem carry dependency</code>的问题。在这里，不采用<code>Max Memory Ports</code>的方法，因为采用多个接口灰消耗大量的LUT资源，并且大大的限制时钟频率的提升。其实，前面分析过了造成<code>gmem carry dependency</code>的原因，在矩阵乘法的实现过程中，我们完全可以将两个输入的数据分开，不需要在一个for循环中同时进行数据的读取而导致一个for循环在pipeline的过程中需要对两个接口进行读取，进而实现了Burst突发传输。</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_SIZE 64</span></span><br><span class="line"></span><br><span class="line">kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mmult</span><span class="params">( __global <span class="keyword">int</span>* in1,  <span class="comment">//Read-only input matrix1</span></span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span>* in2,  <span class="comment">//Read-only input matrix2</span></span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span>* out,  <span class="comment">//Output matrix</span></span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">int</span> dim             <span class="comment">//One dimension of the matrix</span></span></span></span><br><span class="line"><span class="function"><span class="params">          )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//Local memory to store input matrices</span></span><br><span class="line">    <span class="comment">//Local memory is implemented as BRAM memory blocks</span></span><br><span class="line">    __local <span class="keyword">int</span> local_in1[MAX_SIZE][MAX_SIZE];</span><br><span class="line">    __local <span class="keyword">int</span> local_in2[MAX_SIZE][MAX_SIZE];</span><br><span class="line">    __local <span class="keyword">int</span> local_out[MAX_SIZE][MAX_SIZE];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Burst reads on input matrices from DDR memory</span></span><br><span class="line">    <span class="comment">//Burst read for matrix local_in1 and local_in2</span></span><br><span class="line">    read_in1: <span class="keyword">for</span>(<span class="keyword">int</span> iter = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; iter &lt; dim * dim; iter++, j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j == dim)&#123; j = <span class="number">0</span>; i++; &#125;</span><br><span class="line">        local_in1[i][j] = in1[iter];</span><br><span class="line">    &#125;</span><br><span class="line">    read_in2: <span class="keyword">for</span>(<span class="keyword">int</span> iter = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; iter &lt; dim * dim; iter++, j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j == dim)&#123; j = <span class="number">0</span>; i++; &#125;</span><br><span class="line">        local_in2[i][j] = in2[iter];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Reads the input_data from local memory, performs the computations</span></span><br><span class="line">    <span class="comment">//and writes the data to local memory</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dim; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; dim; j++)&#123;</span><br><span class="line">            local_out[i][j] = <span class="number">0</span>;</span><br><span class="line">            write_data: <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; dim; k++)&#123;</span><br><span class="line">                local_out[i][j] += local_in1[i][k] * local_in2[k][ j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Burst write from local_out to DDR memory</span></span><br><span class="line">    write_out: <span class="keyword">for</span>(<span class="keyword">int</span> iter = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; iter &lt; dim * dim; iter++, j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j == dim)&#123; j = <span class="number">0</span>; i++; &#125;</span><br><span class="line">        out[iter] = local_out[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h4><ul><li>vivado hls log文件分析</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">WARNING: [XFORM 203-542] Cannot flatten a loop nest &apos;Loop-3.1&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:132:22) in function &apos;mmult&apos; :</span><br><span class="line">WARNING: [XFORM 203-542] the outer loop is not a perfect loop.</span><br><span class="line">INFO: [XFORM 203-541] Flattening a loop nest &apos;Loop-3&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:131:18) in function &apos;mmult&apos;.</span><br><span class="line">INFO: [XFORM 203-811] Inferring bus burst read of variable length on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:122:9).</span><br><span class="line">INFO: [XFORM 203-811] Inferring bus burst read of variable length on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:126:9).</span><br><span class="line">INFO: [XFORM 203-811] Inferring bus burst write of variable length on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:143:9).</span><br><span class="line">INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:00.79 ; elapsed = 00:00:00.89 . Memory (MB): peak = 494.320 ; gain = 156.758 ; free physical = 19833 ; free virtual = 45208</span><br><span class="line">INFO: [HLS 200-10] Starting hardware synthesis ...</span><br><span class="line">INFO: [HLS 200-10] Synthesizing &apos;mmult&apos; ...</span><br><span class="line">WARNING: [SYN 201-107] Renaming port name &apos;mmult/out&apos; to &apos;mmult/out_r&apos; to avoid the conflict with HDL keywords or other object names.</span><br><span class="line">INFO: [HLS 200-10] ----------------------------------------------------------------</span><br><span class="line">INFO: [HLS 200-42] -- Implementing module &apos;mmult&apos;</span><br><span class="line">INFO: [HLS 200-10] ----------------------------------------------------------------</span><br><span class="line">INFO: [SCHED 204-11] Starting scheduling ...</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;read_in1&apos;.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 1, Depth: 3.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;read_in2&apos;.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 1, Depth: 3.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;write_data&apos;.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 1, Depth: 8.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;write_out&apos;.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 1, Depth: 4.</span><br><span class="line">INFO: [SCHED 204-11] Finished scheduling.</span><br></pre></td></tr></table></figure><ul><li>HLS Report</li></ul><p><img src="http://xukeqiniu.xukeai.cn/bcad78af9618b750611d8b90c59fda4c.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/f69d0874e55c442f9cdc404cf86be084.png" alt=""></p><ul><li>综合结果分析<br>＊ 首先，硬件代码没有优化指令，不需要关注指令是否实现。<br>＊ 然后，相比于<code>Local Memory</code>版本的矩阵乘法实现，<code>Burst Read/Write</code>的实现方式主要是将两个原本在一个循环体内的输入切分到两个for循环中分开读入。从<code>Pipleline</code>的角度考虑：第一段for循环<code>pipeline</code>成功；第二段的for循环只有<code>write_data</code>的for循环成功，最外层的两个for循环成功完成<code>flatten</code>但是<code>write_data</code>与次外层的for循环因为含有<code>LOOP BODY</code>的原因，无法成功<code>flatten</code>，因此也无法完成整体的<code>pipeline</code>；第三段for循环<code>pipeline</code>成功。<br>＊ 从pipeline成功后的II角度考虑:第一段for循环<code>pipeline</code>后的<code>II=1</code>,解决了之前 <code>gmem carry dependency</code>的问题;第二三段for循环<code>pipeline</code>后的<code>II=1</code>。</li></ul><ul><li>硬件仿真结果</li></ul><p><img src="http://xukeqiniu.xukeai.cn/4fc2106b796d24656f67f7fb82c07668.png" alt=""></p><ul><li>硬件实现结果</li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Xilinx/SDAccel_Examples/tree/master/getting_started/cpu_to_fpga" target="_blank" rel="noopener">xilinx github Xilinx/SDAccel_Examples/cpu_to_fpga</a><br>ug1253 SDx Pragma Reference Guide 2017.2<br>ug1207 SDAccel Environment Optmizaton Guide</p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> 综合案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> 综合案例 </tag>
            
            <tag> 矩阵乘法 </tag>
            
            <tag> Burst </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SDAccel矩阵乘法优化（二）</title>
      <link href="/2017/11/23/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%962/"/>
      <url>/2017/11/23/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%962/</url>
      <content type="html"><![CDATA[<blockquote class="blockquote-center"><br><img src="http://xukeqiniu.xukeai.cn/78a27d3a0dfbdabcbb72001593a6d2d0.png" alt=""><br>从一个矩阵乘法的例子一步一步进行功能设计与性能优化。<br><a href="https://xuke225.github.io/2017/11/22/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%961/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（一）</a><br><a href="https://xuke225.github.io/2017/11/23/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%962/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（二）</a><br><a href="https://xuke225.github.io/2017/11/24/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%963/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（三）</a><br><a href="https://xuke225.github.io/2017/11/25/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%964/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（四）</a><br></blockquote><h3 id="mmult实现及优化步骤"><a href="#mmult实现及优化步骤" class="headerlink" title="mmult实现及优化步骤"></a>mmult实现及优化步骤</h3><table><thead><tr><th>步骤</th><th>实现功能</th><th>关键概念/ Keywords</th></tr></thead><tbody><tr><td>1、cpu实现</td><td>即在<code>host</code>端实现简单的矩阵乘法，便于比对数据与性能对比</td></tr><tr><td>2、OpenCL实现</td><td>在<code>device</code>端实现基于OpenCL的FPGA矩阵乘法硬件设计.</td><td><strong>Key</strong> <strong>Concepts</strong><br> - OpenCL APIs</td></tr><tr><td>3、加入<code>Local Memory</code></td><td>采用 <code>Local Memory</code> 减少<strong>数据访存</strong>次数</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Kernel Optimization<br> - Local Memory</td></tr><tr><td>4、实现读写的突发传输</td><td>采用突发传输的方式更好的实现<code>DDR</code>与 <code>Local Memory</code>数据的读写访问</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Kernel Optimization<br> - Burst Read/Write</td></tr><tr><td>5、数组分割</td><td>通过循环展开与数组分割的方式，实现更好的计算性能</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Array Partition<br> - Loop Unroll<br><strong>Keywords</strong><br> - xcl_pipeline_loop<br> - xcl_array_partition(complete, dim)<br> - opencl_unroll_hint</td></tr></tbody></table><a id="more"></a><h3 id="方案分析及优化思路一（Local-Memory）"><a href="#方案分析及优化思路一（Local-Memory）" class="headerlink" title="方案分析及优化思路一（Local Memory）"></a>方案分析及优化思路一（Local Memory）</h3><p>首先，我们先进行访存上的优化。原始版本的矩阵乘法实现虽然简单，但是在进行计算的过程中需要频繁的与DDR进行数据交互，但是DDR与FPGA进行交互的过程中是十分耗费时间与功耗的，因此，我们需要在FPGA上开一个局部的存储空间，先将数据从DDR搬运到FPGA片上的存储空间上，然后再进行计算，计算的过程数据在片上的空间进行索引，最后将计算完的数据再统一搬运回DDR上。这样，在片上的计算过程就不会频繁的受到DDR与FPGA访存慢的限制。</p><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_SIZE 64</span></span><br><span class="line"></span><br><span class="line">kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mmult</span><span class="params">( __global <span class="keyword">int</span>* in1,  <span class="comment">//Read-only input matrix1</span></span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span>* in2,  <span class="comment">//Read-only input matrix2</span></span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span>* out,  <span class="comment">//Output matrix</span></span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">int</span> dim             <span class="comment">//One dimension of the matrix</span></span></span></span><br><span class="line"><span class="function"><span class="params">          )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//Local memory to store input matrices</span></span><br><span class="line">    <span class="comment">//Local memory is implemented as BRAM memory blocks</span></span><br><span class="line">    <span class="comment">//MAX_SIZE * MAX_SIZE buffer is created because the size</span></span><br><span class="line">    <span class="comment">//need to be known at compile time</span></span><br><span class="line">    __local <span class="keyword">int</span> local_in1[MAX_SIZE][MAX_SIZE];</span><br><span class="line">    __local <span class="keyword">int</span> local_in2[MAX_SIZE][MAX_SIZE];</span><br><span class="line">    __local <span class="keyword">int</span> local_out[MAX_SIZE][MAX_SIZE];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Read the input data from DDR memory to local memory</span></span><br><span class="line">    read_in1: <span class="keyword">for</span>(<span class="keyword">int</span> iter = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; iter &lt; dim * dim; iter++, j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j == dim)&#123; j = <span class="number">0</span>; i++; &#125;</span><br><span class="line">        local_in1[i][j] = in1[iter];</span><br><span class="line">        local_in2[i][j] = in2[iter];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Reads the input_data from local memory, performs the computations</span></span><br><span class="line">    <span class="comment">//and writes the data to local memory</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dim; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; dim; j++)&#123;</span><br><span class="line">            local_out[i][j] = <span class="number">0</span>;</span><br><span class="line">            write_data: <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; dim; k++)&#123;</span><br><span class="line">                local_out[i][j] += local_in1[i][k] * local_in2[k][ j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Write the data from local memory to DDR memory</span></span><br><span class="line">    write_out: <span class="keyword">for</span>(<span class="keyword">int</span> iter = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>; iter &lt; dim * dim; iter++, j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(j == dim)&#123; j = <span class="number">0</span>; i++; &#125;</span><br><span class="line">        out[iter] = local_out[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h4><ul><li>vivado hls log文件分析</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">WARNING: [XFORM 203-542] Cannot flatten a loop nest &apos;Loop-2.1&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:86:22) in function &apos;mmult&apos; :</span><br><span class="line">WARNING: [XFORM 203-542] the outer loop is not a perfect loop.</span><br><span class="line">INFO: [XFORM 203-541] Flattening a loop nest &apos;Loop-2&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:85:18) in function &apos;mmult&apos;.</span><br><span class="line">INFO: [XFORM 203-811] Inferring bus burst write of variable length on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:97:9).</span><br><span class="line">INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:00.79 ; elapsed = 00:00:00.88 . Memory (MB): peak = 494.316 ; gain = 156.758 ; free physical = 19901 ; free virtual = 45272</span><br><span class="line">INFO: [HLS 200-10] Starting hardware synthesis ...</span><br><span class="line">INFO: [HLS 200-10] Synthesizing &apos;mmult&apos; ...</span><br><span class="line">WARNING: [SYN 201-107] Renaming port name &apos;mmult/out&apos; to &apos;mmult/out_r&apos; to avoid the conflict with HDL keywords or other object names.</span><br><span class="line">INFO: [HLS 200-10] ----------------------------------------------------------------</span><br><span class="line">INFO: [HLS 200-42] -- Implementing module &apos;mmult&apos;</span><br><span class="line">INFO: [HLS 200-10] ----------------------------------------------------------------</span><br><span class="line">INFO: [SCHED 204-11] Starting scheduling ...</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;read_in1&apos;.</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 1)</span><br><span class="line">   between bus request on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:80) and bus request on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:79).</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 2, Depth: 138.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;write_data&apos;.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 1, Depth: 8.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;write_out&apos;.</span><br><span class="line">INFO: [SCHED 204-61] Pipelining result: Target II: 1, Final II: 1, Depth: 4.</span><br><span class="line">INFO: [SCHED 204-11] Finished scheduling.</span><br></pre></td></tr></table></figure><ul><li>HLS Report<br><img src="http://xukeqiniu.xukeai.cn/204c361338d39d9a3ce57f7a94347ebd.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/7da81db7839a827c1ee82b665dfb0d9d.png" alt=""></li><li>综合结果分析</li></ul><p>＊ 首先，硬件代码没有优化指令，不需要关注指令是否实现。<br>＊ 然后，相比于原始版本的矩阵乘法实现，<code>Local Memory</code>的实现方式首先将整体的代码风格进行了调整，切分成三段并列的<code>for</code>循环形式。从<code>Pipleline</code>的角度考虑：第一段for循环<code>pipeline</code>成功；第二段的for循环只有<code>write_data</code>的for循环成功，最外层的两个for循环成功完成<code>flatten</code>但是<code>write_data</code>与次外层的for循环因为含有<code>LOOP BODY</code>的原因，无法成功<code>flatten</code>，因此也无法完成整体的<code>pipeline</code>；第三段for循环<code>pipeline</code>成功。<br>＊ 从pipeline成功后的II角度考虑:第一段for循环<code>pipeline</code>后的<code>II=2</code>,原因依然是 <code>gmem carry dependency</code>;第二三段for循环<code>pipeline</code>后的<code>II=1</code>。</p><ul><li>硬件仿真结果</li></ul><p><img src="http://xukeqiniu.xukeai.cn/18831f5a85127d53a257ecb44dcb6e59.png" alt=""></p><ul><li>硬件实现结果</li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Xilinx/SDAccel_Examples/tree/master/getting_started/cpu_to_fpga" target="_blank" rel="noopener">xilinx github Xilinx/SDAccel_Examples/cpu_to_fpga</a><br>ug1253 SDx Pragma Reference Guide 2017.2<br>ug1207 SDAccel Environment Optmizaton Guide</p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> 综合案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> 综合案例 </tag>
            
            <tag> 矩阵乘法 </tag>
            
            <tag> Local Memory </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SDAccel矩阵乘法优化（一）</title>
      <link href="/2017/11/22/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%961/"/>
      <url>/2017/11/22/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%961/</url>
      <content type="html"><![CDATA[<blockquote class="blockquote-center"><br><img src="http://xukeqiniu.xukeai.cn/78a27d3a0dfbdabcbb72001593a6d2d0.png" alt=""><br>从一个矩阵乘法的例子一步一步进行功能设计与性能优化。<br><a href="https://xuke225.github.io/2017/11/22/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%961/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（一）</a><br><a href="https://xuke225.github.io/2017/11/23/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%962/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（二）</a><br><a href="https://xuke225.github.io/2017/11/24/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%963/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（三）</a><br><a href="https://xuke225.github.io/2017/11/25/SDAccel/%E4%BC%98%E5%8C%96/%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B/mmult%E4%BC%98%E5%8C%964/" target="_blank" rel="noopener">SDAccel矩阵乘法优化（四）</a><br></blockquote><h3 id="mmult实现及优化步骤"><a href="#mmult实现及优化步骤" class="headerlink" title="mmult实现及优化步骤"></a>mmult实现及优化步骤</h3><table><thead><tr><th>步骤</th><th>实现功能</th><th>关键概念/ Keywords</th></tr></thead><tbody><tr><td>1、cpu实现</td><td>即在<code>host</code>端实现简单的矩阵乘法，便于比对数据与性能对比</td></tr><tr><td>2、OpenCL实现</td><td>在<code>device</code>端实现基于OpenCL的FPGA矩阵乘法硬件设计.</td><td><strong>Key</strong> <strong>Concepts</strong><br> - OpenCL APIs</td></tr><tr><td>3、加入<code>Local Memory</code></td><td>采用 <code>Local Memory</code> 减少<strong>数据访存</strong>次数</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Kernel Optimization<br> - Local Memory</td></tr><tr><td>4、实现读写的突发传输</td><td>采用突发传输的方式更好的实现<code>DDR</code>与 <code>Local Memory</code>数据的读写访问</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Kernel Optimization<br> - Burst Read/Write</td></tr><tr><td>5、数组分割</td><td>通过循环展开与数组分割的方式，实现更好的计算性能</td><td><strong>Key</strong> <strong>Concepts</strong><br> - Array Partition<br> - Loop Unroll<br><strong>Keywords</strong><br> - xcl_pipeline_loop<br> - xcl_array_partition(complete, dim)<br> - opencl_unroll_hint</td></tr></tbody></table><a id="more"></a><h3 id="CPU端实现mmult计算"><a href="#CPU端实现mmult计算" class="headerlink" title="CPU端实现mmult计算"></a>CPU端实现mmult计算</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mmult_cpu</span><span class="params">( <span class="keyword">int</span> *in1,   <span class="comment">// Input matrix 1</span></span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> *in2,   <span class="comment">// Input matrix 2</span></span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> *out,   <span class="comment">// Output matrix (out = A x B)</span></span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> dim     <span class="comment">// Matrix size of one dimension</span></span></span></span><br><span class="line"><span class="function"><span class="params">              )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//Performs matrix multiplication out = in1 x in2</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dim; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; dim; j++)&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; dim; k++)&#123;</span><br><span class="line">                out[i * dim + j] += in1[i * dim + k] * in2[k * dim  + j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="FPGA端实现mmult计算"><a href="#FPGA端实现mmult计算" class="headerlink" title="FPGA端实现mmult计算"></a>FPGA端实现mmult计算</h3><h4 id="OpenCL-Host端初始化流程"><a href="#OpenCL-Host端初始化流程" class="headerlink" title="OpenCL Host端初始化流程"></a>OpenCL Host端初始化流程</h4><p><img src="http://xukeqiniu.xukeai.cn/OpenCL%20Host%E7%AB%AF%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B.png" alt="OpenCL初始化"></p><h4 id="host-端代码实现"><a href="#host-端代码实现" class="headerlink" title="host 端代码实现"></a>host 端代码实现</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//OpenCL utility layer include</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"xcl2.hpp"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//Array Size to access</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> DATA_SIZE 64</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">uint64_t</span> get_duration_ns (<span class="keyword">const</span> cl::Event &amp;event) &#123;</span><br><span class="line">    <span class="keyword">uint64_t</span> nstimestart, nstimeend;</span><br><span class="line">    event.getProfilingInfo&lt;<span class="keyword">uint64_t</span>&gt;(CL_PROFILING_COMMAND_START,&amp;nstimestart);</span><br><span class="line">    event.getProfilingInfo&lt;<span class="keyword">uint64_t</span>&gt;(CL_PROFILING_COMMAND_END,&amp;nstimeend);</span><br><span class="line">    <span class="keyword">return</span>(nstimeend-nstimestart);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//CPU implementation of Matrix Multiplication</span></span><br><span class="line"><span class="comment">//The inputs are of the size (DATA_SIZE x DATA_SIZE)</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mmult_cpu</span> <span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *in1,   <span class="comment">//Input Matrix 1</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *in2,   <span class="comment">//Input Matrix 1</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> *out,   <span class="comment">//Input Matrix 1</span></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> dim     <span class="comment">//One dimension of matrix</span></span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//Performs Matrix multiply Out = In1 x In2</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dim; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; dim; j++) &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; dim; k++) &#123;</span><br><span class="line">                out[i * dim + j] += in1[i * dim + k] * in2[k * dim + j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Functionality to setup OpenCL context and trigger the Kernel</span></span><br><span class="line"><span class="keyword">uint64_t</span> mmult_fpga (</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt;&amp; source_in1,   <span class="comment">//Input Matrix 1</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt;&amp; source_in2,   <span class="comment">//Input Matrix 2</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt;&amp; source_fpga_results,    <span class="comment">//Output Matrix</span></span><br><span class="line">    <span class="keyword">int</span> dim                                                <span class="comment">//One dimension of matrix</span></span><br><span class="line">)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> size = dim;</span><br><span class="line">    <span class="keyword">size_t</span> matrix_size_bytes = <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * size * size;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//The get_xil_devices will return vector of Xilinx Devices</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;cl::Device&gt; devices = xcl::get_xil_devices();</span><br><span class="line">    cl::Device device = devices[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Creating Context and Command Queue for selected Device</span></span><br><span class="line">    cl::<span class="function">Context <span class="title">context</span><span class="params">(device)</span></span>;</span><br><span class="line">    cl::<span class="function">CommandQueue <span class="title">q</span><span class="params">(context, device, CL_QUEUE_PROFILING_ENABLE)</span></span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> device_name = device.getInfo&lt;CL_DEVICE_NAME&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//import_binary() command will find the OpenCL binary file created using the</span></span><br><span class="line">    <span class="comment">//xocc compiler load into OpenCL Binary and return as Binaries</span></span><br><span class="line">    <span class="comment">//OpenCL and it can contain many functions which can be executed on the</span></span><br><span class="line">    <span class="comment">//device.</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> binaryFile = xcl::find_binary_file(device_name,<span class="string">"mmult"</span>);</span><br><span class="line">    cl::Program::Binaries bins = xcl::import_binary_file(binaryFile);</span><br><span class="line">    devices.resize(<span class="number">1</span>);</span><br><span class="line">    cl::<span class="function">Program <span class="title">program</span><span class="params">(context, devices, bins)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//This call will extract a kernel out of the program we loaded in the</span></span><br><span class="line">    <span class="comment">//previous line. A kernel is an OpenCL function that is executed on the</span></span><br><span class="line">    <span class="comment">//FPGA. This function is defined in the src/mmult.cl file.</span></span><br><span class="line">    cl::<span class="function">Kernel <span class="title">kernel</span><span class="params">(program,<span class="string">"mmult"</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//These commands will allocate memory on the FPGA. The cl::Buffer</span></span><br><span class="line">    <span class="comment">//objects can be used to reference the memory locations on the device.</span></span><br><span class="line">    <span class="comment">//The cl::Buffer object cannot be referenced directly and must be passed</span></span><br><span class="line">    <span class="comment">//to other OpenCL functions.</span></span><br><span class="line">    cl::<span class="function">Buffer <span class="title">buffer_in1</span><span class="params">(context,CL_MEM_USE_HOST_PTR | CL_MEM_READ_ONLY,</span></span></span><br><span class="line">            matrix_size_bytes,source_in1.data());</span><br><span class="line">    cl::<span class="function">Buffer <span class="title">buffer_in2</span><span class="params">(context,CL_MEM_USE_HOST_PTR | CL_MEM_READ_ONLY,</span></span></span><br><span class="line">            matrix_size_bytes,source_in2.data());</span><br><span class="line">    cl::<span class="function">Buffer <span class="title">buffer_output</span><span class="params">(context,CL_MEM_USE_HOST_PTR | CL_MEM_WRITE_ONLY,</span></span></span><br><span class="line">            matrix_size_bytes,source_fpga_results.data());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//These commands will load the source_in1 and source_in2 vectors from the host</span></span><br><span class="line">    <span class="comment">//application into the buffer_in1 and buffer_in2 cl::Buffer objects. The data</span></span><br><span class="line">    <span class="comment">//will be be transferred from system memory over PCIe to the FPGA on-board</span></span><br><span class="line">    <span class="comment">//DDR memory.</span></span><br><span class="line">    q.enqueueMigrateMemObjects(&#123;buffer_in1, buffer_in2&#125;,<span class="number">0</span><span class="comment">/* 0 means from host*/</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Set the kernel arguments</span></span><br><span class="line">    <span class="keyword">int</span> narg = <span class="number">0</span>;</span><br><span class="line">    kernel.setArg(narg++, buffer_in1);</span><br><span class="line">    kernel.setArg(narg++, buffer_in2);</span><br><span class="line">    kernel.setArg(narg++, buffer_output);</span><br><span class="line">    kernel.setArg(narg++, size);</span><br><span class="line"></span><br><span class="line">    cl::Event event;</span><br><span class="line">    <span class="keyword">uint64_t</span> kernel_duration = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Launch the kernel</span></span><br><span class="line">    q.enqueueTask(kernel, <span class="literal">NULL</span>, &amp;event);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//The result of the previous kernel execution will need to be retrieved in</span></span><br><span class="line">    <span class="comment">//order to view the results. This call will write the data from the</span></span><br><span class="line">    <span class="comment">//buffer_output cl_mem object to the source_fpga_results vector</span></span><br><span class="line">    q.enqueueMigrateMemObjects(&#123;buffer_output&#125;,CL_MIGRATE_MEM_OBJECT_HOST);</span><br><span class="line">    q.finish();</span><br><span class="line"></span><br><span class="line">    kernel_duration = get_duration_ns(event);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> kernel_duration;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//Allocate Memory in Host Memory</span></span><br><span class="line">    <span class="keyword">int</span> size = DATA_SIZE;</span><br><span class="line">    <span class="keyword">size_t</span> matrix_size_bytes = <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * size * size;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//When creating a buffer with user pointer, under the hood user ptr is</span></span><br><span class="line">    <span class="comment">//used if and only if it is properly aligned (page aligned). When not</span></span><br><span class="line">    <span class="comment">//aligned, runtime has no choice but to create its own host side buffer</span></span><br><span class="line">    <span class="comment">//that backs user ptr. This in turn implies that all operations that move</span></span><br><span class="line">    <span class="comment">//data to/from device incur an extra memcpy to move data to/from runtime's</span></span><br><span class="line">    <span class="comment">//own host buffer from/to user pointer. So it is recommended to use this</span></span><br><span class="line">    <span class="comment">//allocator if user wish to Create Buffer/Memory Object to align user buffer</span></span><br><span class="line">    <span class="comment">//to the page boundary. It will ensure that user buffer will be used when</span></span><br><span class="line">    <span class="comment">//user create Buffer/Mem Object.</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt; source_in1(matrix_size_bytes);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt; source_in2(matrix_size_bytes);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt; source_fpga_results(matrix_size_bytes);</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt; source_cpu_results(matrix_size_bytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Create the test data</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; DATA_SIZE * DATA_SIZE ; i++)&#123;</span><br><span class="line">        source_in1[i] = i;</span><br><span class="line">        source_in2[i] = i * i;</span><br><span class="line">        source_cpu_results[i] = <span class="number">0</span>;</span><br><span class="line">        source_fpga_results[i] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">uint64_t</span> kernel_duration = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Compute CPU Results</span></span><br><span class="line">    mmult_cpu(source_in1.data(), source_in2.data(), source_cpu_results.data(), size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Compute FPGA Results</span></span><br><span class="line">    kernel_duration = mmult_fpga(source_in1, source_in2, source_fpga_results, size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Compare the results of FPGA to CPU</span></span><br><span class="line">    <span class="keyword">bool</span> match = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; size * size; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (source_fpga_results[i] != source_cpu_results[i])&#123;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Error: Result mismatch"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"i = "</span> &lt;&lt; i &lt;&lt; <span class="string">" CPU result = "</span> &lt;&lt; source_cpu_results[i]</span><br><span class="line">                &lt;&lt; <span class="string">" FPGA result = "</span> &lt;&lt; source_fpga_results[i] &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">            match = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"TEST "</span> &lt;&lt; (match ? <span class="string">"PASSED"</span> : <span class="string">"FAILED"</span>) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Wall Clock Time (Kernel execution): "</span> &lt;&lt; kernel_duration &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Note: Wall Clock Time is meaningful for real hardware execution only,"</span></span><br><span class="line">            &lt;&lt; <span class="string">"not for emulation."</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (match ? EXIT_SUCCESS :  EXIT_FAILURE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="device端代码实现（简单实现mmult逻辑）"><a href="#device端代码实现（简单实现mmult逻辑）" class="headerlink" title="device端代码实现（简单实现mmult逻辑）"></a>device端代码实现（简单实现mmult逻辑）</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mmult</span><span class="params">( __global <span class="keyword">int</span>* in1,  <span class="comment">//Read-only input matrix1</span></span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span>* in2,  <span class="comment">//Read-only input matrix2</span></span></span></span><br><span class="line"><span class="function"><span class="params">            __global <span class="keyword">int</span>* out,  <span class="comment">//Output matrix</span></span></span></span><br><span class="line"><span class="function"><span class="params">            <span class="keyword">int</span> dim             <span class="comment">//One dimension of the matrix</span></span></span></span><br><span class="line"><span class="function"><span class="params">          )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//Reads the data from DDR, performs the computation</span></span><br><span class="line">    <span class="comment">//and writes back the result to DDR.</span></span><br><span class="line">    LOOP1：<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; dim ; i++)&#123;</span><br><span class="line">        LOOP2：<span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; dim; j++)&#123;</span><br><span class="line">                   out[i * dim + j] = <span class="number">0</span>;</span><br><span class="line">            LOOP3：<span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; dim; k++)&#123;</span><br><span class="line">                       out[i * dim + j] += in1[i * dim + k] * in2[k * dim + j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h4><ul><li>vivado hls log文件分析(<span id="inline-red">重点关注WARNING</span>)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">WARNING: [XFORM 203-542] Cannot flatten a loop nest &apos;LOOP2&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:47:44) in function &apos;mmult&apos; :</span><br><span class="line">WARNING: [XFORM 203-542] the outer loop is not a perfect loop.</span><br><span class="line">INFO: [XFORM 203-541] Flattening a loop nest &apos;LOOP1&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:45:43) in function &apos;mmult&apos;.</span><br><span class="line">INFO: [HLS 200-111] Finished Architecture Synthesis Time (s): cpu = 00:00:00.77 ; elapsed = 00:00:00.88 . Memory (MB): peak = 494.320 ; gain = 156.758 ; free physical = 19872 ; free virtual = 45217</span><br><span class="line">INFO: [HLS 200-10] Starting hardware synthesis ...</span><br><span class="line">INFO: [HLS 200-10] Synthesizing &apos;mmult&apos; ...</span><br><span class="line">WARNING: [SYN 201-107] Renaming port name &apos;mmult/out&apos; to &apos;mmult/out_r&apos; to avoid the conflict with HDL keywords or other object names.</span><br><span class="line">INFO: [HLS 200-10] ----------------------------------------------------------------</span><br><span class="line">INFO: [HLS 200-42] -- Implementing module &apos;mmult&apos;</span><br><span class="line">INFO: [HLS 200-10] ----------------------------------------------------------------</span><br><span class="line">INFO: [SCHED 204-11] Starting scheduling ...</span><br><span class="line">INFO: [SCHED 204-61] Pipelining loop &apos;LOOP3&apos;.</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 1, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 2, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 3, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 4, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 130, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 193, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 225, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 241, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 249, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 253, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 255, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">WARNING: [SCHED 204-68] Unable to enforce a carried dependence constraint (II = 256, distance = 1, offset = 0)</span><br><span class="line">   between &apos;add&apos; operation (&apos;tmp_13&apos;, /home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51) and bus write on port &apos;gmem&apos; (/home/lab611/workspace/xuke/mmult_test/src/mmult.cl:51).</span><br><span class="line">INFO: [SCHED 204-61] Unable to satisfy pipeline directive: Unable to pipeline the region.</span><br><span class="line">INFO: [SCHED 204-11] Finished scheduling.</span><br></pre></td></tr></table></figure><ul><li>HLS Report</li></ul><p><img src="http://xukeqiniu.xukeai.cn/6ce926b1698a23ef7495a0bf7ac2676d.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/9fd0edfee83d73464d03fcbc8ba11d0b.png" alt=""></p><ul><li>综合结果分析</li></ul><p><span id="inline-red">分析综合结果的方法：</span><br>＊ 首先分析对于添加的优化指令是否综合实现，若不能实现，原因是什么？<br>＊ 然后分析代码<code>pipeline</code>的情况。SDAccel对于嵌套的for循环来讲：<span id="inline-blue">pipeline内层的for循环全部unroll，pipeline外层的for循环试图进行Flattening，Flatten成功则统一到一个pipeline中。</span><br>＊ 对于<code>pipeline</code>的循环进一步分析II值是多少，理论能优化到多少？</p><p>从上述日志分析可知，该硬件的综合实现有很多问题：<br>＊ 首先，硬件代码没有优化指令，不需要关注指令是否实现。<br>＊ 然后，对于实现的三层for循环，只是实现了最内层<code>LOOP3</code>循环的<code>pipeline</code>，中间层未实现<code>Flatten</code>的原因是：<code>the outer loop is not a perfect loop.</code>。而<code>LOOP2</code>向<code>LOOP1</code>继续试图进行<code>Flattening</code>,成功则<code>LOOP2</code>与<code>LOOP1</code>统一为<code>LOOP1_LOOP2</code>。一般情况下对于Flattening不成功的原因有两种：一种是外层for循环中夹杂内层for循环的结构；另一种是内层for循环的循环边界是变量。具体循环的类型如下图所示。所以此例中<code>LOOP2</code>不能与<code>LOOP3</code>实现<code>Flatten</code>的原因是前者。也就是在<code>LOOP2</code>循环体中有<code>out[i * dim + j] = 0;</code>操作，而<code>out</code>数组在内层<code>LOOP3</code>中同样用到。反过来说，假如说编译器对<code>LOOP2</code>与<code>LOOP3</code>进行<code>Flatten</code>，那么对于<code>out[i * dim + j] = 0</code>操作在同一个循环中将不知如何与内部的循环体进行融合。<br><img src="http://xukeqiniu.xukeai.cn/9fcff7ae5de5f6541839db30cee5a1c7.png" alt="loop nest class"><br>＊ 最后对于试图<code>Pipeline</code>的<code>LOOP3</code>进行<code>II</code>值的分析，从log文件中可知II值过大，以至于无法进行<code>Pipeline</code>,原因是产生接口<code>gmem</code>的<code>carried dependence</code>。所以，所有的<code>loop</code>都未能实现<code>pipeline</code>。<br>关于<code>gmem</code>的<code>carried dependence</code>问题可以关注我的另一篇文章 <a href="https://xuke225.github.io/2017/11/15/SDAccel/%E4%BC%98%E5%8C%96/kernel_to_gmem/gmem-carry-dependency-%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">gmem carry dependency 分析</a></p><ul><li>硬件仿真结果</li></ul><p><img src="http://xukeqiniu.xukeai.cn/a1d9c0cf9b615dd9ced9d8806c5df6bc.png" alt=""></p><ul><li>硬件实现结果</li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Xilinx/SDAccel_Examples/tree/master/getting_started/cpu_to_fpga" target="_blank" rel="noopener">xilinx github Xilinx/SDAccel_Examples/cpu_to_fpga</a><br>ug1253 SDx Pragma Reference Guide 2017.2<br>ug1207 SDAccel Environment Optmizaton Guide</p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> 综合案例 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> 综合案例 </tag>
            
            <tag> 矩阵乘法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>gmem carry dependency 分析</title>
      <link href="/2017/11/15/SDAccel/%E4%BC%98%E5%8C%96/kernel_to_gmem/gmem-carry-dependency-%E5%88%86%E6%9E%90/"/>
      <url>/2017/11/15/SDAccel/%E4%BC%98%E5%8C%96/kernel_to_gmem/gmem-carry-dependency-%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<h3 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h3><ul><li>方案一  源码</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> J_CNT       2</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> I_CNT       4</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUFFER_SIZE I_CNT*J_CNT</span></span><br><span class="line"></span><br><span class="line">kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(__global <span class="keyword">int</span>* c,</span></span></span><br><span class="line"><span class="function"><span class="params">               __global <span class="keyword">const</span> <span class="keyword">int</span>* a,</span></span></span><br><span class="line"><span class="function"><span class="params">               __global <span class="keyword">const</span> <span class="keyword">int</span>* b,</span></span></span><br><span class="line"><span class="function"><span class="params">               <span class="keyword">const</span> <span class="keyword">int</span> n_elements)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   local <span class="keyword">int</span> arrayA[BUFFER_SIZE];</span><br><span class="line">   local <span class="keyword">int</span> arrayB[BUFFER_SIZE];</span><br><span class="line">   local <span class="keyword">int</span> arrayC[BUFFER_SIZE];</span><br><span class="line"></span><br><span class="line">   __attribute__((xcl_pipeline_loop))</span><br><span class="line">loop_1:</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; I_CNT; i++) &#123;</span><br><span class="line">       <span class="comment">//__attribute__((xcl_pipeline_loop))</span></span><br><span class="line">       loop_2:</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; J_CNT; j++) &#123;</span><br><span class="line">           arrayA[i * J_CNT + j] = a[i * J_CNT + j];</span><br><span class="line">           arrayB[i * J_CNT + j] = b[i * J_CNT + j];</span><br><span class="line">           c[i * J_CNT + j] = arrayA[i * J_CNT + j]+arrayB[i * J_CNT + j];</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>方案一  kernel 设置</li></ul><p><img src="http://xukeqiniu.xukeai.cn/6761df957be09d56c49a393122fee9a3.png" alt=""></p><ul><li>方案一  综合结果</li></ul><p><img src="http://xukeqiniu.xukeai.cn/44244df7984202643cc6c31cda5b9b57.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/8641efad2576de07149a4a88f3a1ae03.png" alt=""></p><ul><li>方案一  HLS log 文件</li></ul><p><img src="http://xukeqiniu.xukeai.cn/050993521dabc3db276d2acdf9a666e5.png" alt=""></p><ul><li>方案一  Performance图</li></ul><p><img src="http://xukeqiniu.xukeai.cn/2c8d0ec01cd2286b9eaf77dff54c4316.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/0e9c17ba66f84433714e488fd3ce0f31.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/5ac4e9160997b061791ff6ccc7cd312f.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/9dc9c6e9d56d4468db807068b236994a.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/5dda1f895a647e9e45ccb6a00b4476e6.png" alt=""></p><h3 id="方案二（修改gmem位宽为64bit）"><a href="#方案二（修改gmem位宽为64bit）" class="headerlink" title="方案二（修改gmem位宽为64bit）"></a>方案二（修改gmem位宽为64bit）</h3><ul><li>方案二  kernel 设置</li></ul><p><img src="http://xukeqiniu.xukeai.cn/5dda1f895a647e9e45ccb6a00b4476e6.png" alt=""></p><ul><li>为何为64bit?<br>首先，源码中对外层<code>loop_1</code>进行pipeline,因此对于内层的<code>loop_2</code>自动进行uroll展开。内层for循环的边界为<code>J_CNT = 2</code>因此将<code>loop_2</code> 代码<br>展开为如下代码形式。因此需要对<code>a</code>和<code>b</code>进行两次读取，对<code>a</code>请求一次读入，对<code>b</code>请求一次读入。一拍只能对gmem请求一次，因为是并行执行因此<code>a</code>有两个数据<code>a[i * J_CNT + 0]</code> 和<code>a[i * J_CNT + 1]</code>需要读入所以接口位宽为<code>sizeof(int) * 2</code>。<br>对于代码来说<code>gmem</code>位宽应该等于<code>sizeof(int) * J_CNT</code></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// loop_2 uroll 展开过程</span></span><br><span class="line">    arrayA[i * J_CNT + <span class="number">0</span>] = a[i * J_CNT + <span class="number">0</span>];</span><br><span class="line">    arrayA[i * J_CNT + <span class="number">1</span>] = a[i * J_CNT + <span class="number">1</span>];</span><br><span class="line">    arrayB[i * J_CNT + <span class="number">0</span>] = b[i * J_CNT + <span class="number">0</span>];</span><br><span class="line">    arrayB[i * J_CNT + <span class="number">1</span>] = b[i * J_CNT + <span class="number">1</span>];</span><br></pre></td></tr></table></figure><ul><li>方案二  综合结果</li></ul><p><img src="http://xukeqiniu.xukeai.cn/7dd3ae852d90aca8b5afa8836c6b0848.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/56606e7b4da8d4cc13d267d7cbec198c.png" alt=""></p><ul><li>方案二  HLS log文件</li></ul><p><img src="http://xukeqiniu.xukeai.cn/937b1ca302f2ee702e4cecd83a63bea8.png" alt=""></p><ul><li>方案二  Performance图</li></ul><p><img src="http://xukeqiniu.xukeai.cn/18e958245b430f116c095fb59f21a6e7.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/4ca1ed6e8e0d08cd0e2e6a2f074abf94.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/acf0ff0829e335425ca144cff64031ee.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/acf0ff0829e335425ca144cff64031ee.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/e0408803736b1b7c7ee46344555f1d0b.png" alt=""></p><h3 id="方案二-II-2-原因分析"><a href="#方案二-II-2-原因分析" class="headerlink" title="方案二 II = 2 原因分析"></a>方案二 II = 2 原因分析</h3><ul><li>修改源码,解决gmem ii = 2 问题(屏蔽掉b向arrayB赋值)</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> J_CNT       2</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> I_CNT       4</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUFFER_SIZE I_CNT*J_CNT</span></span><br><span class="line"></span><br><span class="line">kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(__global <span class="keyword">int</span>* c,</span></span></span><br><span class="line"><span class="function"><span class="params">               __global <span class="keyword">const</span> <span class="keyword">int</span>* a,</span></span></span><br><span class="line"><span class="function"><span class="params">               __global <span class="keyword">const</span> <span class="keyword">int</span>* b,</span></span></span><br><span class="line"><span class="function"><span class="params">               <span class="keyword">const</span> <span class="keyword">int</span> n_elements)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   local <span class="keyword">int</span> arrayA[BUFFER_SIZE];</span><br><span class="line">   local <span class="keyword">int</span> arrayB[BUFFER_SIZE];</span><br><span class="line">   local <span class="keyword">int</span> arrayC[BUFFER_SIZE];</span><br><span class="line"></span><br><span class="line">   __attribute__((xcl_pipeline_loop))</span><br><span class="line">loop_1:</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; I_CNT; i++) &#123;</span><br><span class="line">       <span class="comment">//__attribute__((xcl_pipeline_loop))</span></span><br><span class="line">       loop_2:</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; J_CNT; j++) &#123;</span><br><span class="line">           arrayA[i * J_CNT + j] = a[i * J_CNT + j];</span><br><span class="line"><span class="comment">//            arrayB[i * J_CNT + j] = b[i * J_CNT + j];</span></span><br><span class="line">           c[i * J_CNT + j] = arrayA[i * J_CNT + j];<span class="comment">//+arrayB[i * J_CNT + j];</span></span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>综合结果分析</li></ul><p><img src="http://xukeqiniu.xukeai.cn/10f729b114b275541fcfc02fdb391bae.png" alt=""></p><ul><li>HLS log文件</li></ul><p><img src="http://xukeqiniu.xukeai.cn/5c773a40ac3464dd18c91f35c9852037.png" alt=""></p><ul><li>Performence图</li></ul><p><img src="http://xukeqiniu.xukeai.cn/d0ff83cd0c8288e0df9180bda08b3f68.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/0b746748e1f0ddca79d39be4956d2372.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/8db95363cb4843a7a2df6a8a139a212b.png" alt=""></p><ul><li>为何源程序 II = 2？</li></ul><p>该问题还要从<code>loop_2</code> unroll说起</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// loop_2 uroll 展开过程</span></span><br><span class="line">    arrayA[i * J_CNT + <span class="number">0</span>] = a[i * J_CNT + <span class="number">0</span>];</span><br><span class="line">    arrayA[i * J_CNT + <span class="number">1</span>] = a[i * J_CNT + <span class="number">1</span>];</span><br><span class="line">    arrayB[i * J_CNT + <span class="number">0</span>] = b[i * J_CNT + <span class="number">0</span>];</span><br><span class="line">    arrayB[i * J_CNT + <span class="number">1</span>] = b[i * J_CNT + <span class="number">1</span>];</span><br></pre></td></tr></table></figure><p>关键原因是<strong>一拍只能对<code>gmem</code>请求一次</strong> 因为是并行执行因此第一拍对<code>gmem</code>进行<code>a</code>的读请求，<code>a</code>有两个数据<code>a[i * J_CNT + 0]</code> 和<code>a[i * J_CNT + 1]</code>需要读入,同理下一拍对<code>gmem</code>进行<code>b</code>的请求，<code>b</code>有两个数据<code>b[i * J_CNT + 0]</code> 和<code>b[i * J_CNT + 1]</code>需要读入。<br>因此解决办法也不言而喻：一个<code>gmem</code>只能一次读请求，但是综合多个<code>gmem</code>便可以在一拍内分别进行请求！<br><img src="http://xukeqiniu.xukeai.cn/e15fb942130d0657331e4332e94a02dd.png" alt=""></p><h3 id="方案三（解决-II-2问题）"><a href="#方案三（解决-II-2问题）" class="headerlink" title="方案三（解决 II = 2问题）"></a>方案三（解决 II = 2问题）</h3><ul><li>方案三 源码</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> J_CNT       2</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> I_CNT       4</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUFFER_SIZE I_CNT*J_CNT</span></span><br><span class="line"></span><br><span class="line">kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(__global <span class="keyword">int</span>* c,</span></span></span><br><span class="line"><span class="function"><span class="params">               __global <span class="keyword">const</span> <span class="keyword">int</span>* a,</span></span></span><br><span class="line"><span class="function"><span class="params">               __global <span class="keyword">const</span> <span class="keyword">int</span>* b,</span></span></span><br><span class="line"><span class="function"><span class="params">               <span class="keyword">const</span> <span class="keyword">int</span> n_elements)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   local <span class="keyword">int</span> arrayA[BUFFER_SIZE];</span><br><span class="line">   local <span class="keyword">int</span> arrayB[BUFFER_SIZE];</span><br><span class="line">   local <span class="keyword">int</span> arrayC[BUFFER_SIZE];</span><br><span class="line"></span><br><span class="line">   __attribute__((xcl_pipeline_loop))</span><br><span class="line">loop_1:</span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; I_CNT; i++) &#123;</span><br><span class="line">       <span class="comment">//__attribute__((xcl_pipeline_loop))</span></span><br><span class="line">       loop_2:</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; J_CNT; j++) &#123;</span><br><span class="line">           arrayA[i * J_CNT + j] = a[i * J_CNT + j];</span><br><span class="line">           arrayB[i * J_CNT + j] = b[i * J_CNT + j];</span><br><span class="line">           c[i * J_CNT + j] = arrayA[i * J_CNT + j]+arrayB[i * J_CNT + j];</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>方案三 设置</li></ul><p><img src="http://xukeqiniu.xukeai.cn/e598125028eea15c73df4998ec157285.png" alt=""></p><ul><li>方案三 综合结果</li></ul><p><img src="http://xukeqiniu.xukeai.cn/439580baf77868142d0a9b7858ac7860.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/b3a0106b044dd3b1b7b268b0ee259611.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/103f16fb347355a4670592992b0173ed.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/81d167188d4f054e9e0f1061dae5c174.png" alt=""></p><ul><li>方案三 HLS log文件<br><img src="http://xukeqiniu.xukeai.cn/11a495df12fd1151547727906a48ae33.png" alt=""></li></ul><ul><li>方案三 Performence图<br><img src="http://xukeqiniu.xukeai.cn/6778fada1a100f886c7ff480c6c151a7.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/a952567ee07875ac7787e6b0d2774891.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/a5ed8c3f40c32a85238715b109ccc2eb.png" alt=""></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>产生gmem carry dependency的两种解决办法：<ul><li>进行 gmem位宽大小的调整 32bit – 512bit</li><li>多个 __global 参数采用多个gmem进行数据传输（max memory ports）</li></ul></li></ul><h3 id="TODO-采用pipe传输（做一个memRead）"><a href="#TODO-采用pipe传输（做一个memRead）" class="headerlink" title="TODO 采用pipe传输（做一个memRead）"></a>TODO 采用pipe传输（做一个memRead）</h3><ul><li>问题（当数据为非32bit的倍数-512bit）</li></ul>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> kernel_to_gmem </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> kernel_to_gmem </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>host overlap</title>
      <link href="/2017/11/10/SDAccel/%E4%BC%98%E5%8C%96/host/overlap/"/>
      <url>/2017/11/10/SDAccel/%E4%BC%98%E5%8C%96/host/overlap/</url>
      <content type="html"><![CDATA[<h2 id="例程描述"><a href="#例程描述" class="headerlink" title="例程描述"></a>例程描述</h2><div class="note default"><p>这个示例将演示在一个应用中用户在主机端（CPU）与FPGA交叠的运算,从而达到将数据传输隐藏在计算过程中，提高数据传输效率。其中包括异步操作和事件对象。</p></div><h2 id="主要学习知识点"><a href="#主要学习知识点" class="headerlink" title="主要学习知识点"></a>主要学习知识点</h2><div class="note primary"><ul><li>Key Concepts<ul><li>OpenCL API</li><li>Host 和 FPGA 同步 Synchronize Host and FPGA</li><li>处理过程异步 Asynchronous Processing</li><li>事件 Events</li><li>异步拷贝 Asynchronous memcpy</li><li>Double Buffer 乒乓</li><li>Burst Transfer 突发传输</li></ul></li><li>Keywords<ul><li>cl_event</li><li>CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE</li><li>clEnqueueMigrateMemObjects</li><li>clEnqueueMapBuffer</li></ul></li></ul></div><a id="more"></a><ul><li>clEnqueueReadBuffer VS clEnqueueWriteBuffer VS clEnqueueMapBuffer<ul><li>clEnqueueReadBuffer<ul><li>从Cl_mem读回host mem（就算Cl_mem是直接使用host mem实现的，想读它的内容，还是要这样读回来，可以看做cl_mem是更高一层封装）</li></ul></li><li>clEnqueueWriteBuffer<ul><li>使用host_mem的值写cl_mem</li></ul></li><li>clEnqueueMapBuffer<ul><li>在Cl_mem和host mem之间做映射</li><li>这个函数比较特殊,在创建buf时有一种方法CL_MEM_USE_HOST_PTR，是直接让device使用host上已有的一块的mem（p1）做buf，但是这个产生的CL_mem（p2）经过计算后值会改变,p2改变后通常p1不会被改变，因为虽然用的一块物理空间，但是cl_mem是高层封装，和host上的mem还是不一样的，要想使p1同步到p2的最新值，就要调用这句map</li></ul></li></ul></li></ul><h2 id="主机端代码分析"><a href="#主机端代码分析" class="headerlink" title="主机端代码分析"></a>主机端代码分析</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  Overlap Host Code</span></span><br><span class="line"><span class="comment">  There are many applications where all of the data cannot reside in an FPGA.</span></span><br><span class="line"><span class="comment">  For example, the data is too big to fit in an FPGA or the data is being</span></span><br><span class="line"><span class="comment">  streamed from a sensor or the network. In these situations data must be</span></span><br><span class="line"><span class="comment">  transferred to the host memory to the FPGA before the computation can be</span></span><br><span class="line"><span class="comment">  performed.</span></span><br><span class="line"><span class="comment">  Because PCIe is an full-duplex interconnect, you can transfer data to and from</span></span><br><span class="line"><span class="comment">  the FPGA simultaneously. Xilinx FPGAs can also perform computations during</span></span><br><span class="line"><span class="comment">  these data transfers. Performing all three of these operations at the same</span></span><br><span class="line"><span class="comment">  time allows you to keep the FPGA busy and take full advantage of all of the</span></span><br><span class="line"><span class="comment">  hardware on your system.</span></span><br><span class="line"><span class="comment">  In this example, we will demonstrate how to perform this using an out of order</span></span><br><span class="line"><span class="comment">  command queue.</span></span><br><span class="line"><span class="comment">  +---------+---------+---------+----------+---------+---------+---------</span></span><br><span class="line"><span class="comment">  | WriteA1 | WriteB1 | WriteA2 | Write B2 | WriteA1 | WriteB1 |   Wri...</span></span><br><span class="line"><span class="comment">  +---------+---------+---------+----------+---------+---------+---------</span></span><br><span class="line"><span class="comment">                      |       Compute1     |     Compute2      |  Compu...</span></span><br><span class="line"><span class="comment">                      +--------------------+-------------------+--------+</span></span><br><span class="line"><span class="comment">                                           | ReadC1 |          | ReadC2 |</span></span><br><span class="line"><span class="comment">                                           +--------+          +--------+</span></span><br><span class="line"><span class="comment">  Many OpenCL commands are asynchronous. This means that whenever you call an</span></span><br><span class="line"><span class="comment">  OpenCL function, the function will return before the operation has completed.</span></span><br><span class="line"><span class="comment">  Asynchronous nature of OpenCL allows you to simultaneously perform tasks on</span></span><br><span class="line"><span class="comment">  the host CPU as well as the FPGA.</span></span><br><span class="line"><span class="comment">  Memory transfer operations are asynchronous when the blocking_read,</span></span><br><span class="line"><span class="comment">  blocking_write parameters are set to CL_FALSE. These operations are behaving</span></span><br><span class="line"><span class="comment">  on host memory so it is important to make sure that the command has completed</span></span><br><span class="line"><span class="comment">  before that memory is used.</span></span><br><span class="line"><span class="comment">  You can make sure an operation has completed by querying events returned by</span></span><br><span class="line"><span class="comment">  these commands. Events are OpenCL objects that track the status of operations.</span></span><br><span class="line"><span class="comment">  Event objects are created by kernel execution commands, read, write, map, copy</span></span><br><span class="line"><span class="comment">  commands on memory objects or user events created using clCreateUserEvent.</span></span><br><span class="line"><span class="comment">  Events can be used to synchronize operations between the host thread and the</span></span><br><span class="line"><span class="comment">  device or between two operations in the same context. You can also use events</span></span><br><span class="line"><span class="comment">  to time a particular operation if the command queue was created using the</span></span><br><span class="line"><span class="comment">  CL_QUEUE_PROFILING_ENABLE flag.</span></span><br><span class="line"><span class="comment">  Most enqueuing commands return events by accepting a cl_event pointer as their</span></span><br><span class="line"><span class="comment">  last argument of the call. These events can be queried using the</span></span><br><span class="line"><span class="comment">  clGetEventInfo function to get the status of a particular operation.</span></span><br><span class="line"><span class="comment">  Many functions also accept event lists that can be used to enforce ordering in</span></span><br><span class="line"><span class="comment">  an OpenCL context. These events lists are especially important in the context</span></span><br><span class="line"><span class="comment">  of out of order command queues as they are the only way specify dependency.</span></span><br><span class="line"><span class="comment">  Normal in-order command queues do not need this because dependency is enforced</span></span><br><span class="line"><span class="comment">  in the order the operation was enqueued. See the concurrent execution example</span></span><br><span class="line"><span class="comment">  for additional details on how create an use these types of command queues.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"CL/cl.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"xcl.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;array&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;random&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">array</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::chrono::duration;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::chrono::nanoseconds;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::chrono::seconds;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::default_random_engine;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::generate;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::uniform_int_distribution;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">vector</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Allocator template to align buffer to Page boundary for better data transfer</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">aligned_allocator</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">using</span> value_type = T;</span><br><span class="line">  <span class="function">T* <span class="title">allocate</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> num)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">void</span>* ptr = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">if</span> (posix_memalign(&amp;ptr,<span class="number">4096</span>,num*<span class="keyword">sizeof</span>(T)))</span><br><span class="line">      <span class="keyword">throw</span> <span class="built_in">std</span>::bad_alloc();</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">reinterpret_cast</span>&lt;T*&gt;(ptr);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">deallocate</span><span class="params">(T* p, <span class="built_in">std</span>::<span class="keyword">size_t</span> num)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="built_in">free</span>(p);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> ARRAY_SIZE = <span class="number">1</span> &lt;&lt; <span class="number">14</span>; <span class="comment">//  ARRAY_SIZE = 2^14</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> *error_message =</span><br><span class="line">    <span class="string">"Error: Result mismatch:\n"</span></span><br><span class="line">    <span class="string">"i = %d CPU result = %d Device result = %d\n"</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Wrap any OpenCL API calls that return error code(cl_int) with the below macros</span></span><br><span class="line"><span class="comment">// to quickly check for an error</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> OCL_CHECK(call)                                                        \</span></span><br><span class="line">  <span class="keyword">do</span> &#123;                                                                         \</span><br><span class="line">    cl_int err = call;                                                         \</span><br><span class="line">    <span class="keyword">if</span> (err != CL_SUCCESS) &#123;                                                   \</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"Error calling "</span> #call <span class="string">", error code is: %d\n"</span>, err);             \</span><br><span class="line">      <span class="built_in">exit</span>(EXIT_FAILURE);                                                      \</span><br><span class="line">    &#125;                                                                          \</span><br><span class="line">  &#125; <span class="keyword">while</span> (<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gen_random</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">static</span> default_random_engine e;</span><br><span class="line">  <span class="keyword">static</span> uniform_int_distribution&lt;<span class="keyword">int</span>&gt; dist(<span class="number">0</span>, <span class="number">100</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dist(e);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// An event callback function that prints the operations performed by the OpenCL</span></span><br><span class="line"><span class="comment">// runtime.</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">event_cb</span><span class="params">(cl_event event, cl_int cmd_status, <span class="keyword">void</span> *data)</span> </span>&#123;</span><br><span class="line">  cl_command_type command;</span><br><span class="line">  clGetEventInfo(event, CL_EVENT_COMMAND_TYPE, <span class="keyword">sizeof</span>(cl_command_type),</span><br><span class="line">                 &amp;command, <span class="literal">nullptr</span>);</span><br><span class="line">  cl_int status;</span><br><span class="line">  clGetEventInfo(event, CL_EVENT_COMMAND_EXECUTION_STATUS, <span class="keyword">sizeof</span>(cl_int),</span><br><span class="line">                 &amp;status, <span class="literal">nullptr</span>);</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">char</span> *command_str;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">char</span> *status_str;</span><br><span class="line">  <span class="keyword">switch</span> (command) &#123;</span><br><span class="line">  <span class="keyword">case</span> CL_COMMAND_READ_BUFFER:</span><br><span class="line">    command_str = <span class="string">"buffer read"</span>;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> CL_COMMAND_WRITE_BUFFER:</span><br><span class="line">    command_str = <span class="string">"buffer write"</span>;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> CL_COMMAND_NDRANGE_KERNEL:</span><br><span class="line">    command_str = <span class="string">"kernel"</span>;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> CL_COMMAND_MAP_BUFFER:</span><br><span class="line">    command_str = <span class="string">"kernel"</span>;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> CL_COMMAND_COPY_BUFFER:</span><br><span class="line">    command_str = <span class="string">"kernel"</span>;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> CL_COMMAND_MIGRATE_MEM_OBJECTS:</span><br><span class="line">        command_str = <span class="string">"buffer migrate"</span>;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    command_str = <span class="string">"unknown"</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">switch</span> (status) &#123;</span><br><span class="line">  <span class="keyword">case</span> CL_QUEUED:</span><br><span class="line">    status_str = <span class="string">"Queued"</span>;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> CL_SUBMITTED:</span><br><span class="line">    status_str = <span class="string">"Submitted"</span>;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> CL_RUNNING:</span><br><span class="line">    status_str = <span class="string">"Executing"</span>;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> CL_COMPLETE:</span><br><span class="line">    status_str = <span class="string">"Completed"</span>;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"[%s]: %s %s\n"</span>, <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">char</span> *&gt;(data), status_str,</span><br><span class="line">         command_str);</span><br><span class="line">  fflush(<span class="built_in">stdout</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Sets the callback for a particular event</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set_callback</span><span class="params">(cl_event event, <span class="keyword">const</span> <span class="keyword">char</span> *queue_name)</span> </span>&#123;</span><br><span class="line">  OCL_CHECK(</span><br><span class="line">      clSetEventCallback(event, CL_COMPLETE, event_cb, (<span class="keyword">void</span> *)queue_name));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  cl_int err;</span><br><span class="line"></span><br><span class="line">  xcl_world world = xcl_world_single();</span><br><span class="line">  cl_program program = xcl_import_binary(world, <span class="string">"vector_addition"</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// We will break down our problem into multiple iterations. Each iteration</span></span><br><span class="line">  <span class="comment">// will perform computation on a subset of the entire data-set.</span></span><br><span class="line">  <span class="keyword">size_t</span> elements_per_iteration = <span class="number">2048</span>;</span><br><span class="line">  <span class="keyword">size_t</span> bytes_per_iteration = elements_per_iteration * <span class="keyword">sizeof</span>(<span class="keyword">int</span>);</span><br><span class="line">  <span class="keyword">size_t</span> num_iterations = ARRAY_SIZE / elements_per_iteration; <span class="comment">//num_iterations = 8</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// This example will use an out of order command queue. The default command</span></span><br><span class="line">  <span class="comment">// queue created by xcl_world_single is an inorder command queue. Here we will</span></span><br><span class="line">  <span class="comment">// release the original queue and replace it with an out of order queue.</span></span><br><span class="line">  clReleaseCommandQueue(world.command_queue);</span><br><span class="line">  world.command_queue =</span><br><span class="line">      clCreateCommandQueue(world.context, world.device_id,</span><br><span class="line">                           CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE, &amp;err);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Allocate memory on the host and fill with random data.</span></span><br><span class="line">  <span class="comment">// 生成A,B随机初始值数据</span></span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt; A(ARRAY_SIZE);</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt; B(ARRAY_SIZE);</span><br><span class="line">  generate(begin(A), end(A), gen_random);</span><br><span class="line">  generate(begin(B), end(B), gen_random);</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>,aligned_allocator&lt;<span class="keyword">int</span>&gt;&gt; device_result(ARRAY_SIZE);</span><br><span class="line"></span><br><span class="line">  cl_kernel kernel = xcl_get_kernel(program, <span class="string">"vadd"</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// This pair of events will be used to track when a kernel is finished with</span></span><br><span class="line">  <span class="comment">// the input buffers. Once the kernel is finished processing the data, a new</span></span><br><span class="line">  <span class="comment">// set of elements will be written into the buffer.</span></span><br><span class="line">  <span class="comment">// 合理建立同步事件</span></span><br><span class="line">  <span class="built_in">array</span>&lt;cl_event, 2&gt; kernel_events;</span><br><span class="line">  <span class="built_in">array</span>&lt;cl_event, 2&gt; read_events;</span><br><span class="line">  <span class="built_in">array</span>&lt;cl_event, 2&gt; map_events;</span><br><span class="line">  cl_mem buffer_a[<span class="number">2</span>], buffer_b[<span class="number">2</span>], buffer_c[<span class="number">2</span>]; <span class="comment">// Double Buffer 定义</span></span><br><span class="line">  <span class="keyword">size_t</span> global = <span class="number">1</span>, local = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> iteration_idx = <span class="number">0</span>; iteration_idx &lt; num_iterations; iteration_idx++) &#123;</span><br><span class="line">    <span class="keyword">int</span> flag = iteration_idx % <span class="number">2</span>; <span class="comment">// 建立Double Buffer的索引</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (iteration_idx &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">        clWaitForEvents(<span class="number">1</span>, &amp;map_events[flag]);</span><br><span class="line">        OCL_CHECK(clReleaseMemObject(buffer_a[flag]));</span><br><span class="line">        OCL_CHECK(clReleaseMemObject(buffer_b[flag]));</span><br><span class="line">        OCL_CHECK(clReleaseMemObject(buffer_c[flag]));</span><br><span class="line">        OCL_CHECK(clReleaseEvent(read_events[flag]));</span><br><span class="line">        OCL_CHECK(clReleaseEvent(kernel_events[flag]));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 一次申请2048个int值大小的只读内存，从A拷贝到buffer_a，buffer_b中。</span></span><br><span class="line">    buffer_a[flag] = clCreateBuffer(world.context,</span><br><span class="line">            CL_MEM_READ_ONLY | CL_MEM_USE_HOST_PTR,</span><br><span class="line">           bytes_per_iteration, &amp;A[iteration_idx * elements_per_iteration], <span class="literal">NULL</span>);</span><br><span class="line">    buffer_b[flag] = clCreateBuffer(world.context,</span><br><span class="line">            CL_MEM_READ_ONLY | CL_MEM_USE_HOST_PTR,</span><br><span class="line">           bytes_per_iteration, &amp;B[iteration_idx * elements_per_iteration], <span class="literal">NULL</span>);</span><br><span class="line">    <span class="comment">//buffer_c申请2048个int值大小的只写内存，从device_result写入。</span></span><br><span class="line">    buffer_c[flag] = clCreateBuffer(world.context,</span><br><span class="line">            CL_MEM_WRITE_ONLY | CL_MEM_USE_HOST_PTR,</span><br><span class="line">           bytes_per_iteration, &amp;device_result[iteration_idx * elements_per_iteration], <span class="literal">NULL</span>);</span><br><span class="line">    <span class="built_in">array</span>&lt;cl_event, 2&gt; write_events;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Enqueueing Migrate Mem Object (Host to Device) calls\n"</span>);</span><br><span class="line">    <span class="comment">// These calls are asynchronous with respect to the main thread because we</span></span><br><span class="line">    <span class="comment">// are passing the CL_FALSE as the third parameter. Because we are passing</span></span><br><span class="line">    <span class="comment">// the events from the previous kernel call into the wait list, it will wait</span></span><br><span class="line">    <span class="comment">// for the previous operations to complete before continuing</span></span><br><span class="line">    <span class="comment">//clEnqueueMigrateMemObjects 替代 clEnqueueWriteBuffer</span></span><br><span class="line">    OCL_CHECK(clEnqueueMigrateMemObjects(</span><br><span class="line">        world.command_queue, <span class="number">1</span>, &amp;buffer_a[flag],</span><br><span class="line">        <span class="number">0</span> <span class="comment">/* flags, 0 means from host */</span>,</span><br><span class="line">        <span class="number">0</span>, <span class="literal">NULL</span>,</span><br><span class="line">        &amp;write_events[<span class="number">0</span>]));</span><br><span class="line">    set_callback(write_events[<span class="number">0</span>], <span class="string">"ooo_queue_write_events[0]"</span>);</span><br><span class="line">    <span class="comment">////clEnqueueMigrateMemObjects 替代 clEnqueueWriteBuffer</span></span><br><span class="line">    OCL_CHECK(clEnqueueMigrateMemObjects(</span><br><span class="line">        world.command_queue, <span class="number">1</span>, &amp;buffer_b[flag],</span><br><span class="line">        <span class="number">0</span> <span class="comment">/* flags, 0 means from host */</span>,</span><br><span class="line">        <span class="number">0</span>, <span class="literal">NULL</span>,</span><br><span class="line">        &amp;write_events[<span class="number">1</span>]));</span><br><span class="line">    set_callback(write_events[<span class="number">1</span>], <span class="string">"ooo_queue_write_events[1]"</span>);</span><br><span class="line"></span><br><span class="line">    xcl_set_kernel_arg(kernel, <span class="number">0</span>, <span class="keyword">sizeof</span>(cl_mem), &amp;buffer_c[iteration_idx % <span class="number">2</span>]);</span><br><span class="line">    xcl_set_kernel_arg(kernel, <span class="number">1</span>, <span class="keyword">sizeof</span>(cl_mem), &amp;buffer_a[iteration_idx % <span class="number">2</span>]);</span><br><span class="line">    xcl_set_kernel_arg(kernel, <span class="number">2</span>, <span class="keyword">sizeof</span>(cl_mem), &amp;buffer_b[iteration_idx % <span class="number">2</span>]);</span><br><span class="line">    xcl_set_kernel_arg(kernel, <span class="number">3</span>, <span class="keyword">sizeof</span>(<span class="keyword">int</span>), &amp;elements_per_iteration);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Enqueueing NDRange kernel.\n"</span>);</span><br><span class="line">    <span class="comment">// This event needs to wait for the write buffer operations to complete</span></span><br><span class="line">    <span class="comment">// before executing. We are sending the write_events into its wait list to</span></span><br><span class="line">    <span class="comment">// ensure that the order of operations is correct.</span></span><br><span class="line">    OCL_CHECK(clEnqueueNDRangeKernel(world.command_queue, kernel, <span class="number">1</span>, <span class="literal">nullptr</span>,</span><br><span class="line">                                     &amp;global, &amp;local, <span class="number">2</span> , write_events.data(),</span><br><span class="line">                                     &amp;kernel_events[flag]));</span><br><span class="line">    set_callback(kernel_events[flag], <span class="string">"ooo_queue_kernel_events"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Enqueueing Migrate Mem Object (Device to Host) calls\n"</span>);</span><br><span class="line">    <span class="comment">// This operation only needs to wait for the kernel call. This call will</span></span><br><span class="line">    <span class="comment">// potentially overlap the next kernel call as well as the next read</span></span><br><span class="line">    <span class="comment">// operations</span></span><br><span class="line">    <span class="comment">// //clEnqueueMigrateMemObjects 替代 clEnqueueReadBuffer</span></span><br><span class="line">    OCL_CHECK(clEnqueueMigrateMemObjects(world.command_queue, <span class="number">1</span>, &amp;buffer_c[flag],</span><br><span class="line">                CL_MIGRATE_MEM_OBJECT_HOST, <span class="number">1</span>, &amp;kernel_events[flag], &amp;read_events[flag]));</span><br><span class="line"></span><br><span class="line">    set_callback(read_events[flag], <span class="string">"ooo_queue_read_events"</span>);</span><br><span class="line">    clEnqueueMapBuffer(world.command_queue, buffer_c[flag], CL_FALSE, CL_MAP_READ, <span class="number">0</span>,</span><br><span class="line">            bytes_per_iteration, <span class="number">1</span>, &amp;read_events[flag], &amp;map_events[flag], <span class="number">0</span>);</span><br><span class="line">    set_callback(map_events[flag], <span class="string">"ooo_queue_map_events"</span>);</span><br><span class="line"></span><br><span class="line">    OCL_CHECK(clReleaseEvent(write_events[<span class="number">0</span>]));</span><br><span class="line">    OCL_CHECK(clReleaseEvent(write_events[<span class="number">1</span>]));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Wait for all of the OpenCL operations to complete</span></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"Waiting...\n"</span>);</span><br><span class="line">  clFlush(world.command_queue);</span><br><span class="line">  clFinish(world.command_queue);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//Releasing mem objects and events</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; <span class="number">2</span> ; i++)&#123;</span><br><span class="line">    OCL_CHECK(clWaitForEvents(<span class="number">1</span>, &amp;map_events[i]));</span><br><span class="line">    OCL_CHECK(clReleaseMemObject(buffer_a[i]));</span><br><span class="line">    OCL_CHECK(clReleaseMemObject(buffer_b[i]));</span><br><span class="line">    OCL_CHECK(clReleaseMemObject(buffer_c[i]));</span><br><span class="line">    OCL_CHECK(clReleaseEvent(read_events[i]));</span><br><span class="line">    OCL_CHECK(clReleaseEvent(kernel_events[i]));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> match = <span class="number">0</span>;</span><br><span class="line">  <span class="comment">// verify the results</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++) &#123;</span><br><span class="line">    <span class="keyword">int</span> host_result = A[i] + B[i];</span><br><span class="line">    <span class="keyword">if</span> (device_result[i] != host_result) &#123;</span><br><span class="line">      <span class="built_in">printf</span>(error_message, i, host_result, device_result[i]);</span><br><span class="line">      match = <span class="number">1</span>;</span><br><span class="line">      <span class="comment">// break;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  OCL_CHECK(clReleaseKernel(kernel));</span><br><span class="line">  OCL_CHECK(clReleaseProgram(program));</span><br><span class="line">  xcl_release_world(world);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"TEST %s\n"</span>, (match ? <span class="string">"FAILED"</span> : <span class="string">"PASSED"</span>));</span><br><span class="line">  <span class="keyword">return</span> (match ? EXIT_FAILURE :  EXIT_SUCCESS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="内核代码分析"><a href="#内核代码分析" class="headerlink" title="内核代码分析"></a>内核代码分析</h2><h3 id="内核源码"><a href="#内核源码" class="headerlink" title="内核源码"></a>内核源码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BUFFER_SIZE 256</span></span><br><span class="line">kernel __attribute__((reqd_work_group_size(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vadd</span><span class="params">(global <span class="keyword">int</span>* c,</span></span></span><br><span class="line"><span class="function"><span class="params">          global <span class="keyword">const</span> <span class="keyword">int</span>* a,</span></span></span><br><span class="line"><span class="function"><span class="params">          global <span class="keyword">const</span> <span class="keyword">int</span>* b,</span></span></span><br><span class="line"><span class="function"><span class="params">          <span class="keyword">const</span> <span class="keyword">int</span> elements</span></span></span><br><span class="line"><span class="function"><span class="params">          )</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> arrayA[BUFFER_SIZE];</span><br><span class="line">    <span class="keyword">int</span> arrayB[BUFFER_SIZE];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span> ; i &lt; elements ; i += BUFFER_SIZE)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> size = BUFFER_SIZE;</span><br><span class="line">        <span class="keyword">if</span> (i + size &gt; elements) size = elements - i;</span><br><span class="line">        readA: <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span> ; j &lt; size ; j++) arrayA[j] = a[i+j];</span><br><span class="line">        readB: <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span> ; j &lt; size ; j++) arrayB[j] = b[i+j];</span><br><span class="line">        vadd_writeC: <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span> ; j &lt; size ; j++) c[i+j] = arrayA[j] + arrayB[j];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>内核源码分析：</p><p>向量相加内核模块，采用burst 突发传输的形式，突发长度为BUFFER_SIZE。<br>需要学习的地方是，突发长度与数据导入过程需要进行比较，防止数据读入错误！<br>内核一次运算elements个数据。</p><h3 id="综合报表"><a href="#综合报表" class="headerlink" title="综合报表"></a>综合报表</h3><h3 id="Performence图"><a href="#Performence图" class="headerlink" title="Performence图"></a>Performence图</h3><h2 id="关键理解概念描述"><a href="#关键理解概念描述" class="headerlink" title="关键理解概念描述"></a>关键理解概念描述</h2><p><img src="http://xukeqiniu.xukeai.cn/8da1330ebf6c2b408993c342cec35553.png" alt=""></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">INFO: Importing xclbin/vector_addition.hw.xilinx_adm-pcie-7v3_1ddr.xclbin</span><br><span class="line">INFO: Loaded file</span><br><span class="line">INFO: Created Binary</span><br><span class="line">INFO: Built Program</span><br><span class="line">Enqueueing Migrate Mem Object (Host to Device) calls</span><br><span class="line">Enqueueing NDRange kernel.</span><br><span class="line">Enqueueing Migrate Mem Object (Device to Host) calls</span><br><span class="line">Enqueueing Migrate Mem Object (Host to Device) calls</span><br><span class="line">[ooo_queue_write_events[0]]: Completed buffer migrate  ---&gt; Wa1</span><br><span class="line">[ooo_queue_write_events[1]]: Completed buffer migrate  ---&gt; Wb1</span><br><span class="line">[ooo_queue_write_events[0]]: Completed buffer migrate  ---&gt; Wa2</span><br><span class="line">[ooo_queue_write_events[1]]: Completed buffer migrate  ---&gt; Wb2</span><br><span class="line">[ooo_queue_kernel_events]: Completed kernel            ---&gt; CU1</span><br><span class="line">[ooo_queue_read_events]: Completed buffer migrate      ---&gt; Rc1(1)</span><br><span class="line">Enqueueing NDRange kernel.</span><br><span class="line">[ooo_queue_map_events]: Completed kernel               ---&gt; Rc1(2)</span><br><span class="line">Enqueueing Migrate Mem Object (Device to Host) calls</span><br><span class="line">Enqueueing Migrate Mem Object (Host to Device) calls</span><br><span class="line">[ooo_queue_kernel_events]: Completed kernel            ---&gt; CU2</span><br><span class="line">Enqueueing NDRange kernel.</span><br><span class="line">[ooo_queue_write_events[0]]: Completed buffer migrate  ---&gt; Wa3</span><br><span class="line">[ooo_queue_read_events]: Completed buffer migrate      ---&gt; Rc2(1)</span><br><span class="line">Enqueueing Migrate Mem Object (Device to Host) calls</span><br><span class="line">[ooo_queue_write_events[1]]: Completed buffer migrate  ---&gt; Wb3</span><br><span class="line">[ooo_queue_map_events]: Completed kernel               ---&gt; Rc2(2)</span><br><span class="line">[ooo_queue_kernel_events]: Completed kernel            ---&gt; CU3</span><br><span class="line">Enqueueing Migrate Mem Object (Host to Device) calls</span><br><span class="line">[ooo_queue_read_events]: Completed buffer migrate      ---&gt; Rc3(1)</span><br><span class="line">[ooo_queue_map_events]: Completed kernel               ---&gt; Rc3(2)</span><br><span class="line">[ooo_queue_write_events[0]]: Completed buffer migrate  ---&gt; Wa4</span><br><span class="line">Enqueueing NDRange kernel.</span><br><span class="line">[ooo_queue_write_events[1]]: Completed buffer migrate  ---&gt; Wb4</span><br><span class="line">Enqueueing Migrate Mem Object (Device to Host) calls</span><br><span class="line">Enqueueing Migrate Mem Object (Host to Device) calls</span><br><span class="line">[ooo_queue_kernel_events]: Completed kernel            ---&gt; CU4</span><br><span class="line">Enqueueing NDRange kernel.</span><br><span class="line">[ooo_queue_write_events[0]]: Completed buffer migrate  ---&gt; Wa5</span><br><span class="line">[ooo_queue_write_events[1]]: Completed buffer migrate  ---&gt; Wb5</span><br><span class="line">[ooo_queue_read_events]: Completed buffer migrate      ---&gt; Rc4(1)</span><br><span class="line">[ooo_queue_map_events]: Completed kernel               ---&gt; Rc4(2)</span><br><span class="line">Enqueueing Migrate Mem Object (Device to Host) calls</span><br><span class="line">[ooo_queue_kernel_events]: Completed kernel            ---&gt; CU5</span><br><span class="line">Enqueueing Migrate Mem Object (Host to Device) calls</span><br><span class="line">[ooo_queue_read_events]: Completed buffer migrate      ---&gt; Rc5(1)</span><br><span class="line">[ooo_queue_write_events[0]]: Completed buffer migrate</span><br><span class="line">[ooo_queue_map_events]: Completed kernel               ---&gt; Rc5(2)</span><br><span class="line">[ooo_queue_write_events[1]]: Completed buffer migrate</span><br><span class="line">Enqueueing NDRange kernel.</span><br><span class="line">Enqueueing Migrate Mem Object (Device to Host) calls</span><br><span class="line">[ooo_queue_kernel_events]: Completed kernel</span><br><span class="line">Enqueueing Migrate Mem Object (Host to Device) calls</span><br><span class="line">[ooo_queue_read_events]: Completed buffer migrate</span><br><span class="line">Enqueueing NDRange kernel.</span><br><span class="line">[ooo_queue_map_events]: Completed kernel</span><br><span class="line">[ooo_queue_write_events[0]]: Completed buffer migrate</span><br><span class="line">[ooo_queue_write_events[1]]: Completed buffer migrate</span><br><span class="line">Enqueueing Migrate Mem Object (Device to Host) calls</span><br><span class="line">Enqueueing Migrate Mem Object (Host to Device) calls</span><br><span class="line">[ooo_queue_kernel_events]: Completed kernel</span><br><span class="line">Enqueueing NDRange kernel.</span><br><span class="line">Enqueueing Migrate Mem Object (Device to Host) calls</span><br><span class="line">[ooo_queue_write_events[0]]: Completed buffer migrate</span><br><span class="line">[ooo_queue_read_events]: Completed buffer migrate</span><br><span class="line">[ooo_queue_write_events[1]]: Completed buffer migrate</span><br><span class="line">[ooo_queue_map_events]: Completed kernel</span><br><span class="line">Waiting...</span><br><span class="line">[ooo_queue_kernel_events]: Completed kernel</span><br><span class="line">[ooo_queue_read_events]: Completed buffer migrate</span><br><span class="line">[ooo_queue_map_events]: Completed kernel</span><br><span class="line">TEST PASSED</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> host </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> host </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>gmem_4bank</title>
      <link href="/2017/11/09/SDAccel/%E4%BC%98%E5%8C%96/kernel_to_gmem/gmem-4bank/"/>
      <url>/2017/11/09/SDAccel/%E4%BC%98%E5%8C%96/kernel_to_gmem/gmem-4bank/</url>
      <content type="html"><![CDATA[<h2 id="例程描述"><a href="#例程描述" class="headerlink" title="例程描述"></a>例程描述</h2><div class="note default"><p>测试Kernel与Global Memory之间的带宽 multi bank（4块DDR独立数据传输）clEnqueueMapBuffer 与 clEnqueueUnmapMemobject 在 host 到 Global Memory 的数据优化</p></div><h2 id="主要学习知识点"><a href="#主要学习知识点" class="headerlink" title="主要学习知识点"></a>主要学习知识点</h2><div class="note primary"><ul><li>Key Concepts<ul><li>Concurrent execution （并发执行）</li><li>Out of Order Command Queues  （命令队列的执行顺序）</li><li>Multiple Command Queues （多个命令队列）</li></ul></li><li>Keywords<ul><li>cl_mem_ext_ptr_t</li><li>clEnqueueMapBuffer()</li><li>clEnqueueUnmapMemobject()</li></ul></li></ul></div><a id="more"></a><h2 id="主机端代码分析"><a href="#主机端代码分析" class="headerlink" title="主机端代码分析"></a>主机端代码分析</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdint.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;CL/opencl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;CL/cl_ext.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> USE_4DDR <span class="comment">//4块DDR独立数据传输方式</span></span></span><br><span class="line"><span class="comment">/////////////////////////////////////////////////////////////////////////////////</span></span><br><span class="line"><span class="comment">//load_file_to_memory</span></span><br><span class="line"><span class="comment">//Allocated memory for and load file from disk memory</span></span><br><span class="line"><span class="comment">//Return value</span></span><br><span class="line"><span class="comment">// 0   Success</span></span><br><span class="line"><span class="comment">//-1   Failure to open file</span></span><br><span class="line"><span class="comment">//-2   Failure to allocate memory</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">load_file_to_memory</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *filename, <span class="keyword">char</span> **result,<span class="keyword">size_t</span> *inputsize)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line">   FILE *f = fopen(filename, <span class="string">"rb"</span>);</span><br><span class="line">   <span class="keyword">if</span> (f == <span class="literal">NULL</span>) &#123;</span><br><span class="line">       *result = <span class="literal">NULL</span>;</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>; <span class="comment">// -1 means file opening fail</span></span><br><span class="line">   &#125;</span><br><span class="line">   fseek(f, <span class="number">0</span>, SEEK_END);</span><br><span class="line">   size = ftell(f);</span><br><span class="line">   fseek(f, <span class="number">0</span>, SEEK_SET);</span><br><span class="line">   *result = (<span class="keyword">char</span> *)<span class="built_in">malloc</span>(size+<span class="number">1</span>);</span><br><span class="line">   <span class="keyword">if</span> (size != fread(*result, <span class="keyword">sizeof</span>(<span class="keyword">char</span>), size, f))</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="built_in">free</span>(*result);</span><br><span class="line">           <span class="keyword">return</span> <span class="number">-2</span>; <span class="comment">// -2 means file reading fail</span></span><br><span class="line">       &#125;</span><br><span class="line">   fclose(f);</span><br><span class="line">   (*result)[size] = <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">if</span>(inputsize!=<span class="literal">NULL</span>) (*inputsize)=size;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/////////////////////////////////////////////////////////////////////////////////</span></span><br><span class="line"><span class="comment">//opencl_setup</span></span><br><span class="line"><span class="comment">//Create context for Xilinx platform, Accelerator device</span></span><br><span class="line"><span class="comment">//Create single command queue for accelerator device</span></span><br><span class="line"><span class="comment">//Create program object with clCreateProgramWithBinary using given xclbin file name</span></span><br><span class="line"><span class="comment">//Return value</span></span><br><span class="line"><span class="comment">// 0    Success</span></span><br><span class="line"><span class="comment">//-1    Error</span></span><br><span class="line"><span class="comment">//-2    Failed to load XCLBIN file from disk</span></span><br><span class="line"><span class="comment">//-3    Failed to clCreateProgramWithBinary</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">opencl_setup</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *xclbinfilename, cl_platform_id *platform_id,</span></span></span><br><span class="line"><span class="function"><span class="params">                cl_device_id *devices, cl_device_id *device_id, cl_context  *context,</span></span></span><br><span class="line"><span class="function"><span class="params">                cl_command_queue *command_queue, cl_program *program,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">char</span> *cl_platform_name, <span class="keyword">const</span> <span class="keyword">char</span> *target_device_name)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">char</span> cl_platform_vendor[<span class="number">1001</span>];</span><br><span class="line">   <span class="keyword">char</span> cl_device_name[<span class="number">1001</span>];</span><br><span class="line">   cl_int err;</span><br><span class="line">   cl_uint num_devices;</span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">int</span> device_found = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Get first platform</span></span><br><span class="line">   err = clGetPlatformIDs(<span class="number">1</span>,platform_id,<span class="literal">NULL</span>);</span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Failed to find an OpenCL platform!\n"</span>);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   err = clGetPlatformInfo(*platform_id,CL_PLATFORM_VENDOR,<span class="number">1000</span>,(<span class="keyword">void</span> *)cl_platform_vendor,<span class="literal">NULL</span>);</span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: clGetPlatformInfo(CL_PLATFORM_VENDOR) failed!\n"</span>);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">"CL_PLATFORM_VENDOR %s\n"</span>,cl_platform_vendor);</span><br><span class="line">   err = clGetPlatformInfo(*platform_id,CL_PLATFORM_NAME,<span class="number">1000</span>,(<span class="keyword">void</span> *)cl_platform_name,<span class="literal">NULL</span>);</span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR: clGetPlatformInfo(CL_PLATFORM_NAME) failed!\n"</span>);</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">           <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">"CL_PLATFORM_NAME %s\n"</span>,cl_platform_name);</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Get Accelerator compute device</span></span><br><span class="line">   err = clGetDeviceIDs(*platform_id, CL_DEVICE_TYPE_ACCELERATOR, <span class="number">16</span>, devices, &amp;num_devices);</span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Failed to create a device group!\n"</span>);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">//iterate all devices to select the target device.</span></span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> i=<span class="number">0</span>; i&lt;num_devices; i++) &#123;</span><br><span class="line">       err = clGetDeviceInfo(devices[i], CL_DEVICE_NAME, <span class="number">1024</span>, cl_device_name, <span class="number">0</span>);</span><br><span class="line">       <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"Error: Failed to get device name for device %d!\n"</span>, i);</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"Test failed\n"</span>);</span><br><span class="line">           <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="comment">//printf("CL_DEVICE_NAME %s\n", cl_device_name);</span></span><br><span class="line">       <span class="keyword">if</span>(<span class="built_in">strcmp</span>(cl_device_name, target_device_name) == <span class="number">0</span>) &#123;</span><br><span class="line">           *device_id = devices[i];</span><br><span class="line">           device_found = <span class="number">1</span>;</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"Selected %s as the target device\n"</span>, cl_device_name);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (!device_found) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Target device %s not found. Exit.\n"</span>, target_device_name);</span><br><span class="line">       <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Create a compute context containing accelerator device</span></span><br><span class="line">   (*context)= clCreateContext(<span class="number">0</span>, <span class="number">1</span>, device_id, <span class="literal">NULL</span>, <span class="literal">NULL</span>, &amp;err);</span><br><span class="line">   <span class="keyword">if</span> (!(*context))</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR: Failed to create a compute context!\n"</span>);</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">           <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Create a command queue for accelerator device</span></span><br><span class="line">   (*command_queue) = clCreateCommandQueue(*context, *device_id, CL_QUEUE_PROFILING_ENABLE, &amp;err);</span><br><span class="line">   <span class="keyword">if</span> (!(*command_queue))</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR: Failed to create a command commands!\n"</span>);</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR: code %i\n"</span>,err);</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">           <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Load XCLBIN file binary from disk</span></span><br><span class="line">   <span class="keyword">int</span> status;</span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">char</span> *kernelbinary;</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">"loading %s\n"</span>, xclbinfilename);</span><br><span class="line">   <span class="keyword">size_t</span> xclbinlength;</span><br><span class="line">   err = load_file_to_memory(xclbinfilename, (<span class="keyword">char</span> **) &amp;kernelbinary,&amp;xclbinlength);</span><br><span class="line">   <span class="keyword">if</span> (err != <span class="number">0</span>) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: failed to load kernel from xclbin: %s\n"</span>, xclbinfilename);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-2</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Create the program from XCLBIN file, configuring accelerator device</span></span><br><span class="line">   (*program) = clCreateProgramWithBinary(*context, <span class="number">1</span>, device_id, &amp;xclbinlength, (<span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">char</span> **) &amp;kernelbinary, &amp;status, &amp;err);</span><br><span class="line">   <span class="keyword">if</span> ((!(*program)) || (err!=CL_SUCCESS)) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Failed to create compute program from binary %d!\n"</span>, err);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-3</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Build the program executable (no-op)</span></span><br><span class="line">   err = clBuildProgram(*program, <span class="number">0</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">           <span class="keyword">size_t</span> len;</span><br><span class="line">           <span class="keyword">char</span> buffer[<span class="number">2048</span>];</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR: Failed to build program executable!\n"</span>);</span><br><span class="line">           clGetProgramBuildInfo(*program, *device_id, CL_PROGRAM_BUILD_LOG, <span class="keyword">sizeof</span>(buffer), buffer, &amp;len);</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"%s\n"</span>, buffer);</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">           <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/////////////////////////////////////////////////////////////////////////////////</span></span><br><span class="line"><span class="comment">//main</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> defined(SDX_PLATFORM) &amp;&amp; !defined(TARGET_DEVICE)</span></span><br><span class="line"> <span class="meta">#<span class="meta-keyword">define</span> STR_VALUE(arg)      #arg</span></span><br><span class="line"> <span class="meta">#<span class="meta-keyword">define</span> GET_STRING(name) STR_VALUE(name)</span></span><br><span class="line"> <span class="meta">#<span class="meta-keyword">define</span> TARGET_DEVICE GET_STRING(SDX_PLATFORM)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">   <span class="comment">//TARGET_DEVICE macro needs to be passed from gcc command line</span></span><br><span class="line">   <span class="keyword">const</span> <span class="keyword">char</span> *target_device_name = TARGET_DEVICE;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">int</span> err, err1, err2, err3;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">size_t</span> globalbuffersize = <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">1024</span>;    <span class="comment">//1GB</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">//Reducing the data size for emulation mode</span></span><br><span class="line">   <span class="keyword">char</span> *xcl_mode = getenv(<span class="string">"XCL_EMULATION_MODE"</span>);</span><br><span class="line">   <span class="keyword">if</span> (xcl_mode != <span class="literal">NULL</span>)&#123;</span><br><span class="line">     globalbuffersize = <span class="number">1024</span> * <span class="number">1024</span> ;  <span class="comment">// 1MB</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">//opencl setup</span></span><br><span class="line">   cl_platform_id platform_id;</span><br><span class="line">   cl_device_id device_id;</span><br><span class="line">   cl_device_id devices[<span class="number">16</span>];  <span class="comment">// compute device id</span></span><br><span class="line">   cl_context context;</span><br><span class="line">   cl_command_queue command_queue;</span><br><span class="line">   cl_program program;</span><br><span class="line">   <span class="keyword">char</span> cl_platform_name[<span class="number">1001</span>];</span><br><span class="line"></span><br><span class="line">   <span class="comment">//variables for profiling</span></span><br><span class="line">   <span class="keyword">uint64_t</span> nsduration;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (argc != <span class="number">2</span>)&#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Usage: %s &lt;xclbin_file&gt;\n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">       <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   err = opencl_setup(argv[<span class="number">1</span>], &amp;platform_id, devices, &amp;device_id,</span><br><span class="line">                      &amp;context, &amp;command_queue, &amp;program, cl_platform_name,</span><br><span class="line">                      target_device_name);</span><br><span class="line">   <span class="keyword">if</span>(err==<span class="number">-1</span>)&#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error : general failure setting up opencl context\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">if</span>(err==<span class="number">-2</span>) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error : failed to bandwidth.xclbin from disk\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">if</span>(err==<span class="number">-3</span>) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error : failed to clCreateProgramWithBinary with contents of xclbin\n"</span>);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">//access the ACCELERATOR kernel</span></span><br><span class="line">   cl_int clstatus;</span><br><span class="line">   cl_kernel kernel = clCreateKernel(program, <span class="string">"bandwidth"</span>, &amp;clstatus);</span><br><span class="line">   <span class="keyword">if</span> (!kernel || clstatus != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Failed to create compute kernel!\n"</span>);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">//input buffer</span></span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">char</span> *input_host = ((<span class="keyword">unsigned</span> <span class="keyword">char</span> *)<span class="built_in">malloc</span>(globalbuffersize));</span><br><span class="line">   <span class="keyword">if</span>(input_host==<span class="literal">NULL</span>) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Failed to allocate host side copy of OpenCL source buffer of size %zu\n"</span>,globalbuffersize);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">int</span> i;</span><br><span class="line">   <span class="keyword">for</span>(i=<span class="number">0</span>; i&lt;globalbuffersize; i++)</span><br><span class="line">       input_host[i]=i%<span class="number">256</span>;</span><br><span class="line"></span><br><span class="line">   cl_mem input_buffer0, output_buffer0;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> defined(USE_2DDR) || defined(USE_4DDR)</span></span><br><span class="line">   <span class="keyword">cl_mem_ext_ptr_t</span> input_buffer0_ext, output_buffer0_ext;</span><br><span class="line">   input_buffer0_ext.flags = XCL_MEM_DDR_BANK0; <span class="comment">//选择DDR0</span></span><br><span class="line">   input_buffer0_ext.obj = <span class="literal">NULL</span>;</span><br><span class="line">   input_buffer0_ext.param = <span class="number">0</span>;</span><br><span class="line">   input_buffer0 = clCreateBuffer(context,</span><br><span class="line">                                 CL_MEM_READ_WRITE | CL_MEM_EXT_PTR_XILINX,</span><br><span class="line">                                 globalbuffersize,</span><br><span class="line">                                 &amp;input_buffer0_ext,</span><br><span class="line">                                 &amp;err);</span><br><span class="line"></span><br><span class="line">   output_buffer0_ext.flags = XCL_MEM_DDR_BANK1;<span class="comment">//选择DDR1</span></span><br><span class="line">   output_buffer0_ext.obj = <span class="literal">NULL</span>;</span><br><span class="line">   output_buffer0_ext.param = <span class="number">0</span>;</span><br><span class="line">   output_buffer0 = clCreateBuffer(context,</span><br><span class="line">                                  CL_MEM_READ_WRITE | CL_MEM_EXT_PTR_XILINX,</span><br><span class="line">                                  globalbuffersize,</span><br><span class="line">                                  &amp;output_buffer0_ext,</span><br><span class="line">                                  &amp;err1);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> defined(USE_4DDR)</span></span><br><span class="line">   cl_mem input_buffer1, output_buffer1;</span><br><span class="line">   <span class="keyword">cl_mem_ext_ptr_t</span> input_buffer1_ext, output_buffer1_ext;</span><br><span class="line">   input_buffer1_ext.flags = XCL_MEM_DDR_BANK2;<span class="comment">//选择DDR2</span></span><br><span class="line">   input_buffer1_ext.obj = <span class="literal">NULL</span>;</span><br><span class="line">   input_buffer1_ext.param = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">   input_buffer1 = clCreateBuffer(context,</span><br><span class="line">                                 CL_MEM_READ_WRITE | CL_MEM_EXT_PTR_XILINX,</span><br><span class="line">                                 globalbuffersize,</span><br><span class="line">                                 &amp;input_buffer1_ext,</span><br><span class="line">                                 &amp;err2);</span><br><span class="line"></span><br><span class="line">   output_buffer1_ext.flags = XCL_MEM_DDR_BANK3;<span class="comment">//选择DDR3</span></span><br><span class="line">   output_buffer1_ext.obj = <span class="literal">NULL</span>;</span><br><span class="line">   output_buffer1_ext.param = <span class="number">0</span>;</span><br><span class="line">   output_buffer1 = clCreateBuffer(context,</span><br><span class="line">                                  CL_MEM_READ_WRITE | CL_MEM_EXT_PTR_XILINX,</span><br><span class="line">                                  globalbuffersize,</span><br><span class="line">                                  &amp;output_buffer1_ext,</span><br><span class="line">                                  &amp;err3);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">   input_buffer0 = clCreateBuffer(context,</span><br><span class="line">                                 CL_MEM_READ_WRITE,</span><br><span class="line">                                 globalbuffersize,</span><br><span class="line">                                 <span class="literal">NULL</span>,</span><br><span class="line">                                 &amp;err);</span><br><span class="line"></span><br><span class="line">   output_buffer0 = clCreateBuffer(context,</span><br><span class="line">                                  CL_MEM_READ_WRITE,</span><br><span class="line">                                  globalbuffersize,</span><br><span class="line">                                  <span class="literal">NULL</span>,</span><br><span class="line">                                  &amp;err1);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span>(err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Failed to allocate input_buffer0 of size %zu\n"</span>, globalbuffersize);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (err1 != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Failed to allocate output_buffer0 of size %zu\n"</span>, globalbuffersize);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_4DDR</span></span><br><span class="line">   <span class="keyword">if</span>(err2 != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Failed to allocate input_buffer1 of size %zu\n"</span>, globalbuffersize);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (err3 != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Failed to allocate output_buffer1 of size %zu\n"</span>, globalbuffersize);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">   <span class="comment">//</span></span><br><span class="line">   cl_ulong num_blocks = globalbuffersize/<span class="number">64</span>; <span class="comment">//1GB数据 一次处理uint16个数 16*4 = 64B个</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_4DDR</span></span><br><span class="line">   <span class="keyword">double</span> dbytes = globalbuffersize*<span class="number">2.0</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">   <span class="keyword">double</span> dbytes = globalbuffersize;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">   <span class="keyword">double</span> dmbytes = dbytes / (((<span class="keyword">double</span>)<span class="number">1024</span>) * ((<span class="keyword">double</span>)<span class="number">1024</span>));</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">"Starting kernel to read/write %.0lf MB bytes from/to global memory... \n"</span>, dmbytes);</span><br><span class="line"></span><br><span class="line">   <span class="comment">//Write input buffer</span></span><br><span class="line">   <span class="comment">//Map input buffer for PCIe write （提高host --&gt; global memory 数据传输的速度）</span></span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">char</span> *map_input_buffer0;</span><br><span class="line">   <span class="comment">//将input_buffer0（global的地址映射到host端，便于在host端直接操作）</span></span><br><span class="line">   map_input_buffer0 = (<span class="keyword">unsigned</span> <span class="keyword">char</span> *) clEnqueueMapBuffer(command_queue,</span><br><span class="line">                                                           input_buffer0,</span><br><span class="line">                                                           CL_FALSE,</span><br><span class="line">                                                           CL_MAP_WRITE_INVALIDATE_REGION,</span><br><span class="line">                                                           <span class="number">0</span>,</span><br><span class="line">                                                           globalbuffersize,</span><br><span class="line">                                                           <span class="number">0</span>,</span><br><span class="line">                                                           <span class="literal">NULL</span>,</span><br><span class="line">                                                           <span class="literal">NULL</span>,</span><br><span class="line">                                                           &amp;err);</span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Failed to clEnqueueMapBuffer OpenCL buffer\n"</span>);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   clFinish(command_queue);</span><br><span class="line"></span><br><span class="line">   <span class="comment">//prepare data to be written to the device</span></span><br><span class="line">   <span class="keyword">for</span>(i=<span class="number">0</span>; i&lt;globalbuffersize; i++)</span><br><span class="line">       map_input_buffer0[i] = input_host[i];</span><br><span class="line"></span><br><span class="line">   cl_event event1;</span><br><span class="line">   <span class="comment">//取消Map 映射</span></span><br><span class="line">   err = clEnqueueUnmapMemObject(command_queue,</span><br><span class="line">                                 input_buffer0,</span><br><span class="line">                                 map_input_buffer0,</span><br><span class="line">                                 <span class="number">0</span>,</span><br><span class="line">                                 <span class="literal">NULL</span>,</span><br><span class="line">                                 &amp;event1);</span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Failed to copy input dataset to OpenCL buffer\n"</span>);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_4DDR</span></span><br><span class="line">   <span class="comment">//Map input buffer for PCIe write</span></span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">char</span> *map_input_buffer1;</span><br><span class="line">   map_input_buffer1 = (<span class="keyword">unsigned</span> <span class="keyword">char</span> *) clEnqueueMapBuffer(command_queue,</span><br><span class="line">                                                           input_buffer1,</span><br><span class="line">                                                           CL_FALSE,</span><br><span class="line">                                                           CL_MAP_WRITE_INVALIDATE_REGION,</span><br><span class="line">                                                           <span class="number">0</span>,</span><br><span class="line">                                                           globalbuffersize,</span><br><span class="line">                                                           <span class="number">0</span>,</span><br><span class="line">                                                           <span class="literal">NULL</span>,</span><br><span class="line">                                                           <span class="literal">NULL</span>,</span><br><span class="line">                                                           &amp;err);</span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Failed to clEnqueueMapBuffer OpenCL buffer\n"</span>);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   clFinish(command_queue);</span><br><span class="line"></span><br><span class="line">   <span class="comment">//prepare data to be written to the device</span></span><br><span class="line">   <span class="keyword">for</span>(i=<span class="number">0</span>; i&lt;globalbuffersize; i++)</span><br><span class="line">       map_input_buffer1[i] = input_host[i];</span><br><span class="line"></span><br><span class="line">   cl_event event2;</span><br><span class="line">   err = clEnqueueUnmapMemObject(command_queue,</span><br><span class="line">                                 input_buffer1,</span><br><span class="line">                                 map_input_buffer1,</span><br><span class="line">                                 <span class="number">0</span>,</span><br><span class="line">                                 <span class="literal">NULL</span>,</span><br><span class="line">                                 &amp;event2);</span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Failed to copy input dataset to OpenCL buffer\n"</span>);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"Error: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">   <span class="comment">//execute kernel</span></span><br><span class="line">   <span class="keyword">int</span> arg_num = <span class="number">0</span>;</span><br><span class="line">   err  = <span class="number">0</span>;</span><br><span class="line">   err  = clSetKernelArg(kernel, arg_num++, <span class="keyword">sizeof</span>(cl_mem), &amp;input_buffer0);</span><br><span class="line">   err |= clSetKernelArg(kernel, arg_num++, <span class="keyword">sizeof</span>(cl_mem), &amp;output_buffer0);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_4DDR</span></span><br><span class="line">   err |= clSetKernelArg(kernel, arg_num++, <span class="keyword">sizeof</span>(cl_mem), &amp;input_buffer1);</span><br><span class="line">   err |= clSetKernelArg(kernel, arg_num++, <span class="keyword">sizeof</span>(cl_mem), &amp;output_buffer1);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">   err |= clSetKernelArg(kernel, arg_num++,  <span class="keyword">sizeof</span>(cl_ulong), &amp;num_blocks);</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Failed to set kernel arguments! %d\n"</span>, err);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">size_t</span> global[<span class="number">1</span>];</span><br><span class="line">   <span class="keyword">size_t</span> local[<span class="number">1</span>];</span><br><span class="line">   global[<span class="number">0</span>]=<span class="number">1</span>;</span><br><span class="line">   local[<span class="number">0</span>]=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">   cl_event ndrangeevent;</span><br><span class="line">   err = clEnqueueNDRangeKernel(command_queue, kernel, <span class="number">1</span>, <span class="literal">NULL</span>, global, local,</span><br><span class="line">                                <span class="number">0</span>, <span class="literal">NULL</span>, &amp;ndrangeevent);</span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Failed to execute kernel %d\n"</span>, err);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   clFinish(command_queue);</span><br><span class="line"></span><br><span class="line">   <span class="comment">//copy results back from OpenCL buffer</span></span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">char</span> *map_output_buffer0;</span><br><span class="line">   map_output_buffer0 = (<span class="keyword">unsigned</span> <span class="keyword">char</span> *)clEnqueueMapBuffer(command_queue,</span><br><span class="line">                                                           output_buffer0,</span><br><span class="line">                                                           CL_FALSE,</span><br><span class="line">                                                           CL_MAP_READ,</span><br><span class="line">                                                           <span class="number">0</span>,</span><br><span class="line">                                                           globalbuffersize,</span><br><span class="line">                                                           <span class="number">0</span>,</span><br><span class="line">                                                           <span class="literal">NULL</span>,</span><br><span class="line">                                                           &amp;event1,</span><br><span class="line">                                                           &amp;err);</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Failed to read output size buffer %d\n"</span>, err);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">   &#125;</span><br><span class="line">   clFinish(command_queue);</span><br><span class="line"></span><br><span class="line">   <span class="comment">//check</span></span><br><span class="line">   <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;globalbuffersize; i++) &#123;</span><br><span class="line">       <span class="keyword">if</span> (map_output_buffer0[i] != input_host[i]) &#123;</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR : kernel failed to copy entry %i input0=%i output0=%i\n"</span>,</span><br><span class="line">                  i, input_host[i], map_output_buffer0[i]);</span><br><span class="line">           <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_4DDR</span></span><br><span class="line">   <span class="keyword">unsigned</span> <span class="keyword">char</span> *map_output_buffer1;</span><br><span class="line">   map_output_buffer1 = (<span class="keyword">unsigned</span> <span class="keyword">char</span> *)clEnqueueMapBuffer(command_queue,</span><br><span class="line">                                                           output_buffer1,</span><br><span class="line">                                                           CL_FALSE,</span><br><span class="line">                                                           CL_MAP_READ,</span><br><span class="line">                                                           <span class="number">0</span>,</span><br><span class="line">                                                           globalbuffersize,</span><br><span class="line">                                                           <span class="number">0</span>,</span><br><span class="line">                                                           <span class="literal">NULL</span>,</span><br><span class="line">                                                           &amp;event1,</span><br><span class="line">                                                           &amp;err);</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (err != CL_SUCCESS) &#123;</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Failed to read output size buffer %d\n"</span>, err);</span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">"ERROR: Test failed\n"</span>);</span><br><span class="line">       <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">   &#125;</span><br><span class="line">   clFinish(command_queue);</span><br><span class="line"></span><br><span class="line">   <span class="comment">//check</span></span><br><span class="line">   <span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;globalbuffersize; i++) &#123;</span><br><span class="line">       <span class="keyword">if</span> (map_output_buffer1[i] != input_host[i]) &#123;</span><br><span class="line">           <span class="built_in">printf</span>(<span class="string">"ERROR : kernel failed to copy entry %i input1=%i output1=%i\n"</span>,</span><br><span class="line">                  i, input_host[i], map_output_buffer1[i]);</span><br><span class="line">           <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="comment">//--------------------------------------------------------------------------</span></span><br><span class="line">   <span class="comment">//profiling information</span></span><br><span class="line">   <span class="comment">//--------------------------------------------------------------------------</span></span><br><span class="line">   <span class="keyword">uint64_t</span> nstimestart, nstimeend;</span><br><span class="line">   clGetEventProfilingInfo(ndrangeevent, CL_PROFILING_COMMAND_START, <span class="keyword">sizeof</span>(<span class="keyword">uint64_t</span>), ((<span class="keyword">void</span> *)(&amp;nstimestart)), <span class="literal">NULL</span>);</span><br><span class="line">   clGetEventProfilingInfo(ndrangeevent, CL_PROFILING_COMMAND_END,    <span class="keyword">sizeof</span>(<span class="keyword">uint64_t</span>), ((<span class="keyword">void</span> *)(&amp;nstimeend)),    <span class="literal">NULL</span>);</span><br><span class="line">   nsduration = nstimeend-nstimestart;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">double</span> dnsduration = ((<span class="keyword">double</span>)nsduration);</span><br><span class="line">   <span class="keyword">double</span> dsduration = dnsduration / ((<span class="keyword">double</span>) <span class="number">1000000000</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="keyword">double</span> bpersec = (dbytes*<span class="number">2.0</span>/dsduration);</span><br><span class="line">   <span class="keyword">double</span> mbpersec = bpersec / ((<span class="keyword">double</span>) <span class="number">1024</span>*<span class="number">1024</span> );</span><br><span class="line"></span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">"Kernel read %.0lf MB bytes from and wrote %.01f MB to global memory.\n"</span>, dmbytes, dmbytes);</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">"Execution time = %f (sec) \n"</span>, dsduration);</span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">"Concurrent Read and Write Throughput = %f (MB/sec) \n"</span>, mbpersec);</span><br><span class="line"></span><br><span class="line">   <span class="comment">//--------------------------------------------------------------------------</span></span><br><span class="line">   <span class="comment">//add clena up code</span></span><br><span class="line">   <span class="comment">//--------------------------------------------------------------------------</span></span><br><span class="line">   clReleaseMemObject(input_buffer0);</span><br><span class="line">   clReleaseMemObject(output_buffer0);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_4DDR</span></span><br><span class="line">   clReleaseMemObject(input_buffer1);</span><br><span class="line">   clReleaseMemObject(output_buffer1);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">   clReleaseKernel(kernel);</span><br><span class="line">   clReleaseProgram(program);</span><br><span class="line">   clReleaseCommandQueue(command_queue);</span><br><span class="line">   clReleaseContext(context);</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="内核代码分析"><a href="#内核代码分析" class="headerlink" title="内核代码分析"></a>内核代码分析</h2><h3 id="内核源码"><a href="#内核源码" class="headerlink" title="内核源码"></a>内核源码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> USE_4DDR</span></span><br><span class="line"></span><br><span class="line">__kernel</span><br><span class="line">__attribute__ ((reqd_work_group_size(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)))</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bandwidth</span><span class="params">(__global uint16  * __restrict input0,</span></span></span><br><span class="line"><span class="function"><span class="params">               __global uint16  * __restrict output0,</span></span></span><br><span class="line"><span class="function"><span class="params">#ifdef USE_4DDR</span></span></span><br><span class="line"><span class="function"><span class="params">               __global uint16  * __restrict input1,</span></span></span><br><span class="line"><span class="function"><span class="params">               __global uint16  * __restrict output1,</span></span></span><br><span class="line"><span class="function"><span class="params">#endif</span></span></span><br><span class="line"><span class="function"><span class="params">               ulong num_blocks)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    ulong blockindex;</span><br><span class="line">    uint16 temp0, temp1;</span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    blockindex = <span class="number">0</span>;</span><br><span class="line">    __attribute__((xcl_pipeline_loop))</span><br><span class="line">    <span class="keyword">for</span> (blockindex=<span class="number">0</span>; blockindex&lt;num_blocks; blockindex++) &#123;</span><br><span class="line">        temp0 = input0[blockindex];</span><br><span class="line">        output0[blockindex] = temp0;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_4DDR</span></span><br><span class="line">        temp1 = input1[blockindex];</span><br><span class="line">        output1[blockindex] = temp1;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>GUI Setting:<br><img src="http://xukeqiniu.xukeai.cn/5a9c71bb0e48d710533b11b2be3aa5b4.png" alt=""></li><li>Makefile Setting:<br><img src="http://xukeqiniu.xukeai.cn/95089ad51b2da43e1e43cdae2691f544.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/45051bcf3eaa6c329e1fa1d90d785ad0.png" alt=""></li></ul><h3 id="综合报表"><a href="#综合报表" class="headerlink" title="综合报表"></a>综合报表</h3><p><img src="http://xukeqiniu.xukeai.cn/fce920f90e5cbcd30b5f0c12a3c368c1.png" alt=""></p><h3 id="Performence图"><a href="#Performence图" class="headerlink" title="Performence图"></a>Performence图</h3><p><img src="http://xukeqiniu.xukeai.cn/93874202f0f536a07d15a0355583fd5e.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/640479e2f81d9756031aa0154479c5c6.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/47451ffff0ce3738a6b251b047b37bc3.png" alt=""></p><h2 id="关键理解概念描述"><a href="#关键理解概念描述" class="headerlink" title="关键理解概念描述"></a>关键理解概念描述</h2><p><img src="http://xukeqiniu.xukeai.cn/20e9cdb7babfa3c7e6c6b025f210fc99.png" alt=""></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ ./test_memaccess.exe  binary_container_1.xclbin</span><br><span class="line">Linux:4.10.1-041001-generic:#201702260735 SMP Sun Feb 26 12:36:48 UTC 2017:x86_64</span><br><span class="line">---</span><br><span class="line">XILINX_OPENCL=&quot;/opt/xil-kcu1500/xbinst&quot;</span><br><span class="line">LD_LIBRARY_PATH=&quot;/opt/xil-kcu1500/xbinst/runtime/lib/x86_64:/opt/Xilinx/SDx/2017.2/runtime/lib/x86_64:/opt/Xilinx/SDx/2017.2/lib/lnx64.o&quot;</span><br><span class="line">---</span><br><span class="line">CL_PLATFORM_VENDOR Xilinx</span><br><span class="line">CL_PLATFORM_NAME Xilinx</span><br><span class="line">Selected xilinx_kcu1500_4ddr-xpr_4_0 as the target device</span><br><span class="line">loading binary_container_1.xclbin</span><br><span class="line">Starting kernel to read/write 2048 MB bytes from/to global memory...</span><br><span class="line">Kernel read 2048 MB bytes from and wrote 2048.0 MB to global memory.</span><br><span class="line">Execution time = 0.070036 (sec)</span><br><span class="line">Concurrent Read and Write Throughput = 58484.212297 (MB/sec)</span><br></pre></td></tr></table></figure><p>实验结果分析：<br>时钟频率为300Mhz<br>4块DDR独立进行使用，DDR0 –&gt; input0 (1GB) ; DDR1  &lt;– output0 (1GB)  ;  DDR2 –&gt; input1 (1GB) ; DDR3  &lt;– output1 (1GB)<br>共传输4GB的数据，用时0.070036s  总带宽：4096/0.070036 = 58484.212297 (MB/sec)<br>每个通道的带宽为 1024/0.070036  = 14621.052030 (MB/sec)<br>DDR3数据吞吐率为：300MHZ * 512bit / 8 = 19200 (MB/sec)<br>接口位宽利用率： 14621.052030 / 19200  = 76.1513%</p>]]></content>
      
      <categories>
          
          <category> SDAccel </category>
          
          <category> 优化 </category>
          
          <category> kernel_to_gmem </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SDAccel </tag>
            
            <tag> 优化 </tag>
            
            <tag> kernel_to_gmem </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MXNet model zoo 多网络推理</title>
      <link href="/2017/11/09/DeepLearning/mxnet/MXNET%20%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%90%91%E6%8E%A8%E7%90%86/"/>
      <url>/2017/11/09/DeepLearning/mxnet/MXNET%20%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%BD%91%E7%BB%9C%E6%AD%A3%E5%90%91%E6%8E%A8%E7%90%86/</url>
      <content type="html"><![CDATA[<h3 id="预备工作"><a href="#预备工作" class="headerlink" title="预备工作"></a>预备工作</h3><ul><li>从MXNET model zoo 中下载对应的模型参数及json配置文件 <a href="http://mxnet.incubator.apache.org/model_zoo/index.html" target="_blank" rel="noopener">链接</a></li><li>下载 synset.txt 便于程序读取分类结果</li></ul><h3 id="源码实现"><a href="#源码实现" class="headerlink" title="源码实现"></a>源码实现</h3><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2,sys,time</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadModel</span><span class="params">(modelname)</span>:</span></span><br><span class="line">        t1 = time.time()</span><br><span class="line">        sym, arg_params, aux_params = mx.model.load_checkpoint(modelname, <span class="number">0</span>)</span><br><span class="line">        t2 = time.time()</span><br><span class="line">        t = <span class="number">1000</span>*(t2-t1)</span><br><span class="line">        print(<span class="string">"Loaded in %2.2f milliseconds"</span> % t)</span><br><span class="line"> <span class="comment">#       arg_params['prob_label'] = mx.nd.array([0])</span></span><br><span class="line"> <span class="comment">#       arg_params['softmax_label'] = mx.nd.array([0])</span></span><br><span class="line">        mod = mx.mod.Module(symbol=sym , context=mx.gpu() , label_names=<span class="keyword">None</span>)</span><br><span class="line">        mod.bind(for_training=<span class="keyword">False</span>, data_shapes=[(<span class="string">'data'</span>, (<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))])</span><br><span class="line">        mod.set_params(arg_params,aux_params,allow_missing=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> mod</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadCategories</span><span class="params">()</span>:</span></span><br><span class="line">        synsetfile = open(<span class="string">'../picture/synset.txt'</span>, <span class="string">'r'</span>)</span><br><span class="line">        synsets = []</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> synsetfile:</span><br><span class="line">                synsets.append(l.rstrip())</span><br><span class="line">        <span class="keyword">return</span> synsets</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepareNDArray</span><span class="params">(filename)</span>:</span></span><br><span class="line">        img = cv2.imread(filename)</span><br><span class="line">        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">        img = cv2.resize(img, (<span class="number">224</span>, <span class="number">224</span>,))</span><br><span class="line">        img = np.swapaxes(img, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        img = np.swapaxes(img, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        img = img[np.newaxis, :]</span><br><span class="line">        <span class="keyword">return</span> mx.nd.array(img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(filename, model, categories, n)</span>:</span></span><br><span class="line">        array = prepareNDArray(filename)</span><br><span class="line">        Batch = namedtuple(<span class="string">'Batch'</span>, [<span class="string">'data'</span>])</span><br><span class="line">        t1 = time.time()</span><br><span class="line">        model.forward(Batch([array]))</span><br><span class="line">        t2 = time.time()</span><br><span class="line">        t = <span class="number">1000</span>*(t2-t1)</span><br><span class="line">        print(<span class="string">"Predicted in %2.2f millsecond"</span> % t)</span><br><span class="line">        prob = model.get_outputs()[<span class="number">0</span>].asnumpy()</span><br><span class="line">        prob = np.squeeze(prob)</span><br><span class="line">        sortedprobindex = np.argsort(prob)[::<span class="number">-1</span>]</span><br><span class="line">        topn = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> sortedprobindex[<span class="number">0</span>:n]:</span><br><span class="line">                topn.append((prob[i], categories[i]))</span><br><span class="line">        <span class="keyword">return</span> topn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">(modelname)</span>:</span></span><br><span class="line">        model = loadModel(modelname)</span><br><span class="line">        cats = loadCategories()</span><br><span class="line">        <span class="keyword">return</span> model, cats</span><br></pre></td></tr></table></figure><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">filename = sys.argv[<span class="number">1</span>]</span><br><span class="line">print(<span class="string">"*** Inception v3"</span>)</span><br><span class="line">inceptionv3,c = init(<span class="string">"../model/Inception-BN/Inception-BN"</span>)</span><br><span class="line">print(predict(<span class="string">'../picture/cat.jpg'</span>,inceptionv3,c,<span class="number">1</span>))</span><br><span class="line">print(<span class="string">"*** squeezenet_v1.0"</span>)</span><br><span class="line">squeeze_v1_0,c = init(<span class="string">"../model/squeezenet/squeezenet_v1.0"</span>)</span><br><span class="line">print(predict(<span class="string">'../picture/cat.jpg'</span>,squeeze_v1_0,c,<span class="number">1</span>))</span><br><span class="line">print(<span class="string">"*** squeezenet_v1.1"</span>)</span><br><span class="line">squeeze_v1_1,c = init(<span class="string">"../model/squeezenet/squeezenet_v1.1"</span>)</span><br><span class="line">print(predict(<span class="string">'../picture/cat.jpg'</span>,squeeze_v1_1,c,<span class="number">1</span>))</span><br><span class="line">print(<span class="string">"*** nin"</span>)</span><br><span class="line">nin,c = init(<span class="string">"../model/nin/nin"</span>)</span><br><span class="line">print(predict(<span class="string">'../picture/cat.jpg'</span>,nin,c,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>*** Inception v3Loaded in 20.42 millisecondsPredicted in 0.19 millsecond[(0.34680301, &apos;n02112018 Pomeranian&apos;)]*** squeezenet_v1.0Loaded in 2.94 millisecondsPredicted in 0.08 millsecond[(0.22845435, &apos;n02326432 hare&apos;)]*** squeezenet_v1.1Loaded in 2.93 millisecondsPredicted in 0.08 millsecond[(0.71776724, &apos;n02123045 tabby, tabby cat&apos;)]*** ninLoaded in 5.85 millisecondsPredicted in 0.14 millsecond[(0.67466462, &apos;n02119022 red fox, Vulpes vulpes&apos;)]</code></pre>]]></content>
      
      <categories>
          
          <category> Deep Learning </category>
          
          <category> mxnet </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mxnet </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2017/11/07/hello-world/"/>
      <url>/2017/11/07/hello-world/</url>
      <content type="html"><![CDATA[<blockquote class="blockquote-center"><br>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.<br></blockquote><div class="note default"><p>default default default</p></div><div class="note primary"><p>primary primary primary</p></div><div class="note success"><p>success success success</p></div><div class="note info"><p>info info info</p></div><div class="note danger"><p>danger danger danger</p></div><div class="note info"><p>info</p></div><!-- HTML方式: 直接在 Markdown 文件中编写 HTML 来调用 --><!-- 其中 class="blockquote-center" 是必须的 --><blockquote class="blockquote-center">blah blah blah</blockquote><!-- 标签 方式，要求版本在0.4.5或以上 --><blockquote class="blockquote-center"><p>blah blah blah</p></blockquote><!-- 标签别名 --><blockquote class="blockquote-center"><p>blah blah blah </p></blockquote><h3 id="应用技巧"><a href="#应用技巧" class="headerlink" title="应用技巧"></a>应用技巧</h3><h4 id="自定义文字背景块"><a href="#自定义文字背景块" class="headerlink" title="自定义文字背景块"></a>自定义文字背景块</h4><p><span id="inline-red">红色</span><br><span id="inline-purple">紫色</span><br><span id="inline-brown">棕色</span><br><span id="inline-pink">粉红色</span><br><span id="inline-blue">蓝色</span><br><span id="inline-green">绿色</span></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><p>$ index_z_group = (global_z - padd_offset)/VEC_SIZE $<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure></p><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><p><img src="http://xukeqiniu.xukeai.cn/%E5%8D%B7%E7%A7%AF%E8%BF%90%E8%A1%8C%E8%BF%87%E7%A8%8B%E7%A4%BA%E6%84%8F.gif" alt=""><br><a id="more"></a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><h4 id="插入公式"><a href="#插入公式" class="headerlink" title="插入公式"></a>插入公式</h4><p>行內公式 $\sin ^{ 2 }{ \theta +\cos ^{ 2 }{ \theta =1 }  }$ 行內公式<br>$f(x)=ax+b$</p><h4 id="插入视频"><a href="#插入视频" class="headerlink" title="插入视频"></a>插入视频</h4><ul><li><p>直接插入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;iframe src=&quot;https://www.bilibili.com/blackboard/player.html?aid=16516956&amp;as_wide=1&quot; frameborder=&quot;0&quot; width=&quot;640&quot; height=&quot;430&quot; allowfullscreen&gt;&lt;/iframe&gt;</span><br></pre></td></tr></table></figure></li><li><p>插件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-tag-bilibili</span><br><span class="line">使用方法</span><br><span class="line">&#123;% bilibili [av_id] %&#125;</span><br></pre></td></tr></table></figure></li></ul><div class="bili_video"><embed height="452" width="544" quality="high" allowfullscreen="true" type="application/x-shockwave-flash" src="http://share.acg.tv/flash.swf" flashvars="aid=NaN&page=1" pluginspage="http://www.adobe.com/shockwave/download/download.cgi?P1_Prod_Version=ShockwaveFlash"></div><p><span id="inline-red">红色</span><br>More info:<br> <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      <categories>
          
          <category> test </category>
          
      </categories>
      
      
        <tags>
            
            <tag> test </tag>
            
            <tag> hello web </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
