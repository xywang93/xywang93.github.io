<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>异构 AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://xywang93.github.io.git/"/>
  <updated>2019-01-18T08:02:24.418Z</updated>
  <id>http://xywang93.github.io.git/</id>
  
  <author>
    <name>王晓芸</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>树莓派NCNN环境搭建</title>
    <link href="http://xywang93.github.io.git/2018/05/07/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/02-%E6%A0%91%E8%8E%93%E6%B4%BEncnn%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>http://xywang93.github.io.git/2018/05/07/DeepLearning/嵌入式AI/玩转ncnn/02-树莓派ncnn环境搭建/</id>
    <published>2018-05-06T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:24.418Z</updated>
    
    <content type="html"><![CDATA[<h2 id="树莓派NCNN环境搭建"><a href="#树莓派NCNN环境搭建" class="headerlink" title="树莓派NCNN环境搭建"></a>树莓派NCNN环境搭建</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>镜像已经做好了，传到百度网盘中了（请大家及时保存，不定期删除！）</p><p>链接: <a href="https://pan.baidu.com/s/1fhiX86L8iL8tsLbsiVa6Wg" target="_blank" rel="noopener">https://pan.baidu.com/s/1fhiX86L8iL8tsLbsiVa6Wg</a> 密码: e64s</p><p><strong>SD卡要求：至少16GB，板卡型号为树莓派3B+（其他型号未知）</strong></p><h3 id="板卡介绍"><a href="#板卡介绍" class="headerlink" title="板卡介绍"></a>板卡介绍</h3><p>本系列教程采用树莓派3B+开发板：<br><img src="http://xukeqiniu.xukeai.cn/0a676958d61c43741f8cc45f04d3ea1b.png" alt=""><br>板卡特点：</p><ul><li>1.4GHz 64位4核 ARM Cortex-A53 CPU</li><li>双频 802.11ac 无线网卡和蓝牙 4.2</li><li>更快的以太网（千兆以太网 over USB 2.0）</li><li>1G LPDDR2</li><li>PoE 支持（Power-over-Ethernet，with PoE HAT）</li><li>改进 PXE 网络与 USB 大容量存储启动</li></ul><a id="more"></a><h3 id="系统安装"><a href="#系统安装" class="headerlink" title="系统安装"></a>系统安装</h3><h4 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h4><ul><li>SDFormatter(格式化SD卡)</li><li>win32diskimager（为SD卡烧写程序）<br>-<h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4></li></ul><p>首先用SDFormatter将插入的SD卡格式化。<br>然后用win32diskimager找到对应的镜像烧写到SD卡中（原始镜像采用2018-04-18-raspbian-stretch树莓派官方系统）</p><h3 id="更换源"><a href="#更换源" class="headerlink" title="更换源"></a>更换源</h3><h4 id="一步操作"><a href="#一步操作" class="headerlink" title="一步操作"></a>一步操作</h4><p>直接执行以下两步，即可替换将官方默认软件源替换为<br>中科大镜像源<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sed -i <span class="string">'s#://mirrordirector.raspbian.org#s://mirrors.ustc.edu.cn/raspbian#g'</span> /etc/apt/sources.list</span><br><span class="line">$ sudo sed -i <span class="string">'s#://archive.raspberrypi.org/debian#s://mirrors.ustc.edu.cn/archive.raspberrypi.org#g'</span> /etc/apt/sources.list.d/raspi.list</span><br></pre></td></tr></table></figure></p><p>或换为清华镜像源<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sed -i <span class="string">'s#://mirrordirector.raspbian.org#s://mirrors.tuna.tsinghua.edu.cn/raspbian#g'</span> /etc/apt/sources.list</span><br><span class="line">$ sudo sed -i <span class="string">'s#://archive.raspberrypi.org/debian#s://mirrors.tuna.tsinghua.edu.cn/raspberrypi#g'</span> /etc/apt/sources.list.d/raspi.list</span><br></pre></td></tr></table></figure></p><h4 id="或手动修改源"><a href="#或手动修改源" class="headerlink" title="或手动修改源"></a>或手动修改源</h4><ul><li>第一步：修改<code>sources.list</code><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/apt/sources.list</span><br></pre></td></tr></table></figure></li></ul><p>进入之后，屏蔽掉其他的源，输入以下源：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ deb http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi</span><br><span class="line">$ deb-src http://mirrors.tuna.tsinghua.edu.cn/raspbian/raspbian/ stretch main contrib non-free rpi</span><br></pre></td></tr></table></figure></p><ul><li>第二步：修改<code>raspi.list</code><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/apt/sources.list.d/raspi.list</span><br></pre></td></tr></table></figure></li></ul><p>进入之后，屏蔽掉其他的源，输入以下源：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ deb http://mirror.tuna.tsinghua.edu.cn/raspberrypi/ stretch main ui</span><br><span class="line">$ deb-src http://mirror.tuna.tsinghua.edu.cn/raspberrypi/ stretch main ui</span><br></pre></td></tr></table></figure></p><ul><li>第三部修改完源后更新升级系统<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get upgrade</span><br></pre></td></tr></table></figure></li></ul><h3 id="安装VScode"><a href="#安装VScode" class="headerlink" title="安装VScode"></a>安装VScode</h3><p>提供了VScode ARM安装包，直接安装！<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo dpkg -i code-oss_1.14.0-1497990172_armhf.deb</span><br></pre></td></tr></table></figure></p><h3 id="安装cmake工具"><a href="#安装cmake工具" class="headerlink" title="安装cmake工具"></a>安装cmake工具</h3><h4 id="安装cmake"><a href="#安装cmake" class="headerlink" title="安装cmake"></a>安装cmake</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install cmake</span><br></pre></td></tr></table></figure><h4 id="安装cmake-gui"><a href="#安装cmake-gui" class="headerlink" title="安装cmake-gui"></a>安装cmake-gui</h4><p>cmake-gui是可视化的cmake工具，便于配置。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install cmake-gui</span><br></pre></td></tr></table></figure></p><h3 id="安装-protobuf"><a href="#安装-protobuf" class="headerlink" title="安装 protobuf"></a>安装 protobuf</h3><p>下载 protobuf-2.6.1.tar.gz<br>安装<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment">#解压文件</span></span><br><span class="line">$ tar zxvf protobuf-2.6.1.tar.gz</span><br><span class="line">$ <span class="comment">#进入文件</span></span><br><span class="line">$ <span class="built_in">cd</span> protobuf-2.6.1/</span><br><span class="line">$ <span class="comment"># 配置</span></span><br><span class="line">$ ./configure</span><br><span class="line">$ <span class="comment"># 编译（编译过程尽量只用一个核 不要加 j4,j3,j2）</span></span><br><span class="line">$ make</span><br><span class="line">$ <span class="comment"># 编译检查</span></span><br><span class="line">$ make check</span><br><span class="line">$ <span class="comment"># 安装</span></span><br><span class="line">$ sudo make install</span><br><span class="line">$ <span class="comment"># 添加库路径 在/etc/ld.so.conf.d/目录下创建文件bprotobuf.conf文件</span></span><br><span class="line">$ <span class="built_in">cd</span> /etc/ld.so.conf.d/</span><br><span class="line">$ sudo touch bprotobuf.conf</span><br><span class="line">$ <span class="comment"># vim打开bprotobuf.conf文件输入/usr/local/lib</span></span><br><span class="line">$ sudo ldconfig</span><br><span class="line">$ <span class="comment"># 查看版本</span></span><br><span class="line">$ protoc –-version</span><br></pre></td></tr></table></figure></p><h3 id="安装opencv3-4"><a href="#安装opencv3-4" class="headerlink" title="安装opencv3.4"></a>安装opencv3.4</h3><h4 id="预装依赖库"><a href="#预装依赖库" class="headerlink" title="预装依赖库"></a>预装依赖库</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install build-essential git cmake pkg-config -y</span><br><span class="line">$ sudo apt-get install libjpeg8-dev -y</span><br><span class="line">$ sudo apt-get install libtiff5-dev -y</span><br><span class="line">$ sudo apt-get install libjasper-dev -y</span><br><span class="line">$ sudo apt-get install libpng12-dev -y</span><br><span class="line">$ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev -y</span><br><span class="line">$ sudo apt-get install libgtk2.0-dev -y</span><br><span class="line">$ sudo apt-get install libatlas-base-dev gfortran -y</span><br><span class="line">$ sudo apt-get install qt5-default -y</span><br></pre></td></tr></table></figure><p>安装numpy（命令行），第一次使用pip安装时可能会比较慢，耐心等待<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo pip3 install numpy</span><br></pre></td></tr></table></figure></p><p>查看numpy的安装目录并记录（<strong>后面编译时需要PYTHON3_NUMPY_INCLUDE_DIRS路径，如果不带numpy编译可能会卡住</strong>）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ python3</span><br><span class="line">&gt;&gt; <span class="keyword">import</span> numpy</span><br><span class="line">&gt;&gt; numpy.__path__</span><br><span class="line">&gt;&gt; quit()</span><br></pre></td></tr></table></figure></p><h4 id="下载并解压OpenCV"><a href="#下载并解压OpenCV" class="headerlink" title="下载并解压OpenCV"></a>下载并解压OpenCV</h4><p>然后下载OpenCV库和Contrib库（强烈建议在其他环境下载然后拷贝过来），如果需要其他版本，就修改后面的版本号<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/Itseez/opencv/archive/3.4.0.zip</span><br><span class="line">https://github.com/Itseez/opencv_contrib/archive/3.4.0.zip</span><br></pre></td></tr></table></figure></p><h4 id="cmake配置"><a href="#cmake配置" class="headerlink" title="cmake配置"></a>cmake配置</h4><p>解压后在opencv-3.4.0文件夹里创建build文件夹，然后在命令行里面cd到此文件夹，开始cmake，以下内容为一行<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_LIBV4L=ON PYTHON3_EXECUTABLE=/usr/bin/python3.5 PYTHON_INCLUDE_DIR=/usr/include/python3.5 PYTHON_LIBRARY=/usr/lib/arm-linux-gnueabihf/libpython3.5m.so PYTHON3_NUMPY_INCLUDE_DIRS=/home/pi/.<span class="built_in">local</span>/lib/python3.5/site-packages/numpy/core/include ..</span><br><span class="line"></span><br><span class="line"><span class="comment">## 备选方案</span></span><br><span class="line">sudo aptitude search libgtk2.0-dev</span><br><span class="line"></span><br><span class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON -D WITH_QT=ON -D WITH_GTK=ON -D WITH_OPENGL=ON ..</span><br></pre></td></tr></table></figure></p><h4 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h4><p>执行如下操作：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编译（编译过程尽量只用一个核 不要加 j4,j3,j2）</span></span><br><span class="line">$ make</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure></p><h3 id="编译NCNN"><a href="#编译NCNN" class="headerlink" title="编译NCNN"></a>编译NCNN</h3><h4 id="下载NCNN"><a href="#下载NCNN" class="headerlink" title="下载NCNN"></a>下载NCNN</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/Tencent/ncnn.github</span><br></pre></td></tr></table></figure><h4 id="需要安装的依赖库"><a href="#需要安装的依赖库" class="headerlink" title="需要安装的依赖库"></a>需要安装的依赖库</h4><ul><li>protobuf</li><li>opencv</li></ul><h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ncnn</span><br><span class="line">$ mkdir build</span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make</span><br></pre></td></tr></table></figure><h3 id="NCNN测试"><a href="#NCNN测试" class="headerlink" title="NCNN测试"></a>NCNN测试</h3><p>修改项目根目录下的<code>CMakeLists.txt</code>文件，定位到最后几行<br><img src="http://xukeqiniu.xukeai.cn/22dde3b5cb5f30c9939efc595c59c389.png" alt=""></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make</span><br><span class="line">$ cp examples/squeezenet_v1.1.param  build/examples</span><br><span class="line">$ cp examples/squeezenet_v1.1.bin    build/examples</span><br><span class="line">$ <span class="built_in">cd</span> build/examples</span><br><span class="line">$ ./squeezenet cat.jpg</span><br></pre></td></tr></table></figure><p>结果：<br><img src="http://xukeqiniu.xukeai.cn/408d7bac009ae9ba743ed1bff5cb996c.png" alt=""></p><h3 id="制作img系统镜像"><a href="#制作img系统镜像" class="headerlink" title="制作img系统镜像"></a>制作img系统镜像</h3><ul><li><p>安装dcfldd工具</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install dcfldd</span><br></pre></td></tr></table></figure></li><li><p>将已经安装好软件的内存卡插到pc机上，在虚拟机中打开可移动设备。</p></li><li>查看备份设备<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /dev</span><br><span class="line">ls</span><br></pre></td></tr></table></figure></li></ul><p>可以看到插入设备(整体)为sdb，其中有几个分区(sdb1、sdb2、sdb3.)<br><img src="http://xukeqiniu.xukeai.cn/4ecc8595baa8f4eb357a3fee595864d8.png" alt=""></p><ul><li>备份分区</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dcfldd <span class="keyword">if</span>=/dev/sdb of=/root/pi.img</span><br></pre></td></tr></table></figure><blockquote><ul><li>由于我们是将整个内存卡的全部做成镜像(<strong>启动+系统</strong>)，所以输入文件为sdb，若只做系统部分的镜像，可以只选择sdb2.</li><li>备份包括输入文件(if)以及被设置为/root目录下名为pi.img的输出文件(of)。</li></ul></blockquote><h3 id="调整SD卡分区"><a href="#调整SD卡分区" class="headerlink" title="调整SD卡分区"></a>调整SD卡分区</h3><h4 id="使用工具："><a href="#使用工具：" class="headerlink" title="使用工具："></a>使用工具：</h4><p>Linux Ubuntu gparted分区工具</p><h4 id="安装gparted"><a href="#安装gparted" class="headerlink" title="安装gparted"></a>安装gparted</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install gparted</span><br></pre></td></tr></table></figure><h4 id="调整分区容量大小"><a href="#调整分区容量大小" class="headerlink" title="调整分区容量大小"></a>调整分区容量大小</h4><p><img src="http://xukeqiniu.xukeai.cn/df4e237083510783efa110102bbde7b9.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/72c3d1a1f09fbe64faf3ce8af4acce9b.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/34b7f42f56e8d72a93139c6adf6717c5.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/c47170d5af9ce81d2dad4ae897b3145e.png" alt=""></p><p><img src="http://xukeqiniu.xukeai.cn/6e6481769c1a86d76c8d6df3e087fdb0.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.jianshu.com/p/67b9e6ebf8a0" target="_blank" rel="noopener">https://www.jianshu.com/p/67b9e6ebf8a0</a><br><a href="http://www.rabbit8.cn/609.html" target="_blank" rel="noopener">http://www.rabbit8.cn/609.html</a><br><a href="https://linux.cn/article-8477-1.html" target="_blank" rel="noopener">https://linux.cn/article-8477-1.html</a><br><a href="https://www.cnblogs.com/Pyrokine/p/8921285.html" target="_blank" rel="noopener">https://www.cnblogs.com/Pyrokine/p/8921285.html</a><br><a href="https://stackoverflow.com/questions/28776053/opencv-gtk2-x-error" target="_blank" rel="noopener">https://stackoverflow.com/questions/28776053/opencv-gtk2-x-error</a></p><p>相关人工智能与异构计算的知识分享，欢迎关注我的公众号<strong>AI异构</strong><br><img src="http://xukeqiniu.xukeai.cn/64b3b3bf7b3e12ed0f62f94fd001cffb.png" alt="动动手指关注下吧！"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;树莓派NCNN环境搭建&quot;&gt;&lt;a href=&quot;#树莓派NCNN环境搭建&quot; class=&quot;headerlink&quot; title=&quot;树莓派NCNN环境搭建&quot;&gt;&lt;/a&gt;树莓派NCNN环境搭建&lt;/h2&gt;&lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;镜像已经做好了，传到百度网盘中了（请大家及时保存，不定期删除！）&lt;/p&gt;
&lt;p&gt;链接: &lt;a href=&quot;https://pan.baidu.com/s/1fhiX86L8iL8tsLbsiVa6Wg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://pan.baidu.com/s/1fhiX86L8iL8tsLbsiVa6Wg&lt;/a&gt; 密码: e64s&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SD卡要求：至少16GB，板卡型号为树莓派3B+（其他型号未知）&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;板卡介绍&quot;&gt;&lt;a href=&quot;#板卡介绍&quot; class=&quot;headerlink&quot; title=&quot;板卡介绍&quot;&gt;&lt;/a&gt;板卡介绍&lt;/h3&gt;&lt;p&gt;本系列教程采用树莓派3B+开发板：&lt;br&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/0a676958d61c43741f8cc45f04d3ea1b.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;板卡特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1.4GHz 64位4核 ARM Cortex-A53 CPU&lt;/li&gt;
&lt;li&gt;双频 802.11ac 无线网卡和蓝牙 4.2&lt;/li&gt;
&lt;li&gt;更快的以太网（千兆以太网 over USB 2.0）&lt;/li&gt;
&lt;li&gt;1G LPDDR2&lt;/li&gt;
&lt;li&gt;PoE 支持（Power-over-Ethernet，with PoE HAT）&lt;/li&gt;
&lt;li&gt;改进 PXE 网络与 USB 大容量存储启动&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="嵌入式AI" scheme="http://xywang93.github.io.git/categories/Deep-Learning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/"/>
    
      <category term="NCNN" scheme="http://xywang93.github.io.git/categories/Deep-Learning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/NCNN/"/>
    
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
      <category term="嵌入式AI" scheme="http://xywang93.github.io.git/tags/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/"/>
    
      <category term="NCNN" scheme="http://xywang93.github.io.git/tags/NCNN/"/>
    
      <category term="入门" scheme="http://xywang93.github.io.git/tags/%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>NCNN ResNet-50 部署</title>
    <link href="http://xywang93.github.io.git/2018/05/05/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/01-ncnn%20resnet%E9%83%A8%E7%BD%B2/"/>
    <id>http://xywang93.github.io.git/2018/05/05/DeepLearning/嵌入式AI/玩转ncnn/01-ncnn resnet部署/</id>
    <published>2018-05-04T16:00:00.000Z</published>
    <updated>2018-07-30T15:07:42.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="说在前面"><a href="#说在前面" class="headerlink" title="说在前面"></a>说在前面</h3><p>因为NCNN的部署是不依赖任何第三方库的，所以在模型部署的过程中，只需要了解CMake编译工程的步骤。具体详见<a href="http://www.hahack.com/codes/cmake/" target="_blank" rel="noopener">CMake 入门实践</a></p><h3 id="下载ResNet-50模型"><a href="#下载ResNet-50模型" class="headerlink" title="下载ResNet-50模型"></a>下载ResNet-50模型</h3><p><a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank" rel="noopener">caffe model zoo</a>中有对应的各种不同版本的训练好的model,因为是部署，所以我们需要的仅仅是对应的caffemodel文件和deploy.prototxt文件。</p><a id="more"></a><h3 id="转换ncnn网络和模型"><a href="#转换ncnn网络和模型" class="headerlink" title="转换ncnn网络和模型"></a>转换ncnn网络和模型</h3><ul><li>首先建立resnet源文件目录</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">$ <span class="built_in">cd</span> examples</span><br><span class="line">$ mkdir resnet</span><br><span class="line">$ <span class="built_in">cd</span> resnet</span><br><span class="line">$ mkdir model</span><br></pre></td></tr></table></figure><p>将下载好的ResNet-50源文件复制到model文件夹中如下所示：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ root@a488d431ffee:/home/dl/ncnn/examples/resnet/model<span class="comment"># ls</span></span><br><span class="line">ResNet-50-deploy.prototxt  ResNet-50-model.caffemodel</span><br></pre></td></tr></table></figure></p><ul><li>caffe 自带了工具可以把老版本的 caffe 网络和模型转换为新版（ncnn的工具只认识新版）</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">upgrade_net_proto_text ResNet-50-deploy.prototxt  ncnn-ResNet-50-deploy.prototxt</span><br><span class="line">upgrade_net_proto_binary ResNet-50-model.caffemodel ncnn-ResNet-50-model.caffemodel</span><br></pre></td></tr></table></figure><ul><li>输入层改用 Input，因为每次只需要做一个图片，所以第一个 dim 设为 1</li></ul><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"input"</span></span><br><span class="line">  <span class="built_in">type</span>: <span class="string">"Input"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  input_param &#123;</span><br><span class="line">    shape &#123;</span><br><span class="line">      dim: 1</span><br><span class="line">      dim: 3</span><br><span class="line">      dim: 224</span><br><span class="line">      dim: 224</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>使用 caffe2ncnn 工具转换为 ncnn 的网络描述和模型</li></ul><p>注意caffe2ncnn工具在</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">$ <span class="built_in">cd</span> build/tools/caffe/</span><br><span class="line">$ caffe2ncnn ../../../examples/resnet/model/ncnn-ResNet-50-deploy.prototxt  ../../../examples/resnet/model/ncnn-ResNet-50-model.caffemodel ../../../examples/resnet/model/resnet-50.param  ../../../examples/resnet/model/resnet-50.bin</span><br></pre></td></tr></table></figure><p>准备工作全部完成如下,我们最终需要的是<code>resnet-50.bin</code>和<code>resnet-50.param</code>两个文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@a488d431ffee:/home/dl/ncnn/examples/resnet/model<span class="comment"># ls</span></span><br><span class="line">ResNet-50-deploy.prototxt  ResNet-50-model.caffemodel  ncnn-ResNet-50-deploy.prototxt  ncnn-ResNet-50-model.caffemodel  resnet-50.bin  resnet-50.param</span><br></pre></td></tr></table></figure></p><h3 id="建立ResNet工程"><a href="#建立ResNet工程" class="headerlink" title="建立ResNet工程"></a>建立ResNet工程</h3><ul><li><p>需要修改对应的CMakeLists.txt</p><ul><li><p>首先修改examples文件夹下的CMakeLists.txt文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最后一行添加</span></span><br><span class="line">add_subdirectory(resnet)</span><br></pre></td></tr></table></figure></li><li><p>然后进入resnet文件夹，创建CMakeLists.txt文件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">find_package(OpenCV REQUIRED core highgui )</span><br><span class="line"></span><br><span class="line">include_directories(<span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/../../src)</span><br><span class="line">include_directories(<span class="variable">$&#123;CMAKE_CURRENT_BINARY_DIR&#125;</span>/../../src)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">include_directories(<span class="variable">$&#123;CMAKE_CURRENT_BINARY_DIR&#125;</span>)</span><br><span class="line"></span><br><span class="line">add_executable(resnet-50 resnet_50.cpp)</span><br><span class="line">target_link_libraries(resnet-50 ncnn <span class="variable">$&#123;OpenCV_LIBS&#125;</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>编写resnet_50.cpp（仿照SqueezeNet）</p></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"net.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">detect_resnet</span><span class="params">(<span class="keyword">const</span> cv::Mat&amp; bgr, <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;&amp; cls_scores)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ncnn::Net resnet;</span><br><span class="line">    resnet.load_param(<span class="string">"resnet-50.param"</span>);</span><br><span class="line">    resnet.load_model(<span class="string">"resnet-50.bin"</span>);</span><br><span class="line"></span><br><span class="line">    ncnn::Mat in = ncnn::Mat::from_pixels_resize(bgr.data, ncnn::Mat::PIXEL_BGR, bgr.cols, bgr.rows, <span class="number">224</span>, <span class="number">224</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> mean_vals[<span class="number">3</span>] = &#123;<span class="number">104.f</span>, <span class="number">117.f</span>, <span class="number">123.f</span>&#125;;</span><br><span class="line">    in.substract_mean_normalize(mean_vals, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    ncnn::Extractor ex = resnet.create_extractor();</span><br><span class="line">    ex.set_light_mode(<span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">    ex.input(<span class="string">"data"</span>, in);</span><br><span class="line"></span><br><span class="line">    ncnn::Mat out;</span><br><span class="line">    ex.extract(<span class="string">"prob"</span>, out);</span><br><span class="line"></span><br><span class="line">    cls_scores.resize(out.c);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;out.c; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">float</span>* prob = out.data + out.cstep * j;</span><br><span class="line">        cls_scores[j] = prob[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">print_topk</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;&amp; cls_scores, <span class="keyword">int</span> topk, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; index_result, <span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;&amp; score_result)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// partial sort topk with index</span></span><br><span class="line">    <span class="keyword">int</span> size = cls_scores.size();</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt; <span class="built_in">std</span>::pair&lt;<span class="keyword">float</span>, <span class="keyword">int</span>&gt; &gt; vec;</span><br><span class="line">    vec.resize(size);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;size; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        vec[i] = <span class="built_in">std</span>::make_pair(cls_scores[i], i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::partial_sort(vec.begin(), vec.begin() + topk, vec.end(), <span class="built_in">std</span>::greater&lt; <span class="built_in">std</span>::pair&lt;<span class="keyword">float</span>, <span class="keyword">int</span>&gt; &gt;());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// print topk and score</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;topk; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">float</span> score = vec[i].first;</span><br><span class="line">        <span class="keyword">int</span> index = vec[i].second;</span><br><span class="line">index_result.push_back(index);</span><br><span class="line">        score_result.push_back(score);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//fprintf(stderr, "%d = %f\n", index, score);</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">load_labels</span><span class="params">(<span class="built_in">string</span> path, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; labels)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    FILE* fp = fopen(path.c_str(), <span class="string">"r"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (!feof(fp))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">char</span> str[<span class="number">1024</span>];</span><br><span class="line">        fgets(str, <span class="number">1024</span>, fp);</span><br><span class="line">        <span class="function"><span class="built_in">string</span> <span class="title">str_s</span><span class="params">(str)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (str_s.length() &gt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; str_s.length(); i++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span> (str_s[i] == <span class="string">' '</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="built_in">string</span> strr = str_s.substr(i, str_s.length() - i - <span class="number">1</span>);</span><br><span class="line">                    labels.push_back(strr);</span><br><span class="line">                    i = str_s.length();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span>* imagepath = argv[<span class="number">1</span>];</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; labels;</span><br><span class="line">    load_labels(<span class="string">"synset_words.txt"</span>, labels);</span><br><span class="line">    cv::Mat m = cv::imread(imagepath, CV_LOAD_IMAGE_COLOR);</span><br><span class="line">    <span class="keyword">if</span> (m.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"cv::imread %s failed\n"</span>, imagepath);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; cls_scores;</span><br><span class="line">    detect_resnet(m, cls_scores);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; index;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; score;</span><br><span class="line">    print_topk(cls_scores, <span class="number">3</span>, index, score);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index.size(); i++)</span><br><span class="line">    &#123;</span><br><span class="line">       <span class="built_in">cout</span> &lt;&lt; labels[index[i]] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make</span><br><span class="line">$ <span class="built_in">cd</span> resnet</span><br><span class="line">$ cp ../../../examples/resnet/model/resnet-50.param  ./</span><br><span class="line">$ cp ../../../examples/resnet/model/resnet-50.bin    ./</span><br><span class="line">$ ./resnet-50 cat.jpg</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Pembroke, Pembroke Welsh corgi</span><br><span class="line">Egyptian cat</span><br><span class="line">red fox, Vulpes vulpes</span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://www.hahack.com/codes/cmake/" target="_blank" rel="noopener">CMake 入门实践</a><br><a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_blank" rel="noopener">caffe model zoo</a><br><a href="https://github.com/Tencent/ncnn/wiki/ncnn-%E7%BB%84%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8C%97-alexnet" target="_blank" rel="noopener">ncnn 组件使用指北 alexnet</a><br><a href="http://blog.csdn.net/best_coder/article/details/76201275" target="_blank" rel="noopener">Ubuntu16.04—腾讯NCNN框架入门到应用</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;说在前面&quot;&gt;&lt;a href=&quot;#说在前面&quot; class=&quot;headerlink&quot; title=&quot;说在前面&quot;&gt;&lt;/a&gt;说在前面&lt;/h3&gt;&lt;p&gt;因为NCNN的部署是不依赖任何第三方库的，所以在模型部署的过程中，只需要了解CMake编译工程的步骤。具体详见&lt;a href=&quot;http://www.hahack.com/codes/cmake/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CMake 入门实践&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;下载ResNet-50模型&quot;&gt;&lt;a href=&quot;#下载ResNet-50模型&quot; class=&quot;headerlink&quot; title=&quot;下载ResNet-50模型&quot;&gt;&lt;/a&gt;下载ResNet-50模型&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/BVLC/caffe/wiki/Model-Zoo&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;caffe model zoo&lt;/a&gt;中有对应的各种不同版本的训练好的model,因为是部署，所以我们需要的仅仅是对应的caffemodel文件和deploy.prototxt文件。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="嵌入式AI" scheme="http://xywang93.github.io.git/categories/Deep-Learning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/"/>
    
      <category term="NCNN" scheme="http://xywang93.github.io.git/categories/Deep-Learning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/NCNN/"/>
    
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
      <category term="嵌入式AI" scheme="http://xywang93.github.io.git/tags/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/"/>
    
      <category term="NCNN" scheme="http://xywang93.github.io.git/tags/NCNN/"/>
    
      <category term="ResNet" scheme="http://xywang93.github.io.git/tags/ResNet/"/>
    
      <category term="部署" scheme="http://xywang93.github.io.git/tags/%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>NCNN入门</title>
    <link href="http://xywang93.github.io.git/2018/05/04/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/00-ncnn%E5%85%A5%E9%97%A8/"/>
    <id>http://xywang93.github.io.git/2018/05/04/DeepLearning/嵌入式AI/玩转ncnn/00-ncnn入门/</id>
    <published>2018-05-03T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:24.412Z</updated>
    
    <content type="html"><![CDATA[<h3 id="NCNN-简介"><a href="#NCNN-简介" class="headerlink" title="NCNN 简介"></a>NCNN 简介</h3><p>ncnn 是一个为手机端极致优化的高性能神经网络前向计算框架。ncnn 从设计之初深刻考虑手机端的部署和使用。无第三方依赖，跨平台，手机端 cpu 的速度快于目前所有已知的开源框架。基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP，将 AI 带到你的指尖。ncnn 目前已在腾讯多款应用中使用，如 QQ，Qzone，微信，天天P图等。<br> ncnn与同类框架对比：<br><img src="http://xukeqiniu.xukeai.cn/340953f4416410ed2091d1de5f9946e6.png" alt=""></p><a id="more"></a><h3 id="功能概述"><a href="#功能概述" class="headerlink" title="功能概述"></a>功能概述</h3><ul><li>支持卷积神经网络，支持多输入和多分支结构，可计算部分分支</li></ul><p>ncnn 支持卷积神经网络结构，以及多分支多输入的复杂网络结构，如主流的 vgg、googlenet、resnet、squeezenet 等。计算时可以依据需求，先计算公共部分和 prob 分支，待 prob 结果超过阈值后，再计算 bbox 分支。如果 prob 低于阈值，则可以不计算 bbox 分支，减少计算量。</p><ul><li>无任何第三方库依赖，不依赖 BLAS/NNPACK 等计算框架</li></ul><p>cnn 不依赖任何第三方库，完全独立实现所有计算过程，不需要 BLAS/NNPACK 等数学计算库。</p><ul><li>纯 C++ 实现，跨平台，支持 android ios 等</li></ul><p>ncnn 代码全部使用 C/C++ 实现，跨平台的 cmake 编译系统，可在已知的绝大多数平台编译运行，如 Linux，Windows，MacOS，Android，iOS 等。由于 ncnn 不依赖第三方库，且采用 C++ 03 标准实现，只用到了 std::vector 和 std::string 两个 STL 模板，可轻松移植到其他系统和设备上。</p><ul><li>ARM NEON 汇编级良心优化，计算速度极快</li></ul><p>ncnn 为手机端 CPU 运行做了深度细致的优化，使用 ARM NEON 指令集实现卷积层，全连接层，池化层等大部分 CNN 关键层。对于寄存器压力较大的 armv7 架构，手工编写 neon 汇编，内存预对齐，cache 预缓存，排列流水线，充分利用一切硬件资源，防止编译器意外负优化。</p><ul><li>精细的内存管理和数据结构设计，内存占用极低</li></ul><p>在 ncnn 设计之初已考虑到手机上内存的使用限制，在卷积层、全连接层等计算量较大的层实现中，没有采用通常框架中的 im2col + 矩阵乘法，因为这种方式会构造出非常大的矩阵，消耗大量内存。因此，ncnn 采用原始的滑动窗口卷积实现，并在此基础上进行优化，大幅节省了内存。在前向网络计算过程中，ncnn 可自动释放中间结果所占用的内存，进一步减少内存占用。</p><ul><li>支持多核并行计算加速，ARM big.LITTLE cpu 调度优化</li></ul><p>ncnn 提供了基于 openmp 的多核心并行计算加速，在多核心 cpu 上启用后能够获得很高的加速收益。ncnn 提供线程数控制接口，可以针对每个运行实例分别调控，满足不同场景的需求。针对 ARM big.LITTLE 架构的手机 cpu，ncnn 提供了更精细的调度策略控制功能，能够指定使用大核心或者小核心，或者一起使用，获得极限性能和耗电发热之间的平衡。例如，只使用1个小核心，或只使用2个小核心，或只使用2个大核心，都尽在掌控之中。<br><img src="http://xukeqiniu.xukeai.cn/dcdd662eef37b3dd83357f07fa9831c2.png" alt=""></p><ul><li>整体库体积小于 500K，并可轻松精简到小于 300K</li></ul><p>ncnn 自身没有依赖项，且体积很小，默认编译选项下的库体积小于 500K，能够有效减轻手机 APP 安装包大小负担。此外，ncnn 在编译时可自定义是否需要文件加载和字符串输出功能，还可自定义去除不需要的层实现，轻松精简到小于 300K。</p><ul><li>可扩展的模型设计，支持 8bit 量化和半精度浮点存储，可导入 caffe 模型</li></ul><p>ncnn 使用自有的模型格式，模型主要存储模型中各层的权重值。ncnn 模型中含有扩展字段，用于兼容不同权重值的存储方式，如常规的单精度浮点，以及占用更小的半精度浮点和 8bit 量化数。大部分深度模型都可以采用半精度浮点减小一半的模型体积，减少 APP 安装包大小和在线下载模型的耗时。ncnn 带有 caffe 模型转换器，可以转换为 ncnn 的模型格式，方便研究成果快速落地。<br><img src="http://xukeqiniu.xukeai.cn/0b7c0d0fb225db788d66d3e2a0468fca.png" alt=""></p><ul><li>支持直接内存零拷贝引用加载网络模型</li></ul><p>在某些特定应用场景中，如因平台层 API 只能以内存形式访问模型资源，或者希望将模型本身作为静态数据写在代码里，ncnn 提供了直接从内存引用方式加载网络模型的功能。这种加载方式不会拷贝已在内存中的模型，也无需将模型先写入实体的文件再读入，效率极高。</p><ul><li>可注册自定义层实现并扩展</li></ul><p>ncnn 提供了注册自定义层实现的扩展方式，可以将自己实现的特殊层内嵌到 ncnn 的前向计算过程中，组合出更自由的网络结构和更强大的特性。</p><h3 id="入门实践"><a href="#入门实践" class="headerlink" title="入门实践"></a>入门实践</h3><h4 id="Build-for-Linux-x86"><a href="#Build-for-Linux-x86" class="headerlink" title="Build for Linux x86"></a>Build for Linux x86</h4><ul><li><p>实验环境</p></li><li><p>预先配置好Caffe(其实之前预先安装 g++ cmake protobuf即可)</p></li><li><p>安装opencv</p></li><li><p>实践example</p></li></ul><p>首先编译一个工程目录,生成对应的各种工具<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make -j4</span><br></pre></td></tr></table></figure></p><p>然后修改项目根目录下的CMakeLists.txt文件，定位到最后几行<br><img src="http://xukeqiniu.xukeai.cn/22dde3b5cb5f30c9939efc595c59c389.png" alt=""></p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">$ <span class="built_in">cd</span> build</span><br><span class="line">$ cmake ..</span><br><span class="line">$ make</span><br><span class="line">$ cp examples/squeezenet_v1.1.param  build/examples</span><br><span class="line">$ cp examples/squeezenet_v1.1.bin    build/examples</span><br><span class="line">$ <span class="built_in">cd</span> build/examples</span><br><span class="line">$ ./squeezenet cat.jpg</span><br></pre></td></tr></table></figure><p>结果：<br><img src="http://xukeqiniu.xukeai.cn/408d7bac009ae9ba743ed1bff5cb996c.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/Tencent/ncnn/wiki/how-to-build" target="_blank" rel="noopener">https://github.com/Tencent/ncnn/wiki/how-to-build</a><br><a href="http://blog.csdn.net/fuwenyan/article/details/76576708" target="_blank" rel="noopener">http://blog.csdn.net/fuwenyan/article/details/76576708</a><br>新智元·腾讯优图首度开源深度学习框架ncnn 主打手机端，同类cpu框架最快</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;NCNN-简介&quot;&gt;&lt;a href=&quot;#NCNN-简介&quot; class=&quot;headerlink&quot; title=&quot;NCNN 简介&quot;&gt;&lt;/a&gt;NCNN 简介&lt;/h3&gt;&lt;p&gt;ncnn 是一个为手机端极致优化的高性能神经网络前向计算框架。ncnn 从设计之初深刻考虑手机端的部署和使用。无第三方依赖，跨平台，手机端 cpu 的速度快于目前所有已知的开源框架。基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP，将 AI 带到你的指尖。ncnn 目前已在腾讯多款应用中使用，如 QQ，Qzone，微信，天天P图等。&lt;br&gt; ncnn与同类框架对比：&lt;br&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/340953f4416410ed2091d1de5f9946e6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="嵌入式AI" scheme="http://xywang93.github.io.git/categories/Deep-Learning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/"/>
    
      <category term="NCNN" scheme="http://xywang93.github.io.git/categories/Deep-Learning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/NCNN/"/>
    
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
      <category term="嵌入式AI" scheme="http://xywang93.github.io.git/tags/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/"/>
    
      <category term="NCNN" scheme="http://xywang93.github.io.git/tags/NCNN/"/>
    
      <category term="入门" scheme="http://xywang93.github.io.git/tags/%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>CMake 入门</title>
    <link href="http://xywang93.github.io.git/2018/05/02/DeepLearning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/%E7%8E%A9%E8%BD%ACncnn/03-CMake%E5%85%A5%E9%97%A8/"/>
    <id>http://xywang93.github.io.git/2018/05/02/DeepLearning/嵌入式AI/玩转ncnn/03-CMake入门/</id>
    <published>2018-05-01T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:24.424Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CMake-入门实践"><a href="#CMake-入门实践" class="headerlink" title="CMake 入门实践"></a>CMake 入门实践</h2><h3 id="什么是-CMake"><a href="#什么是-CMake" class="headerlink" title="什么是 CMake"></a>什么是 CMake</h3><p>你或许听过好几种 <code>Make</code> 工具，例如 <code>GNU Make</code> ，QT 的 <code>qmake</code> ，微软的 MS <code>nmake</code>，BSD Make（<code>pmake</code>），<code>Makepp</code>，等等。这些 <code>Make</code> 工具遵循着不同的规范和标准，所执行的 <code>Makefile</code> 格式也千差万别。这样就带来了一个严峻的问题：如果软件想跨平台，必须要保证能够在不同平台编译。而如果使用上面的 <code>Make</code> 工具，就得为每一种标准写一次 <code>Makefile</code> ，这将是一件让人抓狂的工作。<br><code>CMake</code>就是针对上面问题所设计的工具：它首先允许开发者编写一种平台无关的 <code>CMakeList.txt</code> 文件来定制整个编译流程，然后再根据目标用户的平台进一步生成所需的本地化 <code>Makefile</code> 和工程文件，如 Unix 的 Makefile 或 Windows 的 Visual Studio 工程。从而做到“<strong>Write once, run everywhere</strong>”。显然，<code>CMake</code> 是一个比上述几种 <code>make</code> 更高级的编译配置工具。一些使用 <code>CMake</code> 作为项目架构系统的知名开源项目有 VTK、ITK、KDE、OpenCV、OSG 等。<br>在 linux 平台下使用 CMake 生成 Makefile 并编译的流程如下：</p><ul><li>编写 <code>CMake</code> 配置文件 <code>CMakeLists.txt</code> 。</li><li>执行命令 <code>cmake PATH</code> 或者 <code>ccmake PATH</code> 生成 <code>Makefile</code>。其中， <code>PATH</code> 是 <code>CMakeLists.txt</code> 所在的目录。</li><li>使用 make 命令进行编译。</li></ul><a id="more"></a><h3 id="入门案例：单个源文件"><a href="#入门案例：单个源文件" class="headerlink" title="入门案例：单个源文件"></a>入门案例：单个源文件</h3><p>对于简单的项目，只需要写几行代码就可以了。例如，假设现在我们的项目中只有一个源文件 <code>main.cc</code> ，该程序的用途是计算一个数的指数幂。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">power</span><span class="params">(<span class="keyword">double</span> base, <span class="keyword">int</span> exponent)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> result = base;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (exponent == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; exponent; ++i)&#123;</span><br><span class="line">        result = result * base;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Usage: %s base exponent \n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> base = atof(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">int</span> exponent = atoi(argv[<span class="number">2</span>]);</span><br><span class="line">    <span class="keyword">double</span> result = power(base, exponent);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%g ^ %d is %g\n"</span>, base, exponent, result);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="编写-CMakeLists-txt"><a href="#编写-CMakeLists-txt" class="headerlink" title="编写 CMakeLists.txt"></a>编写 CMakeLists.txt</h4><p>首先编写 <code>CMakeLists.txt</code> 文件，并保存在与 <code>main.cc</code> 源文件同个目录下：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="comment"># 项目信息</span></span><br><span class="line">project (Demo1)</span><br><span class="line"><span class="comment"># 指定生成目标</span></span><br><span class="line">add_executable(Demo main.cc)</span><br></pre></td></tr></table></figure></p><p><code>CMakeLists.txt</code> 的语法比较简单，由命令、注释和空格组成，其中<strong>命令是不区分大小写的</strong>。符号 # 后面的内容被认为是注释。命令由命令名称、小括号和参数组成，参数之间使用空格进行间隔。<br>对于上面的 <code>CMakeLists.txt</code> 文件，依次出现了几个命令：</p><ul><li><code>cmake_minimum_required</code>：指定运行此配置文件所需的 <code>CMake</code> 的最低版本；</li><li><code>project</code>：参数值是 <code>Demo1</code>，该命令表示项目的名称是 <code>Demo1</code> 。</li><li><code>add_executable</code>： 将名为 <code>main.cc</code> 的源文件编译成一个名称为 <code>Demo</code> 的可执行文件。</li></ul><h4 id="编译项目"><a href="#编译项目" class="headerlink" title="编译项目"></a>编译项目</h4><p>之后，在当前目录执行 <code>cmake .</code> ，得到 <code>Makefile</code> 后再使用 <code>make</code> 命令编译得到 <code>Demo1</code> 可执行文件。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo1$ cmake .</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo1</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo1$ ls</span><br><span class="line">CMakeCache.txt  cmake_install.cmake  main.cc</span><br><span class="line">CMakeFiles      CMakeLists.txt       Makefile</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo1$ make</span><br><span class="line">Scanning dependencies of target Demo</span><br><span class="line">[ 50%] Building CXX object CMakeFiles/Demo.dir/main.cc.o</span><br><span class="line">[100%] Linking CXX executable Demo</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo1$ ls</span><br><span class="line">CMakeCache.txt  cmake_install.cmake  Demo     Makefile</span><br><span class="line">CMakeFiles      CMakeLists.txt       main.cc</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo1$ ./Demo 3 2</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure></p><h3 id="多个源文件"><a href="#多个源文件" class="headerlink" title="多个源文件"></a>多个源文件</h3><h4 id="同一目录，多个源文件"><a href="#同一目录，多个源文件" class="headerlink" title="同一目录，多个源文件"></a>同一目录，多个源文件</h4><p>上面的例子只有单个源文件。现在假如把 <code>power</code> 函数单独写进一个名为 <code>MathFunctions.c</code> 的源文件里，使得这个工程变成如下的形式：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./Demo2</span><br><span class="line">    |</span><br><span class="line">    +--- main.cc</span><br><span class="line">    |</span><br><span class="line">    +--- MathFunctions.cc</span><br><span class="line">    |</span><br><span class="line">    +--- MathFunctions.h</span><br></pre></td></tr></table></figure></p><p>这个时候，CMakeLists.txt 可以改成如下的形式：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="comment"># 项目信息</span></span><br><span class="line">project (Demo2)</span><br><span class="line"><span class="comment"># 指定生成目标</span></span><br><span class="line">add_executable(Demo main.cc MathFunctions.cc)</span><br></pre></td></tr></table></figure></p><p>唯一的改动只是在 <code>add_executable</code> 命令中增加了一个 <code>MathFunctions.cc</code> 源文件。这样写当然没什么问题，但是如果源文件很多，把所有源文件的名字都加进去将是一件烦人的工作。更省事的方法是使用 <code>aux_source_directory</code> 命令，该命令会查找指定目录下的所有源文件，然后将结果存进指定变量名。其语法如下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aux_source_directory(&lt;dir&gt; &lt;variable&gt;)</span><br></pre></td></tr></table></figure></p><p>因此，可以修改 CMakeLists.txt 如下：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="comment"># 项目信息</span></span><br><span class="line">project (Demo2)</span><br><span class="line"><span class="comment"># 查找当前目录下的所有源文件</span></span><br><span class="line"><span class="comment"># 并将名称保存到 DIR_SRCS 变量</span></span><br><span class="line">aux_source_directory(. DIR_SRCS)</span><br><span class="line"><span class="comment"># 指定生成目标</span></span><br><span class="line">add_executable(Demo $&#123;DIR_SRCS&#125;)</span><br></pre></td></tr></table></figure></p><p>这样，CMake 会将当前目录所有源文件的文件名赋值给变量 <code>DIR_SRCS</code> ，再指示变量 <code>DIR_SRCS</code> 中的源文件需要编译成一个名称为 <code>Demo</code> 的可执行文件。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo2$ ls</span><br><span class="line">CMakeLists.txt  main.cc  MathFunctions.cc  MathFunctions.h</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo2$ cmake .</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo2</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo2$ ls</span><br><span class="line">CMakeCache.txt  cmake_install.cmake  main.cc   MathFunctions.cc</span><br><span class="line">CMakeFiles      CMakeLists.txt       Makefile  MathFunctions.h</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo2$ make</span><br><span class="line">Scanning dependencies of target Demo</span><br><span class="line">[ 33%] Building CXX object CMakeFiles/Demo.dir/main.cc.o</span><br><span class="line">[ 66%] Building CXX object CMakeFiles/Demo.dir/MathFunctions.cc.o</span><br><span class="line">[100%] Linking CXX executable Demo</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo2$ ./Demo 3 2</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure></p><h4 id="多个目录，多个源文件"><a href="#多个目录，多个源文件" class="headerlink" title="多个目录，多个源文件"></a>多个目录，多个源文件</h4><p>现在进一步将 <code>MathFunctions.h</code> 和 <code>MathFunctions.cc</code> 文件移动到 <code>math</code> 目录下。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./Demo3</span><br><span class="line">    |</span><br><span class="line">    +--- main.cc</span><br><span class="line">    |</span><br><span class="line">    +--- math/</span><br><span class="line">          |</span><br><span class="line">          +--- MathFunctions.cc</span><br><span class="line">          |</span><br><span class="line">          +--- MathFunctions.h</span><br></pre></td></tr></table></figure></p><p>对于这种情况，需要分别在项目根目录 <code>Demo3</code> 和 <code>math</code> 目录里各编写一个 <code>CMakeLists.txt</code> 文件。为了方便，我们可以先将 <code>math</code> 目录里的文件编译成静态库再由 <code>main</code> 函数调用。</p><h5 id="根目录中的-CMakeLists-txt-："><a href="#根目录中的-CMakeLists-txt-：" class="headerlink" title="根目录中的 CMakeLists.txt ："></a>根目录中的 CMakeLists.txt ：</h5><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="comment"># 项目信息</span></span><br><span class="line">project (Demo3)</span><br><span class="line"><span class="comment"># 查找当前目录下的所有源文件</span></span><br><span class="line"><span class="comment"># 并将名称保存到 DIR_SRCS 变量</span></span><br><span class="line">aux_source_directory(. DIR_SRCS)</span><br><span class="line"><span class="comment"># 添加 math 子目录</span></span><br><span class="line">add_subdirectory(math)</span><br><span class="line"><span class="comment"># 指定生成目标</span></span><br><span class="line">add_executable(Demo main.cc)</span><br><span class="line"><span class="comment"># 添加链接库</span></span><br><span class="line">target_link_libraries(Demo MathFunctions)</span><br></pre></td></tr></table></figure><p>该文件添加了下面的内容: 使用命令 <code>add_subdirectory</code> 指明本项目包含一个子目录 <code>math</code>，这样 <code>math</code> 目录下的 <code>CMakeLists.txt</code> 文件和源代码也会被处理 。使用命令 <code>target_link_libraries</code> 指明可执行文件 <code>main</code> 需要连接一个名为 <code>MathFunctions</code> 的链接库 。</p><h5 id="子目录中的-CMakeLists-txt："><a href="#子目录中的-CMakeLists-txt：" class="headerlink" title="子目录中的 CMakeLists.txt："></a>子目录中的 CMakeLists.txt：</h5><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找当前目录下的所有源文件</span></span><br><span class="line"><span class="comment"># 并将名称保存到 DIR_LIB_SRCS 变量</span></span><br><span class="line">aux_source_directory(. DIR_LIB_SRCS)</span><br><span class="line"><span class="comment"># 生成链接库</span></span><br><span class="line">add_library (MathFunctions $&#123;DIR_LIB_SRCS&#125;)</span><br></pre></td></tr></table></figure><p>在该文件中使用命令 <code>add_library</code> 将 <code>math</code> 目录中的源文件编译为静态链接库。</p><h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ ls</span><br><span class="line">CMakeLists.txt  main.cc  math</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ cmake .</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo3</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ ls</span><br><span class="line">CMakeCache.txt  cmake_install.cmake  main.cc   math</span><br><span class="line">CMakeFiles      CMakeLists.txt       Makefile</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ <span class="built_in">cd</span> math/</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3/math$ ls</span><br><span class="line">CMakeFiles           CMakeLists.txt  MathFunctions.cc</span><br><span class="line">cmake_install.cmake  Makefile        MathFunctions.h</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3/math$ <span class="built_in">cd</span> ..</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ ls</span><br><span class="line">CMakeCache.txt  cmake_install.cmake  main.cc   math</span><br><span class="line">CMakeFiles      CMakeLists.txt       Makefile</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ make</span><br><span class="line">Scanning dependencies of target MathFunctions</span><br><span class="line">[ 25%] Building CXX object math/CMakeFiles/MathFunctions.dir/MathFunctions.cc.o</span><br><span class="line">[ 50%] Linking CXX static library libMathFunctions.a</span><br><span class="line">[ 50%] Built target MathFunctions</span><br><span class="line">Scanning dependencies of target Demo</span><br><span class="line">[ 75%] Building CXX object CMakeFiles/Demo.dir/main.cc.o</span><br><span class="line">[100%] Linking CXX executable Demo</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo3$ ./Demo 3 2</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure><h3 id="自定义编译选项"><a href="#自定义编译选项" class="headerlink" title="自定义编译选项"></a>自定义编译选项</h3><p>CMake 允许为项目增加编译选项，从而可以根据用户的环境和需求选择最合适的编译方案。</p><p>例如，可以将 <code>MathFunctions</code> 库设为一个可选的库，如果该选项为 <code>ON</code> ，就使用该库定义的数学函数来进行运算。否则就调用标准库中的数学函数库。</p><h4 id="修改-CMakeLists-文件"><a href="#修改-CMakeLists-文件" class="headerlink" title="修改 CMakeLists 文件"></a>修改 CMakeLists 文件</h4><p>我们要做的第一步是在根目录的 <code>CMakeLists.txt</code> 文件中添加该选项：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake 最低版本号要求</span></span><br><span class="line">cmake_minimum_required (VERSION 2.8)</span><br><span class="line"><span class="comment"># 项目信息</span></span><br><span class="line">project (Demo4)</span><br><span class="line"><span class="comment"># 是否使用自己的 MathFunctions 库</span></span><br><span class="line">option (USE_MYMATH</span><br><span class="line">       <span class="string">"Use provided math implementation"</span> ON)</span><br><span class="line"><span class="comment"># 加入一个配置头文件，用于处理 CMake 对源码的设置</span></span><br><span class="line">configure_file (</span><br><span class="line">  <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/config.h.in"</span></span><br><span class="line">  <span class="string">"$&#123;PROJECT_BINARY_DIR&#125;/config.h"</span></span><br><span class="line">  )</span><br><span class="line"><span class="comment"># 是否加入 MathFunctions 库</span></span><br><span class="line">if (USE_MYMATH)</span><br><span class="line">  include_directories (<span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/math"</span>)</span><br><span class="line">  add_subdirectory (math)</span><br><span class="line">  set (EXTRA_LIBS $&#123;EXTRA_LIBS&#125; MathFunctions)</span><br><span class="line"><span class="keyword">endif</span> (USE_MYMATH)</span><br><span class="line"><span class="comment"># 查找当前目录下的所有源文件</span></span><br><span class="line"><span class="comment"># 并将名称保存到 DIR_SRCS 变量</span></span><br><span class="line">aux_source_directory(. DIR_SRCS)</span><br><span class="line"><span class="comment"># 指定生成目标</span></span><br><span class="line">add_executable(Demo $&#123;DIR_SRCS&#125;)</span><br><span class="line">target_link_libraries (Demo  $&#123;EXTRA_LIBS&#125;)</span><br></pre></td></tr></table></figure></p><p>其中：</p><ul><li>第6行的 <code>option</code> 命令添加了一个 <code>USE_MYMATH</code> 选项，并且默认值为 <code>ON</code> 。</li><li>第9行的 <code>configure_file</code> 命令用于加入一个配置头文件 <code>config.h</code> ，这个文件由 <code>CMake</code> 从 <code>config.h.in</code> 生成，通过这样的机制，将可以通过预定义一些参数和变量来控制代码的生成。</li><li>第14行根据 <code>USE_MYMATH</code> 变量的值来决定是否使用我们自己编写的 <code>MathFunctions</code> 库。</li></ul><h4 id="修改-main-cc-文件"><a href="#修改-main-cc-文件" class="headerlink" title="修改 main.cc 文件"></a>修改 main.cc 文件</h4><p>之后修改 main.cc 文件，让其根据 <code>USE_MYMATH</code> 的预定义值来决定是否调用标准库还是 <code>MathFunctions</code> 库：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;config.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MYMATH</span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;MathFunctions.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Usage: %s base exponent \n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> base = atof(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">int</span> exponent = atoi(argv[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_MYMATH</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use our own Math library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = power(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use the standard library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = <span class="built_in">pow</span>(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%g ^ %d is %g\n"</span>, base, exponent, result);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h4 id="编写-config-h-in-文件"><a href="#编写-config-h-in-文件" class="headerlink" title="编写 config.h.in 文件"></a>编写 config.h.in 文件</h4><p>上面的程序值得注意的是第2行，这里引用了一个 <code>config.h</code> 文件，这个文件预定义了 <code>USE_MYMATH</code> 的值。但我们并不直接编写这个文件，为了方便从 <code>CMakeLists.txt</code> 中导入配置，我们编写一个 <code>config.h.in</code> 文件，内容如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#cmakedefine USE_MYMATH</span></span><br></pre></td></tr></table></figure></p><p>这样 CMake 会自动根据 CMakeLists 配置文件中的设置自动生成 config.h 文件。</p><h4 id="编译项目-1"><a href="#编译项目-1" class="headerlink" title="编译项目"></a>编译项目</h4><p>现在编译一下这个项目，为了便于交互式的选择该变量的值，可以使用 <code>ccmake .</code> 命令<br><img src="http://xukeqiniu.xukeai.cn/3e5f44d3eaa3c1480e7937f8e2bef4ba.png" alt=""><br>从中可以找到刚刚定义的 <code>USE_MYMATH</code> 选项，按键盘的方向键可以在不同的选项窗口间跳转，按下 <code>enter</code> 键可以修改该选项。修改完成后可以按下 <code>c</code> 选项完成配置，之后再按 <code>g</code> 键确认生成 <code>Makefile</code> 。<code>ccmake</code> 的其他操作可以参考窗口下方给出的指令提示。<br>我们可以试试分别将 USE_MYMATH 设为 ON 和 OFF 得到的结果：</p><h5 id="USE-MYMATH-为-ON"><a href="#USE-MYMATH-为-ON" class="headerlink" title="USE_MYMATH 为 ON"></a>USE_MYMATH 为 ON</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ cmake .</span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo4</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ make</span><br><span class="line">[ 50%] Built target MathFunctions</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ ./Demo 3 2</span><br><span class="line">Now we use our own Math library.</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure><h5 id="USE-MYMATH-为-OFF"><a href="#USE-MYMATH-为-OFF" class="headerlink" title="USE_MYMATH 为 OFF"></a>USE_MYMATH 为 OFF</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ cmake .</span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo4</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ make</span><br><span class="line">Scanning dependencies of target Demo</span><br><span class="line">[ 50%] Building CXX object CMakeFiles/Demo.dir/main.cc.o</span><br><span class="line">[100%] Linking CXX executable Demo</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo4$ ./Demo 3 2</span><br><span class="line">Now we use the standard library.</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure><h3 id="安装和测试"><a href="#安装和测试" class="headerlink" title="安装和测试"></a>安装和测试</h3><p><code>CMake</code> 也可以指定安装规则，以及添加测试。这两个功能分别可以通过在产生 <code>Makefile</code> 后使用 <code>make install</code> 和 <code>make test</code> 来执行。在以前的 <code>GNU Makefile</code> 里，你可能需要为此编写 <code>install</code> 和 <code>test</code> 两个伪目标和相应的规则，但在 <code>CMake</code> 里，这样的工作同样只需要简单的调用几条命令。</p><h4 id="定制安装规则"><a href="#定制安装规则" class="headerlink" title="定制安装规则"></a>定制安装规则</h4><p>首先先在 math/CMakeLists.txt 文件里添加下面两行：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定 MathFunctions 库的安装路径</span></span><br><span class="line">install (TARGETS MathFunctions DESTINATION bin)</span><br><span class="line">install (FILES MathFunctions.h DESTINATION <span class="keyword">include</span>)</span><br></pre></td></tr></table></figure></p><p>指明 MathFunctions 库的安装路径。<br>之后同样修改根目录的 CMakeLists 文件，在末尾添加下面几行：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定安装路径</span></span><br><span class="line">install (TARGETS Demo DESTINATION bin)</span><br><span class="line">install (FILES <span class="string">"$&#123;PROJECT_BINARY_DIR&#125;/config.h"</span></span><br><span class="line">         DESTINATION <span class="keyword">include</span>)</span><br></pre></td></tr></table></figure></p><p>通过上面的定制，生成的 Demo 文件和 MathFunctions 函数库 libMathFunctions.o 文件将会被复制到 /usr/local/bin 中，而 MathFunctions.h 和生成的 config.h 文件则会被复制到 /usr/local/include 中。</p><blockquote><p>顺带一提的是，这里的 /usr/local/ 是默认安装到的根目录，可以通过修改 CMAKE_INSTALL_PREFIX 变量的值来指定这些文件应该拷贝到哪个根目录。</p></blockquote><p>我们可以验证一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu: sudo make install</span><br><span class="line">[ 50%] Built target MathFunctions</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">Install the project...</span><br><span class="line">-- Install configuration: <span class="string">""</span></span><br><span class="line">-- Installing: /usr/<span class="built_in">local</span>/bin/Demo</span><br><span class="line">-- Installing: /usr/<span class="built_in">local</span>/include/config.h</span><br><span class="line">-- Installing: /usr/<span class="built_in">local</span>/bin/libMathFunctions.a</span><br><span class="line">-- Up-to-date: /usr/<span class="built_in">local</span>/include/MathFunctions.h</span><br><span class="line">xuke@ubuntu: ls /usr/<span class="built_in">local</span>/bin</span><br><span class="line">Demo  libMathFunctions.a</span><br><span class="line">xuke@ubuntu: ls /usr/<span class="built_in">local</span>/include</span><br><span class="line">config.h  MathFunctions.h</span><br></pre></td></tr></table></figure></p><h4 id="为工程添加测试"><a href="#为工程添加测试" class="headerlink" title="为工程添加测试"></a>为工程添加测试</h4><p>添加测试同样很简单。<code>CMake</code> 提供了一个称为 <code>CTest</code> 的测试工具。我们要做的只是在项目根目录的 <code>CMakeLists</code> 文件中调用一系列的 <code>add_test</code> 命令。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启用测试</span></span><br><span class="line">enable_testing()</span><br><span class="line"><span class="comment"># 测试程序是否成功运行</span></span><br><span class="line">add_test (test_run Demo 5 2)</span><br><span class="line"><span class="comment"># 测试帮助信息是否可以正常提示</span></span><br><span class="line">add_test (test_usage Demo)</span><br><span class="line">set_tests_properties (test_usage</span><br><span class="line">  PROPERTIES PASS_REGULAR_EXPRESSION <span class="string">"Usage: .* base exponent"</span>)</span><br><span class="line"><span class="comment"># 测试 5 的平方</span></span><br><span class="line">add_test (test_5_2 Demo 5 2)</span><br><span class="line">set_tests_properties (test_5_2</span><br><span class="line"> PROPERTIES PASS_REGULAR_EXPRESSION <span class="string">"is 25"</span>)</span><br><span class="line"><span class="comment"># 测试 10 的 5 次方</span></span><br><span class="line">add_test (test_10_5 Demo 10 5)</span><br><span class="line">set_tests_properties (test_10_5</span><br><span class="line"> PROPERTIES PASS_REGULAR_EXPRESSION <span class="string">"is 100000"</span>)</span><br><span class="line"><span class="comment"># 测试 2 的 10 次方</span></span><br><span class="line">add_test (test_2_10 Demo 2 10)</span><br><span class="line">set_tests_properties (test_2_10</span><br><span class="line"> PROPERTIES PASS_REGULAR_EXPRESSION <span class="string">"is 1024"</span>)</span><br></pre></td></tr></table></figure></p><p>上面的代码包含了四个测试。第一个测试 <code>test_run</code> 用来测试程序是否成功运行并返回 0 值。剩下的三个测试分别用来测试 5 的 平方、10 的 5 次方、2 的 10 次方是否都能得到正确的结果。其中 <code>PASS_REGULAR_EXPRESSION</code> 用来测试输出是否包含后面跟着的字符串。</p><p>让我们看看测试的结果：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo5$ make <span class="built_in">test</span></span><br><span class="line">Running tests...</span><br><span class="line">Test project /home/xuke/work/cmake-demo/Demo5</span><br><span class="line">    Start 1: test_run</span><br><span class="line">1/5 Test <span class="comment">#1: test_run .........................   Passed    0.00 sec</span></span><br><span class="line">    Start 2: test_usage</span><br><span class="line">2/5 Test <span class="comment">#2: test_usage .......................   Passed    0.00 sec</span></span><br><span class="line">    Start 3: test_5_2</span><br><span class="line">3/5 Test <span class="comment">#3: test_5_2 .........................   Passed    0.00 sec</span></span><br><span class="line">    Start 4: test_10_5</span><br><span class="line">4/5 Test <span class="comment">#4: test_10_5 ........................   Passed    0.00 sec</span></span><br><span class="line">    Start 5: test_2_10</span><br><span class="line">5/5 Test <span class="comment">#5: test_2_10 ........................   Passed    0.00 sec</span></span><br><span class="line"></span><br><span class="line">100% tests passed, 0 tests failed out of 5</span><br><span class="line"></span><br><span class="line">Total Test time (real) =   0.01 sec</span><br></pre></td></tr></table></figure></p><p>如果要测试更多的输入数据，像上面那样一个个写测试用例未免太繁琐。这时可以通过编写<strong>宏</strong>来实现：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个宏，用来简化测试工作</span></span><br><span class="line">macro (do_test arg1 arg2 result)</span><br><span class="line">  add_test (test_$&#123;arg1&#125;_$&#123;arg2&#125; Demo $&#123;arg1&#125; $&#123;arg2&#125;)</span><br><span class="line">  set_tests_properties (test_$&#123;arg1&#125;_$&#123;arg2&#125;</span><br><span class="line">    PROPERTIES PASS_REGULAR_EXPRESSION $&#123;result&#125;)</span><br><span class="line">endmacro (do_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用该宏进行一系列的数据测试</span></span><br><span class="line">do_test (5 2 <span class="string">"is 25"</span>)</span><br><span class="line">do_test (10 5 <span class="string">"is 100000"</span>)</span><br><span class="line">do_test (2 10 <span class="string">"is 1024"</span>)</span><br></pre></td></tr></table></figure></p><h3 id="支持-gdb"><a href="#支持-gdb" class="headerlink" title="支持 gdb"></a>支持 gdb</h3><p>让 CMake 支持 <code>gdb</code> 的设置也很容易，只需要指定 <code>Debug</code> 模式下开启 <code>-g</code> 选项：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set(CMAKE_BUILD_TYPE <span class="string">"Debug"</span>)</span><br><span class="line">set(CMAKE_CXX_FLAGS_DEBUG <span class="string">"$ENV&#123;CXXFLAGS&#125; -O0 -Wall -g -ggdb"</span>)</span><br><span class="line">set(CMAKE_CXX_FLAGS_RELEASE <span class="string">"$ENV&#123;CXXFLAGS&#125; -O3 -Wall"</span>)</span><br></pre></td></tr></table></figure></p><p>之后可以直接对生成的程序使用 <code>gdb</code> 来调试。</p><h3 id="添加环境检查"><a href="#添加环境检查" class="headerlink" title="添加环境检查"></a>添加环境检查</h3><p>有时候可能要对系统环境做点检查，例如要使用一个平台相关的特性的时候。在这个例子中，我们检查系统是否自带 <code>pow</code> 函数。如果带有 <code>pow</code> 函数，就使用它；否则使用我们定义的 <code>power</code> 函数。</p><h4 id="添加-CheckFunctionExists-宏"><a href="#添加-CheckFunctionExists-宏" class="headerlink" title="添加 CheckFunctionExists 宏"></a>添加 CheckFunctionExists 宏</h4><p>首先在顶层 <code>CMakeLists</code> 文件中添加 <code>CheckFunctionExists.cmake</code> 宏，并调用 <code>check_function_exists</code> 命令测试链接器是否能够在链接阶段找到 <code>pow</code> 函数。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查系统是否支持 pow 函数</span></span><br><span class="line"><span class="keyword">include</span> ($&#123;CMAKE_ROOT&#125;/Modules/CheckFunctionExists.cmake)</span><br><span class="line">check_function_exists (pow HAVE_POW)</span><br></pre></td></tr></table></figure></p><p>将上面这段代码放在 <code>configure_file</code> 命令前。</p><h4 id="预定义相关宏变量"><a href="#预定义相关宏变量" class="headerlink" title="预定义相关宏变量"></a>预定义相关宏变量</h4><p>接下来修改 <code>config.h.in</code> 文件，预定义相关的宏变量。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// does the platform provide pow function?</span><br><span class="line"><span class="comment">#cmakedefine HAVE_POW</span></span><br></pre></td></tr></table></figure></p><h4 id="在代码中使用宏和函数"><a href="#在代码中使用宏和函数" class="headerlink" title="在代码中使用宏和函数"></a>在代码中使用宏和函数</h4><p>最后一步是修改 main.cc ，在代码中使用宏和函数：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> HAVE_POW</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use the standard library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = <span class="built_in">pow</span>(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use our own Math library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = power(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure></p><h3 id="添加版本号"><a href="#添加版本号" class="headerlink" title="添加版本号"></a>添加版本号</h3><p>给项目添加和维护版本号是一个好习惯，这样有利于用户了解每个版本的维护情况，并及时了解当前所用的版本是否过时，或是否可能出现不兼容的情况。</p><p>首先修改顶层 <code>CMakeLists</code> 文件，在 <code>project</code> 命令之后加入如下两行：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set (Demo_VERSION_MAJOR 1)</span><br><span class="line">set (Demo_VERSION_MINOR 0)</span><br></pre></td></tr></table></figure></p><p>分别指定当前的项目的主版本号和副版本号。<br>之后，为了在代码中获取版本信息，我们可以修改 config.h.in 文件，添加两个预定义变量：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// the configured options and settings for Tutorial</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Demo_VERSION_MAJOR @Demo_VERSION_MAJOR@</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> Demo_VERSION_MINOR @Demo_VERSION_MINOR@</span></span><br></pre></td></tr></table></figure></p><p>这样就可以直接在代码中打印版本信息了：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"config.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"math/MathFunctions.h"</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>)&#123;</span><br><span class="line">        <span class="comment">// print version info</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%s Version %d.%d\n"</span>,</span><br><span class="line">            argv[<span class="number">0</span>],</span><br><span class="line">            Demo_VERSION_MAJOR,</span><br><span class="line">            Demo_VERSION_MINOR);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Usage: %s base exponent \n"</span>, argv[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> base = atof(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">int</span> exponent = atoi(argv[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">if</span> defined (HAVE_POW)</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use the standard library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = <span class="built_in">pow</span>(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Now we use our own Math library. \n"</span>);</span><br><span class="line">    <span class="keyword">double</span> result = power(base, exponent);</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%g ^ %d is %g\n"</span>, base, exponent, result);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>测试的结果：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo7$ ls</span><br><span class="line">CMakeLists.txt  config.h.in  main.cc  math</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo7$ cmake .</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Looking <span class="keyword">for</span> pow</span><br><span class="line">-- Looking <span class="keyword">for</span> pow - not found</span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo7</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo7$ make</span><br><span class="line">Scanning dependencies of target MathFunctions</span><br><span class="line">[ 25%] Building CXX object math/CMakeFiles/MathFunctions.dir/MathFunctions.cc.o</span><br><span class="line">[ 50%] Linking CXX static library libMathFunctions.a</span><br><span class="line">[ 50%] Built target MathFunctions</span><br><span class="line">Scanning dependencies of target Demo</span><br><span class="line">[ 75%] Building CXX object CMakeFiles/Demo.dir/main.cc.o</span><br><span class="line">[100%] Linking CXX executable Demo</span><br><span class="line">[100%] Built target Demo</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo7$ ./Demo 3 2</span><br><span class="line">Now we use our own Math library.</span><br><span class="line">3 ^ 2 is 9</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo7$ ./Demo 3</span><br><span class="line">./Demo Version 1.0</span><br><span class="line">Usage: ./Demo base exponent</span><br></pre></td></tr></table></figure></p><h3 id="生成安装包"><a href="#生成安装包" class="headerlink" title="生成安装包"></a>生成安装包</h3><p>本节将学习如何配置生成各种平台上的安装包，包括二进制安装包和源码安装包。为了完成这个任务，我们需要用到 <code>CPack</code> ，它同样也是由 <code>CMake</code> 提供的一个工具，专门用于打包。<br>首先在顶层的 <code>CMakeLists.txt</code> 文件尾部添加下面几行：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个 CPack 安装包</span></span><br><span class="line"><span class="keyword">include</span> (InstallRequiredSystemLibraries)</span><br><span class="line">set (CPACK_RESOURCE_FILE_LICENSE</span><br><span class="line">  <span class="string">"$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/License.txt"</span>)</span><br><span class="line">set (CPACK_PACKAGE_VERSION_MAJOR <span class="string">"$&#123;Demo_VERSION_MAJOR&#125;"</span>)</span><br><span class="line">set (CPACK_PACKAGE_VERSION_MINOR <span class="string">"$&#123;Demo_VERSION_MINOR&#125;"</span>)</span><br><span class="line"><span class="keyword">include</span> (CPack)</span><br></pre></td></tr></table></figure></p><p>上面的代码做了以下几个工作：</p><ul><li>导入 <code>InstallRequiredSystemLibraries</code> 模块，以便之后导入 <code>CPack</code> 模块；</li><li>设置一些 <code>CPack</code> 相关变量，包括版权信息和版本信息，其中版本信息用了上一节定义的版本号；</li><li>导入 <code>CPack</code> 模块。</li></ul><p>接下来的工作是像往常一样构建工程，并执行 <code>cpack</code> 命令。</p><ul><li><p>生成二进制安装包：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpack -C CPackConfig.cmake</span><br></pre></td></tr></table></figure></li><li><p>生成源码安装包</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpack -C CPackSourceConfig.cmake</span><br></pre></td></tr></table></figure></li></ul><p>我们可以试一下。在生成项目后，执行 cpack -C CPackConfig.cmake 命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ ls</span><br><span class="line">CMakeLists.txt  config.h.in  License.txt  main.cc  math</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ cmake .</span><br><span class="line">-- The C compiler identification is GNU 5.4.0</span><br><span class="line">-- The CXX compiler identification is GNU 5.4.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: /usr/bin/cc -- works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting C compile features</span><br><span class="line">-- Detecting C compile features - <span class="keyword">done</span></span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++</span><br><span class="line">-- Check <span class="keyword">for</span> working CXX compiler: /usr/bin/c++ -- works</span><br><span class="line">-- Detecting CXX compiler ABI info</span><br><span class="line">-- Detecting CXX compiler ABI info - <span class="keyword">done</span></span><br><span class="line">-- Detecting CXX compile features</span><br><span class="line">-- Detecting CXX compile features - <span class="keyword">done</span></span><br><span class="line">-- Looking <span class="keyword">for</span> pow</span><br><span class="line">-- Looking <span class="keyword">for</span> pow - not found</span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: /home/xuke/work/cmake-demo/Demo8</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ cpack -C CPackSourceConfig.cmake</span><br><span class="line">CPack: Create package using STGZ</span><br><span class="line">CPack: Install projects</span><br><span class="line">CPack: - Run preinstall target <span class="keyword">for</span>: Demo8</span><br><span class="line">CPack: - Install project: Demo8</span><br><span class="line">CPack: Create package</span><br><span class="line">CPack: - package: /home/xuke/work/cmake-demo/Demo8/Demo8-1.0.1-Linux.sh generated.</span><br><span class="line">CPack: Create package using TGZ</span><br><span class="line">CPack: Install projects</span><br><span class="line">CPack: - Run preinstall target <span class="keyword">for</span>: Demo8</span><br><span class="line">CPack: - Install project: Demo8</span><br><span class="line">CPack: Create package</span><br><span class="line">CPack: - package: /home/xuke/work/cmake-demo/Demo8/Demo8-1.0.1-Linux.tar.gz generated.</span><br><span class="line">CPack: Create package using TZ</span><br><span class="line">CPack: Install projects</span><br><span class="line">CPack: - Run preinstall target <span class="keyword">for</span>: Demo8</span><br><span class="line">CPack: - Install project: Demo8</span><br><span class="line">CPack: Create package</span><br><span class="line">CPack: - package: /home/xuke/work/cmake-demo/Demo8/Demo8-1.0.1-Linux.tar.Z generated.</span><br></pre></td></tr></table></figure></p><p>此时会在该目录下创建 3 个不同格式的二进制包文件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ ls</span><br><span class="line">CMakeCache.txt       _CPack_Packages           install_manifest.txt</span><br><span class="line">CMakeFiles           CPackSourceConfig.cmake   License.txt</span><br><span class="line">cmake_install.cmake  CTestTestfile.cmake       main.cc</span><br><span class="line">CMakeLists.txt       Demo                      Makefile</span><br><span class="line">config.h             Demo8-1.0.1-Linux.sh      math</span><br><span class="line">config.h.in          Demo8-1.0.1-Linux.tar.gz</span><br><span class="line">CPackConfig.cmake    Demo8-1.0.1-Linux.tar.Z</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ ls Demo8-*</span><br><span class="line">Demo8-1.0.1-Linux.sh  Demo8-1.0.1-Linux.tar.gz  Demo8-1.0.1-Linux.tar.Z</span><br></pre></td></tr></table></figure></p><p>这 3 个二进制包文件所包含的内容是完全相同的。我们可以执行其中一个。此时会出现一个由 CPack 自动生成的交互式安装界面：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8$ sh Demo8-1.0.1-Linux.sh</span><br><span class="line">Demo8 Installer Version: 1.0.1, Copyright (c) Humanity</span><br><span class="line">This is a self-extracting archive.</span><br><span class="line">The archive will be extracted to: /home/xuke/work/cmake-demo/Demo8</span><br><span class="line"></span><br><span class="line">If you want to stop extracting, please press &lt;ctrl-C&gt;.</span><br><span class="line">The MIT License (MIT)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2013 Joseph Pan(http://hahack.com)</span><br><span class="line"></span><br><span class="line">Permission is hereby granted, free of charge, to any person obtaining a copy of</span><br><span class="line">this software and associated documentation files (the <span class="string">"Software"</span>), to deal <span class="keyword">in</span></span><br><span class="line">the Software without restriction, including without limitation the rights to</span><br><span class="line">use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of</span><br><span class="line">the Software, and to permit persons to whom the Software is furnished to <span class="keyword">do</span> so,</span><br><span class="line">subject to the following conditions:</span><br><span class="line"></span><br><span class="line">The above copyright notice and this permission notice shall be included <span class="keyword">in</span> all</span><br><span class="line">copies or substantial portions of the Software.</span><br><span class="line"></span><br><span class="line">THE SOFTWARE IS PROVIDED <span class="string">"AS IS"</span>, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span><br><span class="line">IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS</span><br><span class="line">FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR</span><br><span class="line">COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER</span><br><span class="line">IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN</span><br><span class="line">CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Do you accept the license? [yN]:</span><br><span class="line">y</span><br><span class="line">By default the Demo8 will be installed <span class="keyword">in</span>:</span><br><span class="line">  <span class="string">"/home/xuke/work/cmake-demo/Demo8/Demo8-1.0.1-Linux"</span></span><br><span class="line">Do you want to include the subdirectory Demo8-1.0.1-Linux?</span><br><span class="line">Saying no will install <span class="keyword">in</span>: <span class="string">"/home/xuke/work/cmake-demo/Demo8"</span> [Yn]:</span><br><span class="line">y</span><br><span class="line"></span><br><span class="line">Using target directory: /home/xuke/work/cmake-demo/Demo8/Demo8-1.0.1-Linux</span><br><span class="line">Extracting, please <span class="built_in">wait</span>...</span><br><span class="line"></span><br><span class="line">Unpacking finished successfully</span><br></pre></td></tr></table></figure></p><p>完成后提示安装到了 Demo8-1.0.1-Linux 子目录中，我们可以进去执行该程序：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8/Demo8-1.0.1-Linux$ tree</span><br><span class="line">.</span><br><span class="line">├── bin</span><br><span class="line">│   └── Demo</span><br><span class="line">├── include</span><br><span class="line">│   ├── config.h</span><br><span class="line">│   └── MathFunctions.h</span><br><span class="line">└── lib</span><br><span class="line">    └── libMathFunctions.a</span><br><span class="line"></span><br><span class="line">3 directories, 4 files</span><br><span class="line">xuke@ubuntu:~/work/cmake-demo/Demo8/Demo8-1.0.1-Linux$ ./bin/Demo 3 2</span><br><span class="line">Now we use our own Math library.</span><br><span class="line">3 ^ 2 is 9</span><br></pre></td></tr></table></figure></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://www.hahack.com/codes/cmake/" target="_blank" rel="noopener">CMake 入门实战</a><br><a href="https://github.com/wzpan/cmake-demo" target="_blank" rel="noopener">代码参考</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;CMake-入门实践&quot;&gt;&lt;a href=&quot;#CMake-入门实践&quot; class=&quot;headerlink&quot; title=&quot;CMake 入门实践&quot;&gt;&lt;/a&gt;CMake 入门实践&lt;/h2&gt;&lt;h3 id=&quot;什么是-CMake&quot;&gt;&lt;a href=&quot;#什么是-CMake&quot; class=&quot;headerlink&quot; title=&quot;什么是 CMake&quot;&gt;&lt;/a&gt;什么是 CMake&lt;/h3&gt;&lt;p&gt;你或许听过好几种 &lt;code&gt;Make&lt;/code&gt; 工具，例如 &lt;code&gt;GNU Make&lt;/code&gt; ，QT 的 &lt;code&gt;qmake&lt;/code&gt; ，微软的 MS &lt;code&gt;nmake&lt;/code&gt;，BSD Make（&lt;code&gt;pmake&lt;/code&gt;），&lt;code&gt;Makepp&lt;/code&gt;，等等。这些 &lt;code&gt;Make&lt;/code&gt; 工具遵循着不同的规范和标准，所执行的 &lt;code&gt;Makefile&lt;/code&gt; 格式也千差万别。这样就带来了一个严峻的问题：如果软件想跨平台，必须要保证能够在不同平台编译。而如果使用上面的 &lt;code&gt;Make&lt;/code&gt; 工具，就得为每一种标准写一次 &lt;code&gt;Makefile&lt;/code&gt; ，这将是一件让人抓狂的工作。&lt;br&gt;&lt;code&gt;CMake&lt;/code&gt;就是针对上面问题所设计的工具：它首先允许开发者编写一种平台无关的 &lt;code&gt;CMakeList.txt&lt;/code&gt; 文件来定制整个编译流程，然后再根据目标用户的平台进一步生成所需的本地化 &lt;code&gt;Makefile&lt;/code&gt; 和工程文件，如 Unix 的 Makefile 或 Windows 的 Visual Studio 工程。从而做到“&lt;strong&gt;Write once, run everywhere&lt;/strong&gt;”。显然，&lt;code&gt;CMake&lt;/code&gt; 是一个比上述几种 &lt;code&gt;make&lt;/code&gt; 更高级的编译配置工具。一些使用 &lt;code&gt;CMake&lt;/code&gt; 作为项目架构系统的知名开源项目有 VTK、ITK、KDE、OpenCV、OSG 等。&lt;br&gt;在 linux 平台下使用 CMake 生成 Makefile 并编译的流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;编写 &lt;code&gt;CMake&lt;/code&gt; 配置文件 &lt;code&gt;CMakeLists.txt&lt;/code&gt; 。&lt;/li&gt;
&lt;li&gt;执行命令 &lt;code&gt;cmake PATH&lt;/code&gt; 或者 &lt;code&gt;ccmake PATH&lt;/code&gt; 生成 &lt;code&gt;Makefile&lt;/code&gt;。其中， &lt;code&gt;PATH&lt;/code&gt; 是 &lt;code&gt;CMakeLists.txt&lt;/code&gt; 所在的目录。&lt;/li&gt;
&lt;li&gt;使用 make 命令进行编译。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="嵌入式AI" scheme="http://xywang93.github.io.git/categories/Deep-Learning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/"/>
    
      <category term="NCNN" scheme="http://xywang93.github.io.git/categories/Deep-Learning/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/NCNN/"/>
    
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
      <category term="嵌入式AI" scheme="http://xywang93.github.io.git/tags/%E5%B5%8C%E5%85%A5%E5%BC%8FAI/"/>
    
      <category term="NCNN" scheme="http://xywang93.github.io.git/tags/NCNN/"/>
    
      <category term="入门" scheme="http://xywang93.github.io.git/tags/%E5%85%A5%E9%97%A8/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 60分钟入门</title>
    <link href="http://xywang93.github.io.git/2018/05/01/DeepLearning/PyTorch/PyTorch+60%E5%88%86%E9%92%9F%E5%85%A5%E9%97%A8/"/>
    <id>http://xywang93.github.io.git/2018/05/01/DeepLearning/PyTorch/PyTorch+60分钟入门/</id>
    <published>2018-04-30T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:24.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PyTorch-60分钟入门"><a href="#PyTorch-60分钟入门" class="headerlink" title="PyTorch 60分钟入门"></a>PyTorch 60分钟入门</h2><h3 id="PyTorch简介"><a href="#PyTorch简介" class="headerlink" title="PyTorch简介"></a>PyTorch简介</h3><p>这是一个基于Python的科学计算包，主要针对两类人群：</p><ul><li>替代Numpy以发挥GPU的强大能力</li><li>一个提供最大灵活性和速度的深度学习研究平台</li></ul><a id="more"></a><h4 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h4><h5 id="张量（Tensors）"><a href="#张量（Tensors）" class="headerlink" title="张量（Tensors）"></a>张量（Tensors）</h5><p>Tensors类似于numpy的ndarray，但是带了一些附加的功能，例如可以使用GPU加速计算等等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><p>构建一个未初始化的5*3的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-1.0593e-05,  4.5849e-41,  3.4723e-37],        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],        [ 0.0000e+00,  0.0000e+00,  7.1941e+28],        [ 3.1036e+27,  0.0000e+00,  0.0000e+00],        [ 0.0000e+00,  0.0000e+00,  3.4568e-37]])</code></pre><p>构建一个随机初始化的5*3的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[ 0.7556,  0.7558,  0.2999],        [ 0.7304,  0.3527,  0.1911],        [ 0.8654,  0.4880,  0.1987],        [ 0.5456,  0.9359,  0.2071],        [ 0.1025,  0.5249,  0.3758]])</code></pre><p>构建一个初始化为零类型为long的5*3的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0],        [ 0,  0,  0]])</code></pre><p>从数据构造一个张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([ 5.5000,  3.0000])</code></pre><p>根据现有张量创建张量。这些方法将重用输入张量的属性，例如dtype，除非用户提供了新的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(x) <span class="comment">#打印之前的x值</span></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)      <span class="comment"># new_* 方法可以更改x的值，维度和类型</span></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)    <span class="comment"># 类型与值进行覆盖</span></span><br><span class="line">print(x)                                      <span class="comment"># 不改变维度</span></span><br></pre></td></tr></table></figure><hr><pre><code>tensor([ 5.5000,  3.0000])tensor([[ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.]], dtype=torch.float64)tensor([[-1.4230, -0.7907, -0.0556],        [-0.9403, -0.2937,  1.9447],        [ 0.2958,  0.9914, -0.9550],        [ 1.2439, -0.1390,  0.2889],        [-0.1790, -0.0003,  0.5241]])</code></pre><p>获取尺寸</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure><hr><pre><code>torch.Size([5, 3])</code></pre><blockquote><p>torch.Size 实际上是一个元组（tuple），因此它支持所有的元祖（tuple）的操作。</p></blockquote><h5 id="操作（Operations）"><a href="#操作（Operations）" class="headerlink" title="操作（Operations）"></a>操作（Operations）</h5><p>Pytorch具有100多种操作符（加减乘除，转置，索引，切片，等等），在这里我们以最简单的加法操作，了解Pytorch的操作方法。</p><ul><li>加法：语法1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-1.1514, -0.5880, -0.0083],        [-0.4967,  0.2964,  2.5860],        [ 0.7163,  1.0643,  0.0210],        [ 1.8021,  0.6697,  0.8263],        [ 0.3601,  0.3765,  1.3859]])</code></pre><ul><li>加法：语法2</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-1.1514, -0.5880, -0.0083],        [-0.4967,  0.2964,  2.5860],        [ 0.7163,  1.0643,  0.0210],        [ 1.8021,  0.6697,  0.8263],        [ 0.3601,  0.3765,  1.3859]])</code></pre><ul><li>加法：提供输出张量作为参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-1.1514, -0.5880, -0.0083],        [-0.4967,  0.2964,  2.5860],        [ 0.7163,  1.0643,  0.0210],        [ 1.8021,  0.6697,  0.8263],        [ 0.3601,  0.3765,  1.3859]])</code></pre><ul><li>加法：就地解决（(in-place)）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># adds x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-1.1514, -0.5880, -0.0083],        [-0.4967,  0.2964,  2.5860],        [ 0.7163,  1.0643,  0.0210],        [ 1.8021,  0.6697,  0.8263],        [ 0.3601,  0.3765,  1.3859]])</code></pre><blockquote><p>任何就地改变一个tensor的操作都以<code>_</code>为后缀。例如：<code>x.copy_(y)</code>, <code>x.t_()</code>，都会改变x。</p></blockquote><h4 id="Numpy与Torch张量的相互转换"><a href="#Numpy与Torch张量的相互转换" class="headerlink" title="Numpy与Torch张量的相互转换"></a>Numpy与Torch张量的相互转换</h4><blockquote><p>Torch的Tensor和Numpy的数组会共享它们的底层存储位置，改变其中一个，另外一个也会改变。</p></blockquote><h5 id="Torch张量转换成Numpy数组"><a href="#Torch张量转换成Numpy数组" class="headerlink" title="Torch张量转换成Numpy数组"></a>Torch张量转换成Numpy数组</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>) <span class="comment"># 创建一个torch张量</span></span><br><span class="line">print(a)</span><br><span class="line">b = a.numpy() <span class="comment"># 将torch张量转化为numpy数组</span></span><br><span class="line">print(b)</span><br><span class="line">a.add_(<span class="number">1</span>) <span class="comment"># 就地改变torch张量的值</span></span><br><span class="line">print(a) <span class="comment"># a torch张量发生改变</span></span><br><span class="line">print(b) <span class="comment"># b numpy数组因为共享底层存储所以也同时改变</span></span><br></pre></td></tr></table></figure><hr><pre><code>tensor([ 1.,  1.,  1.,  1.,  1.])[1. 1. 1. 1. 1.]tensor([ 2.,  2.,  2.,  2.,  2.])[2. 2. 2. 2. 2.]</code></pre><h5 id="Numpy数组T转换成orch张量"><a href="#Numpy数组T转换成orch张量" class="headerlink" title="Numpy数组T转换成orch张量"></a>Numpy数组T转换成orch张量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入numpy</span></span><br><span class="line">a = np.ones(<span class="number">5</span>) <span class="comment">#创建numpy数组</span></span><br><span class="line">b = torch.from_numpy(a) <span class="comment">#numpy转化成torch张量</span></span><br><span class="line">np.add(a, <span class="number">1</span>, out=a) <span class="comment">#numpy数组数据加一</span></span><br><span class="line">print(a) <span class="comment"># numpy数组发生变化</span></span><br><span class="line">print(b) <span class="comment"># torch张量因为与numpy共享底层存储因此也发生变化</span></span><br></pre></td></tr></table></figure><hr><pre><code>[2. 2. 2. 2. 2.]tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)</code></pre><h4 id="CUDA张量（CUDA-Tensors）"><a href="#CUDA张量（CUDA-Tensors）" class="headerlink" title="CUDA张量（CUDA Tensors）"></a>CUDA张量（CUDA Tensors）</h4><blockquote><p>可以使用.to方法将张量移动到任何设备上。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 我们使用 ``torch.device`` 对象 将张量移入和移出GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># 一个CUDA设备对象</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># 直接在GPU上创建一个张量对象</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># 或者使用``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))       <span class="comment"># ``.to`` 将结果转回cpu存储，还可以改变数据类型</span></span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-0.4230,  0.2093,  0.9444],        [ 0.0597,  0.7063,  2.9447],        [ 1.2958,  1.9914,  0.0450],        [ 2.2439,  0.8610,  1.2889],        [ 0.8210,  0.9997,  1.5241]], device=&apos;cuda:0&apos;)tensor([[-0.4230,  0.2093,  0.9444],        [ 0.0597,  0.7063,  2.9447],        [ 1.2958,  1.9914,  0.0450],        [ 2.2439,  0.8610,  1.2889],        [ 0.8210,  0.9997,  1.5241]], dtype=torch.float64)</code></pre><h3 id="Autograd-自动求导"><a href="#Autograd-自动求导" class="headerlink" title="Autograd:自动求导"></a>Autograd:自动求导</h3><p>在PyTorch中所有神经网络的核心是<code>autograd</code>软件包。我们先来简单介绍一下这个，然后再构建第一个神经网络。<br><code>autograd</code>包为Tensors上的所有操作提供了自动求导。它是一个运行过程中定义的框架（define-by-run），这意味着反向传播是由代码的运行方式来定义的，并且每一次迭代都可能不同。</p><h4 id="张量（Tensor）-gt-0-4版本前是Variable"><a href="#张量（Tensor）-gt-0-4版本前是Variable" class="headerlink" title="张量（Tensor）-&gt;0.4版本前是Variable"></a>张量（Tensor）-&gt;0.4版本前是Variable</h4><p><code>torch.Tensor</code>是包的中心类。如果你将属性<code>.requires_grad</code>设置为<code>True</code>，它将开始追踪所有的操作。当你完成了计算过程，你可以调用<code>.backward()</code>，之后所有的梯度计算都是自动的。<code>Tensor</code>的梯度将累积到<code>.grad</code>属性中。</p><p>要停止跟踪历史记录的<code>Tensor</code>，可以调用<code>.detach()</code>将其从计算历史记录中分离出来，并防止跟踪将来的计算。</p><p>为了防止跟踪历史记录（和使用内存），你也可以用<code>torch.no_grad()</code>包装代码块。 这在评估模型时特别有用，因为该模型可能具有<code>require_grad = True</code>的可训练参数，但我们不需要梯度值。</p><p>还有一个类对于<code>autograd</code>实现非常重要：一个<code>Function</code>。</p><p><code>Tensor</code>和<code>Function</code>是相互关联的，并建立一个非循环图，它编码构建了完整的计算过程。 每个变量都有一个<code>.grad_fn</code>属性，该属性反应在已创建<code>Tensor</code>的函数上（用户创建的<code>Tensor</code>除外 - 它们的<code>grad_fn</code>为<code>None</code>）。</p><p>如果你想计算导数，可以在<code>Tensor</code>上调用<code>.backward()</code>。如果<code>Tensor</code>是个标量（一个单元素数据），那么你不用为<code>backward()</code>指定任何参数，然而如果它有多个元素，你需要指定一个<code>gradient</code>参数，它是一个匹配尺寸的<code>Tensor</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>) <span class="comment"># 创建一个张量并设置`requires_grad = True`来跟踪计算</span></span><br><span class="line">print(x)  <span class="comment"># 打印x的值</span></span><br><span class="line">y = x + <span class="number">2</span> <span class="comment"># 对x张量进行计算操作</span></span><br><span class="line">print(y)  <span class="comment"># 打印y值</span></span><br><span class="line">print(y.grad_fn)       <span class="comment"># y是一个操作的结果，所以它有一个grad_fn。</span></span><br><span class="line">print(y.requires_grad) <span class="comment"># 打印y的requires_grad标志状态</span></span><br><span class="line">z = y * y * <span class="number">3</span>  <span class="comment"># 继续实现复杂的操作</span></span><br><span class="line">out = z.mean() <span class="comment"># 输出z的均值</span></span><br><span class="line">print(z, out)  <span class="comment"># 打印计算输出结果</span></span><br><span class="line">print(z.grad_fn)<span class="comment"># y是一个操作的结果，所以它有一个grad_fn。</span></span><br><span class="line">print(z.requires_grad) <span class="comment"># 打印z的requires_grad标志状态</span></span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[ 1.,  1.],        [ 1.,  1.]])tensor([[ 3.,  3.],        [ 3.,  3.]])&lt;AddBackward0 object at 0x7f181420a978&gt;Truetensor([[ 27.,  27.],        [ 27.,  27.]]) tensor(27.)&lt;MulBackward0 object at 0x7f180409e400&gt;True</code></pre><p><code>.requires_grad_(...)</code>就地更改现有张量的<code>requires_grad</code>标志。如果没有给出，函数输入标志默认为<code>True</code>。需要注意的是：python 的默认参数，调用的时候，test( ) 与 test(True)等价跟内部flag默认值无关。从打印看，内部flag默认值是False，但是输出结果flag为<code>True</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 创建一个2*2的张量a</span></span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))<span class="comment"># 计算</span></span><br><span class="line">print(a.requires_grad) <span class="comment"># 打印a的requires_grad标志状态</span></span><br><span class="line">a.requires_grad_(<span class="keyword">True</span>) <span class="comment"># 就地设置a的requires_grad标志状态</span></span><br><span class="line">print(a.requires_grad) <span class="comment"># 再次打印a的requires_grad标志状态</span></span><br><span class="line">b = (a * a).sum()      <span class="comment"># 由a计算引入b</span></span><br><span class="line">print(b.grad_fn)       <span class="comment"># b是一个操作的结果，所以它有一个grad_fn。</span></span><br><span class="line">print(b.requires_grad) <span class="comment"># 打印a的requires_grad标志状态</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = x*<span class="number">2</span></span><br><span class="line">    print(x.requires_grad) <span class="comment"># False</span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(x.requires_grad) <span class="comment"># False</span></span><br><span class="line">y = test(x) <span class="comment"># False</span></span><br><span class="line">print(y.requires_grad) <span class="comment"># True</span></span><br></pre></td></tr></table></figure><hr><pre><code>FalseTrue&lt;SumBackward0 object at 0x7f17b4c2d518&gt;TrueFalseFalseTrue</code></pre><h4 id="梯度（Gradients）"><a href="#梯度（Gradients）" class="headerlink" title="梯度（Gradients）"></a>梯度（Gradients）</h4><p>让我们使用反向传播<code>out.backward()</code>，它等同于<code>out.backward(torch.Tensor(1)</code>)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>) <span class="comment"># 创建一个张量并设置`requires_grad = True`来跟踪计算</span></span><br><span class="line">y = x + <span class="number">2</span> <span class="comment"># 对x张量进行计算操作</span></span><br><span class="line">z = y * y * <span class="number">3</span>  <span class="comment"># 继续实现复杂的操作</span></span><br><span class="line">out = z.mean() <span class="comment"># 输出z的均值</span></span><br><span class="line">out.backward() <span class="comment"># 实现反向传播</span></span><br><span class="line">print(x.grad)  <span class="comment"># 打印梯度 d(out)/dx</span></span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[ 4.5000,  4.5000],        [ 4.5000,  4.5000]])</code></pre><blockquote><p>4.5矩阵的计算过程如下所示：<br><img src="http://xukeqiniu.xukeai.cn/d760894019f896f40c8db376a9eb91a8.png" alt=""></p></blockquote><p>我们还可以使用autograd做一些疯狂的事情！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(type(y))</span><br><span class="line">print(type(y.data))</span><br><span class="line">print(y.data.norm())</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line">gradients = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.float)</span><br><span class="line">y.backward(gradients) <span class="comment"># 沿着某方向的梯度</span></span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([-0.3905,  1.3533,  1.0339])&lt;class &apos;torch.Tensor&apos;&gt;&lt;class &apos;torch.Tensor&apos;&gt;tensor(3.4944)tensor([ -399.9199,  1385.7303,  1058.7094])tensor([  102.4000,  1024.0000,     0.1024])</code></pre><p>我们还可以通过使用<code>torch.no_grad()</code>包装代码块来停止<code>autograd</code>跟踪在张量上的历史记录，其中<code>require_grad = True</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure><hr><pre><code>TrueTrueFalse</code></pre><h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>神经网络可以使用<code>torch.nn</code>包来构建。</p><p>前面的学习大致了解了<code>autograd</code>，<code>nn</code>依赖于<code>autograd</code>来定义模型并进行求导。一个<code>nn.Module</code>包含多个神经网络层，以及一个<code>forward(input)</code>方法来返回<code>output</code>。</p><p>例如，看看以下这个分类数字图像的网络：<br><img src="http://xukeqiniu.xukeai.cn/b55446e4c713cb025be7696dc6993bde.png" alt="LeNet"><br>它是一个简单的前馈网络。它将输入逐步地传递给多个层，然后给出输出。<br>一个典型的神经网络训练过程如下：</p><ul><li>定义一个拥有可学习参数（或权重）的神经网络</li><li>在输入数据集上进行迭代</li><li>在网络中处理输入数据</li><li>计算损失（输出离分类正确有多大距离）</li><li>梯度反向传播给网络的参数</li><li>更新网络的权重，通常使用一个简单的更新规则(SGD)：<code>weight = weight + learning_rate * gradient</code></li></ul><h4 id="定义网络结构"><a href="#定义网络结构" class="headerlink" title="定义网络结构"></a>定义网络结构</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># nn.Module子类的函数必须在构造函数中执行父类的构造函数</span></span><br><span class="line">        <span class="comment"># 等价于nn.Model.__init__(self)</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 一个图像输入通道（灰度图）, 6 输出通道（6张FeatureMap）, 5x5 卷积核</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 定义卷积层：输入6张特征图，输出16张特征图，卷积核5x5</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层：线性连接(y = Wx + b)，16*5*5个节点连接到120个节点上</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层：线性连接(y = Wx + b)，120个节点连接到84个节点上</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        <span class="comment"># 定义全连接层：线性连接(y = Wx + b)，84个节点连接到10个节点上</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义向前传播函数，并自动生成向后传播函数(autograd)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 输入x-&gt;conv1-&gt;relu-&gt;2x2窗口的最大池化-&gt;更新到x</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 输入x-&gt;conv2-&gt;relu-&gt;2x2窗口的最大池化-&gt;更新到x</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># view函数将张量x变形成一维向量形式，总特征数不变，为全连接层做准备</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))</span><br><span class="line">        <span class="comment"># 输入x-&gt;fc1-&gt;relu，更新到x</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        <span class="comment"># 输入x-&gt;fc2-&gt;relu，更新到x</span></span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        <span class="comment"># 输入x-&gt;fc3，更新到x</span></span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 除了批处理维度之外的所有维度。</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]</span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><hr><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><p>在实现过程中只需要定义<code>forward</code>函数，<code>backward</code>函数（用来计算梯度）是使用<code>autograd</code><strong>自动</strong>定义的。并且可以在<code>forward</code>中使用任意的<code>Tensor</code>运算操作。</p><p>模型中可学习的参数是通过<code>net.parameters()</code>返回的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[<span class="number">0</span>].size())  <span class="comment"># conv1's .weight</span></span><br></pre></td></tr></table></figure><hr><pre><code>10torch.Size([6, 1, 5, 5])</code></pre><p>让我们尝试一个随机的<code>32x32</code>输入!<br>注意：这个网络（LeNet）的预期输入大小是<code>32x32</code>。要在<code>MNIST</code>数据集上使用此网络，请将数据集中的图像调整为<code>32x32</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-0.0819,  0.1214,  0.0144, -0.0429,  0.0046,  0.0520, -0.0673,          0.0878, -0.1724, -0.1151]])</code></pre><p>将梯度缓冲区中所有的参数置0，并使用随机的梯度进行反向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><blockquote><p><code>torch.nn</code>仅支持<code>mini-batch</code>。整个的<code>torch.nn</code>包仅支持小批量的数据，而不是一个单独的样本。例如，<code>nn.Conv2d</code>应传入一个<code>4D</code>的<code>Tensor</code>，维度为（<code>nSamples X nChannels X Height X Width</code>）。如果你有一个单独的样本，使用<code>input.unsqueeze(0)</code>来添加一个伪批维度。</p></blockquote><p>回顾：</p><ul><li><code>torch.Tensor</code> 一个支持<code>autograd</code>操作（如<code>backward()</code>）的多维数组</li><li><code>nn.Module</code> 神经网络模块。封装参数的便捷方式，帮助者将它们移动到GPU，导出，加载等。</li><li><code>nn.Parameter</code> 一种<code>Tensor</code>，当给<code>Module</code>赋值时自动注册一个参数。</li><li><code>autograd.Function</code> 实现一个<code>autograd</code> 操作的 <code>forward</code> 和 <code>backward</code> 定义。每一个<code>Tensor</code>操作，创建至少一个<code>Function</code>节点，来连接那些创建<code>Tensor</code>的函数，并且记录其历史。</li></ul><p>在这里，我们涵盖了：</p><ul><li>定义神经网络</li><li>处理输入并调用<code>backward</code></li></ul><h4 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h4><p>一个损失函数以一个(<code>output</code>, <code>target</code>)对为输入，然后计算一个值用以估计输出结果离目标结果多远。<br>在nn的包里存在定义了多种损失函数。一个简单的损失函数：<code>nn.MSELoss</code> 它计算输出与目标的均方误差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.arange(<span class="number">1</span>, <span class="number">11</span>)  <span class="comment"># 一个虚拟的目标</span></span><br><span class="line">target = target.view(<span class="number">1</span>, <span class="number">-1</span>)  <span class="comment"># 使其形状与输出相同。</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure><pre><code>tensor(38.9289)</code></pre><p>现在，如果使用其<code>.grad_fn</code>属性反向追踪损失，您将看到一个如下所示的计算图：</p><blockquote><p>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d<br>      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear<br>      -&gt; MSELoss<br>      -&gt; loss</p></blockquote><p>因此，当我们调用<code>loss.backward()</code>时，损失对应的整个图都被求导，并且图中所有的<code>Tensor</code>都会带有累积了梯度的<code>.grad</code>属性<code>requres_grad=True</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure><hr><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;MseLossBackward object at 0x000002AE1A0953C8&gt;</span><br><span class="line">&lt;AddmmBackward object at 0x000002AE1A0954A8&gt;</span><br><span class="line">&lt;ExpandBackward object at 0x000002AE1A0953C8&gt;</span><br></pre></td></tr></table></figure><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>要进行反向传播，我们只需要调用<code>loss.backward()</code>。注意：<strong>需要清除现有的梯度，否则梯度将累积到现有梯度</strong>。</p><p>现在我们将调用<code>loss.backward()</code>，并看看<code>conv1</code>偏置在<code>backward</code>之前和之后的梯度变化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># 清除现有的梯度</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad) <span class="comment"># 打印之前的梯度值</span></span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'conv1.bias.grad after backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad) <span class="comment"># 打印反向传播之后的梯度值</span></span><br></pre></td></tr></table></figure><hr><pre><code>conv1.bias.grad before backwardtensor([ 0.,  0.,  0.,  0.,  0.,  0.])conv1.bias.grad after backwardtensor([ 0.0383,  0.1029,  0.0044,  0.1332,  0.0659, -0.0402])</code></pre><h4 id="权值更新"><a href="#权值更新" class="headerlink" title="权值更新"></a>权值更新</h4><p>实践中最简单的更新规则是随机梯度下降（SGD）：<br><code>weight = weight - learning_rate * gradient</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure><p>然而，当使用神经网络时，希望使用各种不同的更新规则，例如<code>SGD</code>，<code>Nesterov-SGD</code>，<code>Adam</code>，<code>RMSProp</code>等等。为了实现这一点，Pytorch构建一个优化包：<code>torch.optim</code>，来实现所有的方法。使用非常简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建优化器</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练的循环迭代中使用</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># 清除现有的梯度</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># 更新值</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：在观察梯度变化时，首先要通过<code>optimizer.zero_grad()</code><strong>清除现有的梯度，否则梯度将累积到现有梯度</strong>。</p></blockquote><h3 id="训练分类器"><a href="#训练分类器" class="headerlink" title="训练分类器"></a>训练分类器</h3><p>前面的教程中我们已经学习了如何定义神经网络，计算损失并更新网络的权重。接下来，我们完整的训练一个神经网络模型，并测试其性能。</p><h4 id="数据集说明"><a href="#数据集说明" class="headerlink" title="数据集说明"></a>数据集说明</h4><p>一般来说，当在处理图像，文本，音频或视频数据时，可以使用标准的<code>python</code>包将数据加载到一个<code>numpy</code>数组中。然后将这个数组转换成<code>torch.Tensor</code>。</p><ul><li>图像的话，可以用<code>Pillow</code>, <code>OpenCV</code>。</li><li>声音处理可以用<code>scipy</code>和<code>librosa</code>。</li><li>文本的处理使用原生<code>Python</code>或者<code>Cython</code>以及<code>NLTK</code>和<code>SpaCy</code>都可以。<br>特别是对于图像，<code>PyTorch</code>创建了一个名为<code>torchvision</code>的软件包，该软件包具有常用数据集（如<code>Imagenet</code>，<code>CIFAR10</code>，<code>MNIST</code>等）的数据加载器<code>torchvision.datasets</code>，以及用于图像的数据转换器<code>torch.utils.data.DataLoader</code>。这提供了巨大的便利并避免了编写样板代码。<br>本教程使用CIFAR10数据集。 我们要进行的分类的类别有：’airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。 这个数据集中的图像都是3通道，32x32像素的图片。<br><img src="http://xukeqiniu.xukeai.cn/dbb86359ca79fc4aaa66c65ef7258a04.png" alt=""></li></ul><h4 id="训练一个图像分类器"><a href="#训练一个图像分类器" class="headerlink" title="训练一个图像分类器"></a>训练一个图像分类器</h4><p>我们要按顺序做这几个步骤：</p><ul><li>使用torchvision来读取并预处理CIFAR10数据集</li><li>定义一个卷积神经网络</li><li>定义一个代价函数</li><li>在神经网络中训练训练集数据</li><li>使用测试集数据测试神经网络</li></ul><h5 id="1-加载和归一化CIFAR10"><a href="#1-加载和归一化CIFAR10" class="headerlink" title="1.加载和归一化CIFAR10"></a>1.加载和归一化CIFAR10</h5><p><code>torchvision</code>加载的数据集的输出是范围[0，1]的<code>PILImage</code>图像。我们将它们转换为归一化范围[-1，1]的张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="comment"># torchvision数据集的输出是在[0, 1]范围内的PILImage图片。</span></span><br><span class="line"><span class="comment"># 我们此处使用归一化的方法将其转化为Tensor，数据范围为[-1, 1]</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"><span class="comment">#加载训练集数据</span></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>,</span><br><span class="line">                                        download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="keyword">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#加载测试集数据</span></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="keyword">False</span>,</span><br><span class="line">                                       download=<span class="keyword">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                         shuffle=<span class="keyword">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#分类类别定义</span></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Files already downloaded and verifiedFiles already downloaded and verified</code></pre><p>我们来从中找几张图片看看。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#notebook模式下</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#显示图片的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取一些随机的训练图片</span></span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图片</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># 打印类型</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><hr><pre><code>deer   cat   dog  ship</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pytorch_60_output_95_1.png" alt=""></p><h5 id="2-定义卷积神经网络结构"><a href="#2-定义卷积神经网络结构" class="headerlink" title="2.定义卷积神经网络结构"></a>2.定义卷积神经网络结构</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure><h5 id="3-定义损失函数和优化器"><a href="#3-定义损失函数和优化器" class="headerlink" title="3.定义损失函数和优化器"></a>3.定义损失函数和优化器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h5 id="4-训练网络"><a href="#4-训练网络" class="headerlink" title="4.训练网络"></a>4.训练网络</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):  <span class="comment"># 训练集迭代次数</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 获取输入和标签</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度初始化置零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 正向+反向+优化</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印loss值</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># 每2000个batch打印一次</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>[1,  2000] loss: 2.199[1,  4000] loss: 1.887[1,  6000] loss: 1.707[1,  8000] loss: 1.614[1, 10000] loss: 1.536[1, 12000] loss: 1.504[2,  2000] loss: 1.449[2,  4000] loss: 1.411[2,  6000] loss: 1.372[2,  8000] loss: 1.349[2, 10000] loss: 1.325[2, 12000] loss: 1.306Finished Training</code></pre><h5 id="5-测试网络"><a href="#5-测试网络" class="headerlink" title="5.测试网络"></a>5.测试网络</h5><p>我们已经训练了两遍了。 此时需要测试一下到底结果如何。</p><p>通过对比神经网络给出的分类和已知的类别结果，可以得出正确与否。如果预测的正确，我们可以将样本加入正确预测的结果的列表中。</p><p>好的第一步，让我们展示几张照片来熟悉一下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印图片</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><hr><pre><code>GroundTruth:    cat  ship  ship plane</code></pre><p><img src="http://xukeqiniu.xukeai.cn/pyrorch_60_output_104_1.png" alt=""></p><p>现在让我们看看神经网络认为这些例子是什么：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(images)</span><br><span class="line">print(outputs)</span><br></pre></td></tr></table></figure><hr><pre><code>tensor([[-0.4905, -1.5664,  1.0641,  2.4226,  0.1196,  1.9381,  0.9795,         -0.4404, -1.7645, -1.6992],        [ 6.1866,  5.8665, -2.2267, -3.2581, -2.6794, -4.9095, -4.2326,         -5.3548,  6.9980,  2.7097],        [ 1.9322,  3.0127, -1.2481, -1.1180, -1.4086, -1.7913, -1.8129,         -1.9674,  2.3132,  1.7559],        [ 3.6228,  0.1119,  0.6089, -1.5255, -0.5566, -2.7542, -1.1817,         -3.3743,  4.5489, -0.5763]])</code></pre><p>输出是10类对应的数值。一个类对应的数值越高，网络认为这个图像就是越接近这个类。那么，让我们得到最高数值对应的类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><hr><pre><code>Predicted:    cat  ship   car  ship</code></pre><p>结果看起来挺好。<br>看看神经网络在整个数据集上的表现结果如何:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure><hr><pre><code>Accuracy of the network on the 10000 test images: 54 %</code></pre><p>从结果上看，神经网络输出的结果比随机要好，随机选择的话从十个中选择一个出来，准确率大概只有10%。<br>看上去神经网络学到了点东西。<br>我们看一下那么到底哪些类别表现良好又是哪些类别不太行呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure><hr><pre><code>Accuracy of plane : 59 %Accuracy of   car : 82 %Accuracy of  bird : 44 %Accuracy of   cat : 44 %Accuracy of  deer : 48 %Accuracy of   dog : 43 %Accuracy of  frog : 59 %Accuracy of horse : 52 %Accuracy of  ship : 73 %Accuracy of truck : 40 %</code></pre><h4 id="GPU训练"><a href="#GPU训练" class="headerlink" title="GPU训练"></a>GPU训练</h4><h4 id="多GPU训练"><a href="#多GPU训练" class="headerlink" title="多GPU训练"></a>多GPU训练</h4><h3 id="数据并行（选学）"><a href="#数据并行（选学）" class="headerlink" title="数据并行（选学）"></a>数据并行（选学）</h3><h4 id="导入和参数"><a href="#导入和参数" class="headerlink" title="导入和参数"></a>导入和参数</h4><h4 id="虚拟DataSet"><a href="#虚拟DataSet" class="headerlink" title="虚拟DataSet"></a>虚拟DataSet</h4><h4 id="简单模块"><a href="#简单模块" class="headerlink" title="简单模块"></a>简单模块</h4><h4 id="创建模块和数据并行"><a href="#创建模块和数据并行" class="headerlink" title="创建模块和数据并行"></a>创建模块和数据并行</h4><h4 id="执行模块"><a href="#执行模块" class="headerlink" title="执行模块"></a>执行模块</h4><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;PyTorch-60分钟入门&quot;&gt;&lt;a href=&quot;#PyTorch-60分钟入门&quot; class=&quot;headerlink&quot; title=&quot;PyTorch 60分钟入门&quot;&gt;&lt;/a&gt;PyTorch 60分钟入门&lt;/h2&gt;&lt;h3 id=&quot;PyTorch简介&quot;&gt;&lt;a href=&quot;#PyTorch简介&quot; class=&quot;headerlink&quot; title=&quot;PyTorch简介&quot;&gt;&lt;/a&gt;PyTorch简介&lt;/h3&gt;&lt;p&gt;这是一个基于Python的科学计算包，主要针对两类人群：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;替代Numpy以发挥GPU的强大能力&lt;/li&gt;
&lt;li&gt;一个提供最大灵活性和速度的深度学习研究平台&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://xywang93.github.io.git/categories/Deep-Learning/Pytorch/"/>
    
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
      <category term="Pytorch" scheme="http://xywang93.github.io.git/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Tnesorpack入门教程</title>
    <link href="http://xywang93.github.io.git/2018/04/01/DeepLearning/Tensorpack/Tensorpack%E7%AE%80%E4%BB%8B/"/>
    <id>http://xywang93.github.io.git/2018/04/01/DeepLearning/Tensorpack/Tensorpack简介/</id>
    <published>2018-03-31T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:24.409Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Tensorpack架构"><a href="#Tensorpack架构" class="headerlink" title="Tensorpack架构"></a>Tensorpack架构</h3><p><img src="http://xukeqiniu.xukeai.cn/19876566c073a98092648443139221e6.png" alt=""></p><ul><li><code>DataFlow</code>是一个用于在Python中高效地加载数据的库。除了DataFlow之外，本地TF运营商也可以用于数据加载。它们最终将被封装在相同的<code>InputSource</code>接口下并进行预取。</li><li>可以使用任何基于TF的符号函数库来定义模型，其中包括tensorpack中的一小组函数。 <code>ModelDesc</code>是连接模型的接口和<code>InputSource</code>的接口。</li><li>Tensorpack的<code>Trainers</code>用于管理训练过程中的循环迭代。它们还包括用于多GPU或分布式训练的数据并行逻辑。同时，也拥有很强大的定制能力。</li><li><code>Callbacks</code>就像<code>tf.train.SessionRunHook</code>或者<code>plugins</code>。在训练过程中，除了主迭代以外，您想要做的所有事情都可以通过回调进行定义并轻松重用。</li><li>所有组件尽管完美地结合在一起，但都具有高度的去相关性：您可以：<ul><li>单独使用DataFlow作为数据加载库，根本不用tensorfow。</li><li>使用tensorpack构建具有多GPU或分布式支持的图结构，然后使用自己的循环进行训练。</li><li>自行构建图形，并使用tensorpack回调进行训练。</li></ul></li></ul><a id="more"></a><h3 id="DataFlow-数据流接口"><a href="#DataFlow-数据流接口" class="headerlink" title="DataFlow(数据流接口)"></a>DataFlow(数据流接口)</h3><h4 id="DataFlow是什么？"><a href="#DataFlow是什么？" class="headerlink" title="DataFlow是什么？"></a>DataFlow是什么？</h4><p>DataFlow是一个用于构建Python迭代器以提高数据加载效率的库。</p><p><strong>定义</strong>：DataFlow有一个<code>get_data()</code>生成器方法，它产生数据点(<code>datapoints</code>)。datapoints是被称为数据点组件(components of a datapoin)的Python列表对象。</p><p><strong>例子</strong>：要训练MNIST数据集，需要使用<code>DataFlow</code>的<code>get_data()</code>方法，该方法需要生成两个组件的数据点（列表）：一个形状为<code>(64,28,28)</code>的numpy的数组(图片数据)和一个形状<code>(64， )</code>数组（图片标签）。</p><h4 id="DataFlow的组成"><a href="#DataFlow的组成" class="headerlink" title="DataFlow的组成"></a>DataFlow的组成</h4><p>有一个标准接口的好处是能够提供最大的代码可重用性。 tensorpack中有很多现有的DataFlow实用程序，您可以使用这些实用程序来组合具有长数据管道的复杂DataFlow。常见的流水线通常会<strong>从磁盘（或其他来源）读取</strong>，<strong>应用转换（ apply transformations）</strong>，<strong>分组批处理（group into batches）</strong> ，<strong>预取数据（group into batches）</strong>。一个简单的例子如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a DataFlow you implement to produce [tensor1, tensor2, ..] lists from whatever sources:</span></span><br><span class="line">df = MyDataFlow(dir=<span class="string">'/my/data'</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># resize the image component of each datapoint</span></span><br><span class="line">df = AugmentImageComponent(df, [imgaug.Resize((<span class="number">225</span>, <span class="number">225</span>))])</span><br><span class="line"><span class="comment"># group data into batches of size 128</span></span><br><span class="line">df = BatchData(df, <span class="number">128</span>)</span><br><span class="line"><span class="comment"># start 3 processes to run the dataflow in parallel</span></span><br><span class="line">df = PrefetchDataZMQ(df, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><h4 id="为什么要使用DataFlow"><a href="#为什么要使用DataFlow" class="headerlink" title="为什么要使用DataFlow"></a>为什么要使用DataFlow</h4><ul><li><strong>很简单</strong>：用纯Python编写所有内容，并重用现有的实用程序。相反，在TF操作员中编写数据加载器通常很痛苦，性能很难调整。</li><li><strong>速度非常快</strong>：可以构建具有并行性的快速DataFlow。在tensorpack中使用DataFlow，可以采用<code>Input Pipeline</code>,进一步加速图形中的数据加载。</li></ul><blockquote><p>尽管如此，tensorpack还支持用本地TF操作与TF数据集加载数据。</p></blockquote><h4 id="使用DataFlow（Tensorpack外部）"><a href="#使用DataFlow（Tensorpack外部）" class="headerlink" title="使用DataFlow（Tensorpack外部）"></a>使用DataFlow（Tensorpack外部）</h4><p>通常，tensorpack <code>InputSource</code>接口将DataFlow链接到图结构进行训练。如果在某些自定义代码中使用DataFlow，需要<strong>首先调用<code>reset_state()</code>初始化</strong>，然后使用生成器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">df = SomeDataFlow()</span><br><span class="line"></span><br><span class="line">df.reset_state() <span class="comment">#初始化</span></span><br><span class="line">generator = df.get_data()</span><br><span class="line"><span class="keyword">for</span> dp <span class="keyword">in</span> generator:</span><br><span class="line">    <span class="comment"># dp is now a list. do whatever</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>DataFlow独立于tensorpack和TensorFlow</strong>。要导入tensorpack.dataflow，甚至不必安装TensorFlow。<strong>可以简单地将其用作数据处理管道并将其插入任何其他框架</strong>。</p></blockquote><h3 id="Input-Pipeline（输入流水线）"><a href="#Input-Pipeline（输入流水线）" class="headerlink" title="Input Pipeline（输入流水线）"></a>Input Pipeline（输入流水线）</h3><blockquote><p>本教程包含关于“如何高效地读取数据以使用TensorFlow”以及tensorpack如何支持这些方法的主题的一般性讨论。 您不必阅读它，因为这些是tensorpack接口下的细节，但知道它可以帮助您在构建任务时，高效的选择最佳输入管道。</p></blockquote><h4 id="数据准备并行化"><a href="#数据准备并行化" class="headerlink" title="数据准备并行化"></a>数据准备并行化</h4><p><img src="http://xukeqiniu.xukeai.cn/98b00ec58d4658755b92f14dab5fa520.png" alt=""></p><p>无论使用什么框架，都可以有一个常识：<strong>Prepare data in parallel with the training!</strong><br>原因如下：</p><ul><li>数据准备通常会耗费不重要的时间（取决于实际问题）。</li><li><strong>数据准备与训练是独立的</strong>！这个并行的最根本前提！</li><li><strong>数据准备与训练通常使用完全不同的资源</strong>（参见上图）：Training过程使用GPU，加载数据的过程通过磁盘，处理数据采用CPU，拷贝数据到TF经过的是内存带宽，而拷贝到GPU是通过PCI-e总线。因此训练和数据准备两项任务一起完成并不会降低工作效率。事实上，你可以进一步并行化数据准备实现的不同阶段，因为他们也使用不同的资源。</li></ul><p>我们可以简单的计算一下：</p><blockquote><p>4台P100 GPU可以852张/秒的速度训练ResNet50，这些图像的大小为852 <em> 224 </em> 224 <em> 3 </em> 4bytes = 489MB。 假设你有 5GB / s的memcpy带宽（如果你运行单线程拷贝，大致就像这样），只需复制一次数据就需要0.1s ——<strong>将训练速度减慢10％</strong>。 并且在预处理期间还有很多运算成本依然需要耗费大量时间！。</p></blockquote><p><strong>未能隐藏数据准备延迟是看不到良好GPU利用率的主要原因</strong>。 因此一定要选择一个能够延迟隐藏的框架。 但是大多数其他TensorFlow封装都是基于<code>feed_dict</code>设计的。 Tensorpack有内置的机制来隐藏上述阶段的延迟。 这是Tensorpack速度更快的主要原因。</p><h4 id="Python-Reader-or-TF-Reader"><a href="#Python-Reader-or-TF-Reader" class="headerlink" title="Python Reader or TF Reader ?"></a>Python Reader or TF Reader ?</h4><p>无论您使用什么来加载/预处理数据，上述讨论都是有效的，无论是Python代码还是TensorFlow运算符，或者是两者的组合。这两个都支持tensorpack，推荐使用Python。</p><h5 id="TensorFlow-Reader优点"><a href="#TensorFlow-Reader优点" class="headerlink" title="TensorFlow Reader优点"></a>TensorFlow Reader优点</h5><p>人们经常认为他们应该使用tf.data，因为它很快。</p><ul><li>事实上，它一般情况下很快，但不一定。使用Python，您可以访问许多其他快速库，这些库在TF中可能不受支持。</li><li>Python足够快。只要数据准备与训练保持同步，并且上图中所有四个模块的延迟都隐藏起来，<strong>更快的读取不会对整体吞吐量产生任何影响</strong>。 对于大多数类型的问题，直到多GPU ImageNet训练的规模，如果您使用快速库（例如<code>tensorpack.dataflow</code>），Python可以提供足够的速度。</li></ul><h5 id="TensorFlow-Reader缺点"><a href="#TensorFlow-Reader缺点" class="headerlink" title="TensorFlow Reader缺点"></a>TensorFlow Reader缺点</h5><p>TensorFlow Reader的缺点显而易见——<strong>接口设计太复杂了！</strong><br>与运行数学模型不同，数据处理是一项复杂且结构不良（ poorly-structured）的任务。 您需要处理不同的格式，处理特殊案例，嘈杂的数据，数据组合。 这样做需要条件操作，循环，数据结构，有时甚至是异常处理。 这些操作自然不是符号图的该做的相关任务。</p><p>我们来看看用户对tf.data的要求：</p><ul><li>填充数据，混洗数据的不同方式</li><li>处理数据中的任何值</li><li>处理不是批量大小倍数的数据集</li><li>排序/跳过一些数据</li><li>将数据写入文件</li></ul><p>为了支持所有这些可以在Python中使用3行代码完成的功能，您需要一个新的TF API，或者向Dataset.from_generator（即Python再次）提出要求。<br>如果您的数据本来就非常干净并且格式正确，那么使用TF来读取数据才有意义。如果没有，你可能会想写一个脚本来格式化你的数据，但是你几乎已经写了一个Python加载器了！<br>考虑一下：编写一个Python脚本以便从某种格式转换为TF友好格式，然后是从这种格式转换为张量的TF脚本是浪费时间。 中间格式不一定存在。 您只需要正确的界面即可直接高效地将Python连接到图形。 <code>tensorpack.InputSource</code>就是这样的一个接口。</p><h4 id="InputSource"><a href="#InputSource" class="headerlink" title="InputSource"></a>InputSource</h4><p><code>InputSource</code>是tensorpack中的抽象接口，用于描述输入来自哪里以及它们如何进入图形。例如，</p><ul><li>FeedInput：来自DataFlow并获取图表（缓慢）。</li><li>QueueInput：来自DataFlow并由TF队列缓存在CPU上。</li><li>StagingInput：来自某个<code>InputSource</code>，然后由TF StagingArea在GPU上预取。</li><li>TFDatasetInput：来自<code>tf.data.Dataset</code>。</li><li>dataflow_to_dataset：来自DataFlow，并由<code>tf.data.Dataset</code>进一步处理。</li><li>TensorInput：来自你定义的张量（例如可以是读操作）。</li><li>ZMQInput：来自一些ZeroMQ管道，读取/预处理可能发生在不同的过程中，甚至不同的机器中。</li></ul><blockquote><p>通常，我们推荐<code>QueueInput + StagingInput</code>，因为它对大多数用例都很有用。 如果您的数据因任何原因必须来自单独的进程，请使用<code>ZMQInput</code>。 如果您仍然想使用TF读取操作，请定义一个<code>tf.data.Dataset</code>并使用<code>TFDatasetInput</code>。</p></blockquote><h3 id="Symbolic-Layers（符号层）"><a href="#Symbolic-Layers（符号层）" class="headerlink" title="Symbolic Layers（符号层）"></a>Symbolic Layers（符号层）</h3><p>Tensorpack包含一小部分通用模型基元，如conv / deconv，fc，bn，pooling层。 这些层的编写只是因为在tensorpack开发时没有其他选择。 现在，这些实现实际上可以<strong>直接调用<code>tf.layers</code></strong>。</p><p>现在，<strong>可以在tensorpack中使用tf.layers或任何其他符号库</strong>。使用tensorpack实现，您还可以通过<code>argscope</code>和<code>LinearWrap</code>，以简化代码。</p><blockquote><p>请注意，为了保持代码和预先训练模型的向后兼容性，tensorpack层与tf.layers有一些细微差别，包括变量名称和默认选项。有关详细信息，请参阅tensorpack API文档。</p></blockquote><h4 id="argscope-and-LinearWrap"><a href="#argscope-and-LinearWrap" class="headerlink" title="argscope and LinearWrap"></a>argscope and LinearWrap</h4><p><code>argscope</code>为您提供默认参数的上下文。 <code>LinearWrap</code>是简化构建“线性结构”模型的糖衣语法(syntax sugar)。</p><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> argscope(Conv2D, filters=<span class="number">32</span>, kernel_size=<span class="number">3</span>, activation=tf.nn.relu):</span><br><span class="line">  l = (LinearWrap(image)  <span class="comment"># the starting brace is only for line-breaking</span></span><br><span class="line">       .Conv2D(<span class="string">'conv0'</span>)</span><br><span class="line">       .MaxPooling(<span class="string">'pool0'</span>, <span class="number">2</span>)</span><br><span class="line">       .Conv2D(<span class="string">'conv1'</span>, padding=<span class="string">'SAME'</span>)</span><br><span class="line">       .Conv2D(<span class="string">'conv2'</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">       .FullyConnected(<span class="string">'fc0'</span>, <span class="number">512</span>, activation=tf.nn.relu)</span><br><span class="line">       .Dropout(<span class="string">'dropout'</span>, rate=<span class="number">0.5</span>)</span><br><span class="line">       .tf.multiply(<span class="number">0.5</span>)</span><br><span class="line">       .apply(func, *args, **kwargs)</span><br><span class="line">       .FullyConnected(<span class="string">'fc1'</span>, units=<span class="number">10</span>, activation=tf.identity)())</span><br></pre></td></tr></table></figure></p><p>上面示例代码等价如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">l = Conv2D(<span class="string">'conv0'</span>, image, <span class="number">32</span>, <span class="number">3</span>, activation=tf.nn.relu)</span><br><span class="line">l = MaxPooling(<span class="string">'pool0'</span>, l, <span class="number">2</span>)</span><br><span class="line">l = Conv2D(<span class="string">'conv1'</span>, l, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="string">'SAME'</span>, activation=tf.nn.relu)</span><br><span class="line">l = Conv2D(<span class="string">'conv2'</span>, l, <span class="number">32</span>, <span class="number">5</span>, activation=tf.nn.relu)</span><br><span class="line">l = FullyConnected(<span class="string">'fc0'</span>, l, <span class="number">512</span>, activation=tf.nn.relu)</span><br><span class="line">l = Dropout(<span class="string">'dropout'</span>, l, rate=<span class="number">0.5</span>)</span><br><span class="line">l = tf.multiply(l, <span class="number">0.5</span>)</span><br><span class="line">l = func(l, *args, **kwargs)</span><br><span class="line">l = FullyConnected(<span class="string">'fc1'</span>, l, <span class="number">10</span>, activation=tf.identity)</span><br></pre></td></tr></table></figure><h4 id="访问相关的张量"><a href="#访问相关的张量" class="headerlink" title="访问相关的张量"></a>访问相关的张量</h4><p>层中的变量将命名为<code>name / W</code>，<code>name / b</code>等。有关详细信息，请参阅每个图层的API文档。在构建图时，可以像这样访问变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">l = Conv2D(<span class="string">'conv1'</span>, l, <span class="number">32</span>, <span class="number">3</span>)</span><br><span class="line">print(l.variables.W)</span><br><span class="line">print(l.variables.b)</span><br></pre></td></tr></table></figure><blockquote><p>但请注意，这是一种很冒险的方式，可能不适用于未来版本的TensorFlow。此方法也不适用于LinearWrap，并且无法访问由激活函数创建的变量。除非在API中有不同的说明，否则层的输出通常被命名为<code>name/output</code>。你可以打印张量来查看它的名字。</p></blockquote><h4 id="在Tensorpack外使用模型"><a href="#在Tensorpack外使用模型" class="headerlink" title="在Tensorpack外使用模型"></a>在Tensorpack外使用模型</h4><p>您可以单独使用tensorpack模型作为简单的符号函数库。为此，只需在定义模型时输入<code>TowerContext</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> TowerContext(<span class="string">''</span>, is_training=<span class="keyword">True</span>):</span><br><span class="line">  <span class="comment"># call any tensorpack layer</span></span><br></pre></td></tr></table></figure><p>某些层（特别是BatchNorm）具有不同的训练/测试时间行为,此行为由TowerContext控制。如果您需要在测试时间使用它们的tensorpack版本，则需要在另一个上下文中为它们创建操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Open a `reuse=True` variable scope here if you're sharing variables, then:</span></span><br><span class="line"><span class="keyword">with</span> TowerContext(<span class="string">'some_name_or_empty_string'</span>, is_training=<span class="keyword">False</span>):</span><br><span class="line">  <span class="comment"># build the graph again</span></span><br></pre></td></tr></table></figure><h4 id="在Tensorpack中使用其他符号库"><a href="#在Tensorpack中使用其他符号库" class="headerlink" title="在Tensorpack中使用其他符号库"></a>在Tensorpack中使用其他符号库</h4><p>在定义模型的过程中可以使用任何觉得舒服的库来构建图结构。<br>通常，<code>slim</code> / <code>tflearn</code> / <code>tensorlayer</code>只是符号函数包装器，调用它们与调用<code>tf.add</code>没什么区别。 不过，需要小心如何正规化/ BN更新应该在这些库中的处理。<br>这与使用 <code>sonnet</code>/ <code>Keras</code>有点不同。 <code>sonnet</code> / <code>Keras</code> 通过自己的模型类来管理变量范围，调用它们的符号函数总是创建新的变量范围。 请参阅Keras示例以了解如何在tensorpack内使用它。</p><h3 id="Trainers"><a href="#Trainers" class="headerlink" title="Trainers"></a>Trainers</h3><p>Tensorpack训练器包含以下逻辑：</p><ul><li>构件图结构</li><li>运行迭代（带回调）<blockquote><p>通常我们不会直接触摸这些方法，而是在训练器上使用更高级的接口。我们只需选择要使用的训练器。但是他们如何工作的一些基本知识是有用的：</p></blockquote></li></ul><h4 id="Tower-Trainer"><a href="#Tower-Trainer" class="headerlink" title="Tower Trainer"></a>Tower Trainer</h4><p>遵循TensorFlow中的术语，<code>Tower Trainer</code>是一个可调用的函数，它接受输入张量并将模型的<strong>一个重复项</strong>添加到图中。这种概念可以描述大多数类型的神经网络训练。<code>Tower</code>的概念主要用于支持：</p><ul><li>数据并行多GPU训练，其中在每个GPU上构建复制品。</li><li>用于推理的图构造，其中复制是在推理模式下构建的。<br>用户需要提供Tower函数才能使用<code>TowerTrainer</code>。特别是，在使用<code>ModelDesc</code>接口时，<code>build_graph</code>方法将成为Tower函数。</li></ul><p>Tower函数需要遵循一些约定：</p><ul><li>它可能被<strong>多次调用</strong>以进行数据并行训练或推理。<ul><li>因此，要使用tensorflow-hub模块，需要在tower函数外初始化模块，并在tower函数内调用模块。</li></ul></li><li>它必须遵循变量集合<ul><li>（必需）只能将可通过梯度下降调整的变量放入<code>TRAINABLE_VARIABLES</code>中。</li><li>（推荐）将需要用于推理的不可训练变量放入<code>MODEL_VARIABLES</code>中。</li></ul></li><li>它必须遵循变量范围：<ul><li>在函数中创建的任何可训练变量的名称必须与“variable_scope_name / custom / name”类似。不要依赖于name_scope的名字。不要使用两次variable_scope的名字。</li></ul></li><li>创建任何可训练变量都必须遵守<strong>重用</strong>变量范围。要遵守变量重用，请在函数中使用<code>tf.get_variable</code>而不是<code>tf.Variable</code>。另一方面，对于不可训练的变量，可以使用<code>tf.Variable</code>确保在每个tower中创建新变量，即使<code>reuse= True</code>时也是如此。</li><li>总是在<code>TowerContext</code>下调用，可以通过<code>get_current_tower_contxt()</code>来访问它。上下文包含有关训练/推理模式，重用等信息。<blockquote><p>这些约定很容易遵循，并且大多数层封装（例如，tf.layers / slim / tensorlayer）确实遵循它们。 请注意，某些Keras层不遵循这些约定，如果在tensorpack中使用，将需要一些变通方法。</p></blockquote></li></ul><p>当然你也可以不这样编写，但所有现有的tensorpack trainers都是TowerTrainer的子类。</p><h4 id="MultiGPU-Trainers"><a href="#MultiGPU-Trainers" class="headerlink" title="MultiGPU Trainers"></a>MultiGPU Trainers</h4><p>对于数据并行多GPU训练，不同的多GPU训练将实施不同的分配策略。他们以有效的方式照顾器件布局，梯度平均和同步，并且都达到了官方TF基准测试的相同性能。只需要一行代码更改即可使用它们，即<code>trainer=SyncMultiGPUTrainerReplicated()</code>。<br>请注意使用这些trains时的一些<strong>常见问题</strong>：</p><ul><li><p>在每次迭代中，所有GPU（模型的所有复制品）从InputSource中获取张量，而不是全部和分割。 所以总的批量大小将变成<code>(batch size of InputSource) * #GPU</code></p><blockquote><p>为数据并行训练分割张量根本没有意义。 首先，为什么要将时间串联起来分成大批量然后再分开呢？ 其次，这会对数据造成不必要的形状限制。 通过让每个GPU训练自己的输入张量，他们可以同时训练不同形状的输入。</p></blockquote></li><li><p>tower函数（您的模型代码）将被称为乘法时间。 因此，在修改这些函数中的全局状态时需要小心，例如 将操作添加到TF集合中。</p></li></ul><h4 id="Distributed-Trainers"><a href="#Distributed-Trainers" class="headerlink" title="Distributed Trainers"></a>Distributed Trainers</h4><p>分布式培训需要提供高性能allreduce实现的horovod库。要运行分布式培训，首先正确安装horovod，然后参阅HorovodTrainer的文档。</p><p>Tensorpack已经使用TF的本地API实现了一些其他分布式训练，但即使在今天，TF对分布式训练的本地支持也不是很高的性能。 因此这些训练没有积极维护，不推荐使用。</p><h3 id="Training-Interface"><a href="#Training-Interface" class="headerlink" title="Training Interface"></a>Training Interface</h3><p>Tensorpack有一个详尽的接口以获得最大的灵活性。当不想过多定制，训练时可以使用tensorpack提供的接口来简化代码。</p><h4 id="使用ModelDesc和TrainConfig"><a href="#使用ModelDesc和TrainConfig" class="headerlink" title="使用ModelDesc和TrainConfig"></a>使用ModelDesc和TrainConfig</h4><p>这是旧Tensorpack用户最熟悉的接口，仅用于单一成本任务（ single-cost tasks ）。这个接口有很多例子。<br><code>SingleCost trainers</code>需要4个参数来设置图：<code>InputDesc</code>，<code>InputSource</code>，<code>get_cost</code>函数和<code>optimizer</code>。<code>ModelDesc</code>通过将它们中的三个打包成一个对象来描述一个模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span><span class="params">(ModelDesc)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_inputs</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [tf.placeholder(dtype, shape, name), tf.placeholder(dtype, shape, name), ... ]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_build_graph</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">    tensorA, tensorB = inputs</span><br><span class="line">    <span class="comment"># build the graph</span></span><br><span class="line">    self.cost = xxx   <span class="comment"># define the cost tensor</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_optimizer</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.GradientDescentOptimizer(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p><code>_get_inputs</code> 需要定义构建图的所有输入的元信息。</p><p><code>_build_graph</code>获取与<code>_get_inputs</code>匹配的<code>inputs</code>张量列表。</p><p>在<code>_build_graph</code>中可以使用任何符号函数，包括TensorFlow核心库函数和其他符号库。<code>_build_graph</code>是tower函数，所以需要<strong>遵循一些规则</strong>。同时还需要在此函数中设置<code>self.cost</code>。</p><p>定义了这样一个模型后，使用<code>TrainConfig</code>和<code>launch_train_with_config</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">config = TrainConfig(</span><br><span class="line">   model=MyModel()</span><br><span class="line">   dataflow=my_dataflow,</span><br><span class="line">   <span class="comment"># data=my_inputsource, # alternatively, use a customized InputSource</span></span><br><span class="line">   callbacks=[...],    <span class="comment"># some default callbacks are automatically applied</span></span><br><span class="line">   <span class="comment"># some default monitors are automatically applied</span></span><br><span class="line">   steps_per_epoch=<span class="number">300</span>,   <span class="comment"># default to the size of your InputSource/DataFlow</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = SomeTrainer()</span><br><span class="line"><span class="comment"># trainer = SyncMultiGPUTrainerParameterServer(8)</span></span><br><span class="line">launch_train_with_config(config, trainer)</span><br></pre></td></tr></table></figure><h4 id="Raw-Trainer-Interface"><a href="#Raw-Trainer-Interface" class="headerlink" title="Raw Trainer Interface"></a>Raw Trainer Interface</h4><p>要获得较低级别的控件，您还可以直接访问训练的方法：</p><ul><li><strong>构建图</strong>：对于一般训练，请自行构建图结构。 对于单成本训练（ single-cost trainer），请通过<code>SingleCostTrainer.setup_graph</code>构建图结构。</li><li><strong>运行迭代</strong>：调用<code>Trainer.train()</code>或`Trainer.train_with_defaults()``，它为正常用例提供了一些默认选项。</li></ul><h3 id="Callbacks"><a href="#Callbacks" class="headerlink" title="Callbacks"></a>Callbacks</h3><p>回调是除了训练迭代以外的<strong>其他所有事情</strong>的接口。<br>除了最小化cost的实际训练迭代之外，你可能想要做一些其他事情。如下：</p><ul><li>在训练开始之前（例如：初始化保存程序，转储图）</li><li>随着每次训练迭代（例如，在图中运行一些其他操作）</li><li>在训练迭代之间（例如更新进度条，更新超参数）</li><li>在周期之间（例如保存模型，运行一些验证）</li><li>训练后（例如，在某处发送模型，发送消息到您的手机）</li></ul><p>人们传统上倾向于将训练循环与这些额外功能一起编写。这会使循环冗长，同一特征的代码可能会分开（设想一个特征，它需要在开始时进行初始化，然后在迭代之间进行一些实际工作）。</p><p>通过编写回调来实现在每个地方做什么，tensorpack训练器将在适当的时候调用回调。因此，只要您使用tensorpack，这些功能就可以一条线（single line）的重复使用。</p><p>例如，这些在训练ResNet时使用的回调：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">callbacks=[</span><br><span class="line">  <span class="comment"># save the model every epoch</span></span><br><span class="line">  ModelSaver(),</span><br><span class="line">  <span class="comment"># backup the model with best validation error</span></span><br><span class="line">  MinSaver(<span class="string">'val-error-top1'</span>),</span><br><span class="line">  <span class="comment"># run inference on another Dataflow every epoch, compute classification error and log to monitors</span></span><br><span class="line">  InferenceRunner(dataset_val, [</span><br><span class="line">      ClassificationError(<span class="string">'wrong-top1'</span>, <span class="string">'val-error-top1'</span>),</span><br><span class="line">      ClassificationError(<span class="string">'wrong-top5'</span>, <span class="string">'val-error-top5'</span>)]),</span><br><span class="line">  <span class="comment"># schedule the learning rate based on epoch number</span></span><br><span class="line">  ScheduledHyperParamSetter(<span class="string">'learning_rate'</span>,</span><br><span class="line">                            [(<span class="number">30</span>, <span class="number">1e-2</span>), (<span class="number">60</span>, <span class="number">1e-3</span>), (<span class="number">85</span>, <span class="number">1e-4</span>), (<span class="number">95</span>, <span class="number">1e-5</span>)]),</span><br><span class="line">  <span class="comment"># can manually change the learning rate through a file, without interrupting training</span></span><br><span class="line">  HumanHyperParamSetter(<span class="string">'learning_rate'</span>),</span><br><span class="line">  <span class="comment"># send validation error to my phone through pushbullet</span></span><br><span class="line">  SendStat(<span class="string">'curl -u your_id_xxx: https://api.pushbullet.com/v2/pushes \\</span></span><br><span class="line"><span class="string">             -d type=note -d title="validation error" \\</span></span><br><span class="line"><span class="string">             -d body=&#123;val-error-top1&#125; &gt; /dev/null 2&gt;&amp;1'</span>,</span><br><span class="line">             <span class="string">'val-error-top1'</span>),</span><br><span class="line">  <span class="comment"># record GPU utilizations during training</span></span><br><span class="line">  GPUUtilizationTracker(),</span><br><span class="line">  <span class="comment"># touch a file to pause the training and start a debug shell, to observe what's going on</span></span><br><span class="line">  InjectShell(shell=<span class="string">'ipython'</span>),</span><br><span class="line">  <span class="comment"># estimate time until completion</span></span><br><span class="line">  EstimatedTimeLeft()</span><br><span class="line">] + [    <span class="comment"># these callbacks are enabled by default already, though you can customize them</span></span><br><span class="line">  <span class="comment"># maintain those moving average summaries defined in the model (e.g. training loss, training error)</span></span><br><span class="line">  MovingAverageSummary(),</span><br><span class="line">  <span class="comment"># draw a progress bar</span></span><br><span class="line">  ProgressBar(),</span><br><span class="line">  <span class="comment"># run `tf.summary.merge_all` every epoch and log to monitors</span></span><br><span class="line">  MergeAllSummaries(),</span><br><span class="line">  <span class="comment"># run ops in GraphKeys.UPDATE_OPS collection along with training, if any</span></span><br><span class="line">  RunUpdateOps(),</span><br><span class="line">],</span><br><span class="line">monitors=[        <span class="comment"># monitors are a special kind of callbacks. these are also enabled by default</span></span><br><span class="line">  <span class="comment"># write everything to tensorboard</span></span><br><span class="line">  TFEventWriter(),</span><br><span class="line">  <span class="comment"># write all scalar data to a json file, for easy parsing</span></span><br><span class="line">  JSONWriter(),</span><br><span class="line">  <span class="comment"># print all scalar data every epoch (can be configured differently)</span></span><br><span class="line">  ScalarPrinter(),</span><br><span class="line">]</span><br></pre></td></tr></table></figure><blockquote><p>请注意，回调涵盖了训练的每个细节，从图操作到进度条。这意味着我们可以根据自己的喜好自定义训练的各个部分，例如，在进度条上显示不同的内容，以不同训练阶段评估部分结果等。</p></blockquote><p>这些功能并不总是必要的，但是想想如果你将这些逻辑与循环一起写入，主循环看起来有多混乱。如果您在需要时只用一条线即可启用这些功能，那么将变得多么简单。</p><h3 id="Save-and-Load-models"><a href="#Save-and-Load-models" class="headerlink" title="Save and Load models"></a>Save and Load models</h3><h4 id="Work-with-TF-Checkpoint"><a href="#Work-with-TF-Checkpoint" class="headerlink" title="Work with TF Checkpoint"></a>Work with TF Checkpoint</h4><p><code>ModelSaver</code>回调以TensorFlow检查点(checkpoint)格式将模型保存到<code>logger.get_logger_dir()</code>定义的目录中。TF检查点通常包含<code>.data-xxxxx</code>文件和<code>.index</code>文件。两者都是必要的。<br><code>tf.train.NewCheckpointReader</code>是解析TensorFlow检查点的最佳工具。我们有两个示例脚本来演示它的用法，但请阅读TF文档以获取详细信息。</p><ul><li><code>scripts/ls-checkpoint.py</code>演示如何在检查点打印所有变量及其形状。</li><li><code>scripts/dump-model-params.py</code>可用于删除检查点中不必要的变量。它需要一个<code>metagraph</code>文件（它也保存在ModelSaver中），只保存模型在推理时需要的变量。它可以将模型转储到以npz格式保存的<code>var-name：value</code>字典中。</li></ul><h4 id="Load-a-Model"><a href="#Load-a-Model" class="headerlink" title="Load a Model"></a>Load a Model</h4><p>模型加载（在训练或测试中）是通过<code>session_init</code>接口。 目前有两种方法可以恢复会话：<code>session_init = SaverRestore(…)</code>来恢复TF检查点，或<code>session_init = DictRestore(…)</code>来恢复字典。<code>get_model_loader</code>帮助决定使用文件名中哪一个。要加载多个模型，请使用<code>ChainInit</code>。<br>变量还原完全基于当前图中变量与session_init初始值设定项中的变量之间的<strong>名称匹配</strong>。仅出现在一侧的变量将被打印为警告。</p><h4 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h4><p>迁移学习是很简单的。如果你想加载一些模型，只需使用相同的变量名称即可。如果你想重新训练一些层，只需重新命名相关层即可。</p><h3 id="Summary-and-Logging"><a href="#Summary-and-Logging" class="headerlink" title="Summary and Logging"></a>Summary and Logging</h3><blockquote><p>在训练期间，除迭代以外的所有内容均通过回调执行。本教程将解释如何在回调中处理汇总和日志记录，以及如何定制它们。默认的日志记录行为应该足够用于正常的用例，所以你可以跳过本教程。</p></blockquote><h4 id="TensorFlow-Summaries"><a href="#TensorFlow-Summaries" class="headerlink" title="TensorFlow Summaries"></a>TensorFlow Summaries</h4><p>这是TensorFlow如何最终被记录/保存/打印的总结：</p><ul><li><strong>What to Log</strong>：当在构造图代码中调用<code>tf.summary.xxx</code>时，TensorFlow会将一个操作添加到<code>tf.GraphKeys.SUMMARIES</code>集合（默认情况下）。</li><li><strong>When to Log</strong>：<code>MergeAllSummaries</code>回调位于默认回调中。它每个周期（默认情况下）在<code>SUMMARIES</code>集合（默认情况下）运行，并将结果写入监视器。</li><li><strong>Where to Log</strong>：默认情况下启用多个监视器。<ul><li><code>TFEventWriter</code>通过<code>tensorboard</code>将东西写入所使用的事件中。</li><li><code>ScalarPrinter</code>打印终端中的所有标量。</li><li><code>JSONWriter</code>将标量保存到JSON文件。</li></ul></li></ul><p>所有的 “what, when, where”可以在图或者回调(callbacks)/监视器(monitors)设置中自定义。<br>由于<code>TF summaries</code>在默认情况下不会频繁被评估（每个epoch），如果内容是依赖于数据的，那么这些值可能具有很高的方差。 为了解决这个问题，你可以：</p><ul><li>更改“What to Log”：更频繁地记录日志，但请注意，记录某些summaries可能很昂贵。可能需要使用单独的集合进行频繁日志记录。</li><li>更改“When to Log”：可以在标量张量上调用<code>tfutils.summary.add_moving_summary</code>，它将总结这些标量的移动平均值，而不是它们的即时值。移动平均由<code>MovingAverageSummary</code>回调（默认启用）维护。</li></ul><h4 id="Other-Logging-Data"><a href="#Other-Logging-Data" class="headerlink" title="Other Logging Data"></a>Other Logging Data</h4><p>除了TensorFlow summaries外，回调还可以在训练开始后的任何时候通过<code>self.trainer.monitors.put_xxx</code>将其他数据写入监视器后端。只要支持数据类型，数据将被分派到并记录到同一个地方。<br>因此，<strong>tensorboard不仅会显示图中的信息，还会显示我们自定义的数据</strong>。例如，验证集的准确度通常需要在TensorFlow图之外计算。通过一个统一的监视器后端，这个数字也会显示在tensorboard上。</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="Inference-During-Training"><a href="#Inference-During-Training" class="headerlink" title="Inference During Training"></a>Inference During Training</h4><p>训练期间有两种方法可以进行推理。</p><ul><li>最简单的方法是编写回调函数，并使用<code>self.trainer.get_predictor()</code>在推理模式下获取可调用的函数。</li><li>如果推理过程中遵循以下范例：“每个输入获取一些张量，并汇总结果”。可以使用<code>InferenceRunner</code>接口和一些<code>Inferencer</code>。这将进一步支持预取和数据并行推断。</li></ul><p>在这两种方法中，tower函数都会被再次调用，使用<code>TowerContext.is_training == False</code>构建不同的图。</p><h4 id="Inference-After-Training"><a href="#Inference-After-Training" class="headerlink" title="Inference After Training"></a>Inference After Training</h4><p>Tensorpack<strong>不关心训练后的操作</strong>。它将模型保存为标准检查点格式，以及metagraph protobuf文件。它们足以用于TensorFlow进行任何部署方法。但是您需要阅读TF文档并自行完成。</p><h3 id="FAQs"><a href="#FAQs" class="headerlink" title="FAQs"></a>FAQs</h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Tensorpack架构&quot;&gt;&lt;a href=&quot;#Tensorpack架构&quot; class=&quot;headerlink&quot; title=&quot;Tensorpack架构&quot;&gt;&lt;/a&gt;Tensorpack架构&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/19876566c073a98092648443139221e6.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DataFlow&lt;/code&gt;是一个用于在Python中高效地加载数据的库。除了DataFlow之外，本地TF运营商也可以用于数据加载。它们最终将被封装在相同的&lt;code&gt;InputSource&lt;/code&gt;接口下并进行预取。&lt;/li&gt;
&lt;li&gt;可以使用任何基于TF的符号函数库来定义模型，其中包括tensorpack中的一小组函数。 &lt;code&gt;ModelDesc&lt;/code&gt;是连接模型的接口和&lt;code&gt;InputSource&lt;/code&gt;的接口。&lt;/li&gt;
&lt;li&gt;Tensorpack的&lt;code&gt;Trainers&lt;/code&gt;用于管理训练过程中的循环迭代。它们还包括用于多GPU或分布式训练的数据并行逻辑。同时，也拥有很强大的定制能力。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Callbacks&lt;/code&gt;就像&lt;code&gt;tf.train.SessionRunHook&lt;/code&gt;或者&lt;code&gt;plugins&lt;/code&gt;。在训练过程中，除了主迭代以外，您想要做的所有事情都可以通过回调进行定义并轻松重用。&lt;/li&gt;
&lt;li&gt;所有组件尽管完美地结合在一起，但都具有高度的去相关性：您可以：&lt;ul&gt;
&lt;li&gt;单独使用DataFlow作为数据加载库，根本不用tensorfow。&lt;/li&gt;
&lt;li&gt;使用tensorpack构建具有多GPU或分布式支持的图结构，然后使用自己的循环进行训练。&lt;/li&gt;
&lt;li&gt;自行构建图形，并使用tensorpack回调进行训练。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="Tensorpack" scheme="http://xywang93.github.io.git/categories/Deep-Learning/Tensorpack/"/>
    
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
      <category term="Tensorpack" scheme="http://xywang93.github.io.git/tags/Tensorpack/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow神经网络之DCGAN</title>
    <link href="http://xywang93.github.io.git/2018/03/15/DeepLearning/TensorFlow/15Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BDCGAN/"/>
    <id>http://xywang93.github.io.git/2018/03/15/DeepLearning/TensorFlow/15Tensorflow神经网络之DCGAN/</id>
    <published>2018-03-14T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:38.428Z</updated>
    
    <content type="html"><![CDATA[<h3 id="DCGAN简介"><a href="#DCGAN简介" class="headerlink" title="DCGAN简介"></a>DCGAN简介</h3><p>DCGAN在GAN的基础上优化了网络结构，加入了 <code>conv</code>，<code>batch_norm</code> 等层，使得网络更容易训练，网络结构如下：<br><img src="http://xukeqiniu.xukeai.cn/67d781d61c77b7d1985f57ecc932a1e3.png" alt="DCGAN网络结构图"><br>注意：本图只是示例，与下面实际网络参数不对应。</p><a id="more"></a><h3 id="Tensorflow实现DCGAN"><a href="#Tensorflow实现DCGAN" class="headerlink" title="Tensorflow实现DCGAN"></a>Tensorflow实现DCGAN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, absolute_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练参数</span></span><br><span class="line">num_steps = <span class="number">10000</span> <span class="comment"># 总迭代次数</span></span><br><span class="line">batch_size = <span class="number">128</span> <span class="comment"># 批量大小</span></span><br><span class="line">lr_generator = <span class="number">0.002</span> <span class="comment"># 生成器学习率</span></span><br><span class="line">lr_discriminator = <span class="number">0.002</span> <span class="comment"># 判别器学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络参数</span></span><br><span class="line">image_dim = <span class="number">784</span> <span class="comment"># 28*28 pixels * 1 channel</span></span><br><span class="line">noise_dim = <span class="number">100</span> <span class="comment"># Noise data points</span></span><br></pre></td></tr></table></figure><h4 id="构建DCGAN网络"><a href="#构建DCGAN网络" class="headerlink" title="构建DCGAN网络"></a>构建DCGAN网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建网络</span></span><br><span class="line"><span class="comment"># 网络输入</span></span><br><span class="line">noise_input = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, noise_dim])  <span class="comment"># 生成器输入 噪声 batch*100，none后面被赋值batch</span></span><br><span class="line">real_image_input = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]) <span class="comment"># 判别器输入 真实图像 batch*28*28*1</span></span><br><span class="line"><span class="comment"># A boolean to indicate batch normalization if it is training or inference time</span></span><br><span class="line"><span class="comment"># 判断是否在训练</span></span><br><span class="line">is_training = tf.placeholder(tf.bool)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义激活函数 LeakyReLU，在判别器网络中用</span></span><br><span class="line"><span class="comment"># LeakyReLU 是 ReLU 的变种 [^1]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leakyrelu</span><span class="params">(x, alpha=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * (<span class="number">1</span> + alpha) * x + <span class="number">0.5</span> * (<span class="number">1</span> - alpha) * abs(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义生成器网络</span></span><br><span class="line"><span class="comment"># 输入：噪声  输出：图像</span></span><br><span class="line"><span class="comment"># 训练时，才使用batch_normalization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(x, reuse=False)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'Generator'</span>, reuse=reuse):</span><br><span class="line">        <span class="comment"># 第一层为全连接层，含神经元个数为7*7*128，输入是噪声batch*100</span></span><br><span class="line">        x = tf.layers.dense(x, units=<span class="number">7</span> * <span class="number">7</span> * <span class="number">128</span>)</span><br><span class="line">        <span class="comment"># tf.layers.batch_normalization() 的第二个参数axis表示在哪一个维度做normalize，通常数据排布顺序为(batch, height, width, channels)，固默认为-1</span></span><br><span class="line">        <span class="comment"># 全连接层channel=1，所以是对所有数据做normalize</span></span><br><span class="line">        x = tf.layers.batch_normalization(x, training=is_training)</span><br><span class="line">        <span class="comment"># 激活函数 rule</span></span><br><span class="line">        x = tf.nn.relu(x)</span><br><span class="line">        <span class="comment"># Reshape为4维: (batch, height, width, channels)，这里是 (batch, 7, 7, 128)</span></span><br><span class="line">        x = tf.reshape(x, shape=[<span class="number">-1</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">128</span>])</span><br><span class="line">        <span class="comment"># 反卷积层1</span></span><br><span class="line">        <span class="comment"># 卷积核大小5*5*128，64个，步长2（tf.layers.conv2d_transpose函数前几个参数为input，filters(输出feature map通道数)，kernel_size, strides，padding）</span></span><br><span class="line">        <span class="comment"># 输入x shape：(batch，7，7，128)， 输出image shape: (batch, 14, 14, 64)</span></span><br><span class="line">        x = tf.layers.conv2d_transpose(x, <span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">        <span class="comment"># batch normalization，在channel维度上做normalize</span></span><br><span class="line">        x = tf.layers.batch_normalization(x, training=is_training)</span><br><span class="line">        <span class="comment"># 激活函数 relu</span></span><br><span class="line">        x = tf.nn.relu(x)</span><br><span class="line">        <span class="comment"># 反卷积层2</span></span><br><span class="line">        <span class="comment"># 卷积核大小5*5*128，1个，步长2</span></span><br><span class="line">        <span class="comment"># 输入x shape：(batch，14，14，64)， 输出image shape: (batch, 28, 28, 1)</span></span><br><span class="line">        x = tf.layers.conv2d_transpose(x, <span class="number">1</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">        <span class="comment"># 激活函数 tanh</span></span><br><span class="line">        <span class="comment"># Apply tanh for better stability - clip values to [-1, 1].</span></span><br><span class="line">        x = tf.nn.tanh(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义判别器网络</span></span><br><span class="line"><span class="comment"># 输入：图像, 输出: 预测结果（Real/Fake Image）</span></span><br><span class="line"><span class="comment"># 同样训练时，才使用batch_normalization</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(x, reuse=False)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'Discriminator'</span>, reuse=reuse):</span><br><span class="line">        <span class="comment"># 卷积层1，输入x，卷积核大小5x5，64个，步长2</span></span><br><span class="line">        x = tf.layers.conv2d(x, <span class="number">64</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">        x = tf.layers.batch_normalization(x, training=is_training)</span><br><span class="line">        <span class="comment"># 激活函数 leakyrelu</span></span><br><span class="line">        x = leakyrelu(x)</span><br><span class="line">        <span class="comment"># 卷积层2，输入第一个卷积层的输出，卷积核大小5x5，128个，步长2</span></span><br><span class="line">        x = tf.layers.conv2d(x, <span class="number">128</span>, <span class="number">5</span>, strides=<span class="number">2</span>, padding=<span class="string">'same'</span>)</span><br><span class="line">        x = tf.layers.batch_normalization(x, training=is_training)</span><br><span class="line">        <span class="comment"># 激活函数 leakyrelu</span></span><br><span class="line">        x = leakyrelu(x)</span><br><span class="line">        <span class="comment"># 展平</span></span><br><span class="line">        x = tf.reshape(x, shape=[<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">128</span>])</span><br><span class="line">        <span class="comment"># 全连接层，含1024个神经元</span></span><br><span class="line">        x = tf.layers.dense(x, <span class="number">1024</span>)</span><br><span class="line">        x = tf.layers.batch_normalization(x, training=is_training)</span><br><span class="line">        <span class="comment"># 激活函数 leakyrelu</span></span><br><span class="line">        x = leakyrelu(x)</span><br><span class="line">        <span class="comment"># 输出2个类别: Real and Fake images</span></span><br><span class="line">        x = tf.layers.dense(x, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建生成器</span></span><br><span class="line">gen_sample = generator(noise_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建两个判别器（一个是真实图像输入，一个是生成图像）</span></span><br><span class="line">disc_real = discriminator(real_image_input)</span><br><span class="line">disc_fake = discriminator(gen_sample, reuse=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the stacked generator/discriminator</span></span><br><span class="line"><span class="comment"># 用于计算生成器的损失</span></span><br><span class="line">stacked_gan = discriminator(gen_sample, reuse=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数，交叉熵</span></span><br><span class="line"><span class="comment"># 真实图像，标签1</span></span><br><span class="line">disc_loss_real = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    logits=disc_real, labels=tf.ones([batch_size], dtype=tf.int32)))</span><br><span class="line"><span class="comment"># 生成图像，标签0</span></span><br><span class="line">disc_loss_fake = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    logits=disc_fake, labels=tf.zeros([batch_size], dtype=tf.int32)))</span><br><span class="line"><span class="comment"># 判别器损失函数是两者之和</span></span><br><span class="line">disc_loss = disc_loss_real + disc_loss_fake</span><br><span class="line"><span class="comment"># 生成器损失函数 (生成器试图骗过判别器，因此这里标签是1)</span></span><br><span class="line">gen_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    logits=stacked_gan, labels=tf.ones([batch_size], dtype=tf.int32)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建优化器（采用Adam方法）</span></span><br><span class="line">optimizer_gen = tf.train.AdamOptimizer(learning_rate=lr_generator, beta1=<span class="number">0.5</span>, beta2=<span class="number">0.999</span>)</span><br><span class="line">optimizer_disc = tf.train.AdamOptimizer(learning_rate=lr_discriminator, beta1=<span class="number">0.5</span>, beta2=<span class="number">0.999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training Variables for each optimizer</span></span><br><span class="line"><span class="comment"># By default in TensorFlow, all variables are updated by each optimizer, so we</span></span><br><span class="line"><span class="comment"># need to precise for each one of them the specific variables to update.</span></span><br><span class="line"><span class="comment"># 生成网络的变量</span></span><br><span class="line">gen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="string">'Generator'</span>) <span class="comment"># tf.get_collection：从一个结合中取出全部变量，是一个列表</span></span><br><span class="line"><span class="comment"># 判别器网络的变量</span></span><br><span class="line">disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="string">'Discriminator'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练操作</span></span><br><span class="line"><span class="comment"># TensorFlow UPDATE_OPS collection holds all batch norm operation to update the moving mean/stddev</span></span><br><span class="line">gen_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=<span class="string">'Generator'</span>)</span><br><span class="line"><span class="comment"># `control_dependencies` ensure that the `gen_update_ops` will be run before the `minimize` op (backprop)</span></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies(gen_update_ops):</span><br><span class="line">    train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)</span><br><span class="line">disc_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=<span class="string">'Discriminator'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies(disc_update_ops):</span><br><span class="line">    train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 变量全局初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start Training</span></span><br><span class="line"><span class="comment"># Start a new TF session</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the initializer</span></span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_steps+<span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prepare Input Data</span></span><br><span class="line">    <span class="comment"># Get the next batch of MNIST data (only images are needed, not labels)</span></span><br><span class="line">    batch_x, _ = mnist.train.next_batch(batch_size)</span><br><span class="line">    batch_x = np.reshape(batch_x, newshape=[<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># Rescale to [-1, 1], the input range of the discriminator</span></span><br><span class="line">    batch_x = batch_x * <span class="number">2.</span> - <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Discriminator Training</span></span><br><span class="line">    <span class="comment"># Generate noise to feed to the generator</span></span><br><span class="line">    z = np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[batch_size, noise_dim])</span><br><span class="line">    _, dl = sess.run([train_disc, disc_loss], feed_dict=&#123;real_image_input: batch_x, noise_input: z, is_training:<span class="keyword">True</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Generator Training</span></span><br><span class="line">    <span class="comment"># Generate noise to feed to the generator</span></span><br><span class="line">    z = np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[batch_size, noise_dim])</span><br><span class="line">    _, gl = sess.run([train_gen, gen_loss], feed_dict=&#123;noise_input: z, is_training:<span class="keyword">True</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">'Step %i: Generator Loss: %f, Discriminator Loss: %f'</span> % (i, gl, dl))</span><br></pre></td></tr></table></figure><hr><pre><code>Step 1: Generator Loss: 4.064141, Discriminator Loss: 1.679586Step 500: Generator Loss: 1.472707, Discriminator Loss: 0.974612Step 1000: Generator Loss: 1.918907, Discriminator Loss: 0.964812Step 1500: Generator Loss: 2.567637, Discriminator Loss: 0.717904Step 2000: Generator Loss: 2.398796, Discriminator Loss: 0.512406Step 2500: Generator Loss: 3.057401, Discriminator Loss: 1.235215Step 3000: Generator Loss: 2.620444, Discriminator Loss: 0.539795Step 3500: Generator Loss: 3.193395, Discriminator Loss: 0.265896Step 4000: Generator Loss: 5.071162, Discriminator Loss: 0.409445Step 4500: Generator Loss: 5.213869, Discriminator Loss: 0.203033Step 5000: Generator Loss: 6.087250, Discriminator Loss: 0.350634Step 5500: Generator Loss: 5.467363, Discriminator Loss: 0.424895Step 6000: Generator Loss: 4.910432, Discriminator Loss: 0.196554Step 6500: Generator Loss: 3.230242, Discriminator Loss: 0.268745Step 7000: Generator Loss: 4.777361, Discriminator Loss: 0.676658Step 7500: Generator Loss: 4.165446, Discriminator Loss: 0.150221Step 8000: Generator Loss: 5.681596, Discriminator Loss: 0.108955Step 8500: Generator Loss: 6.023059, Discriminator Loss: 0.114312Step 9000: Generator Loss: 4.660669, Discriminator Loss: 0.182506Step 9500: Generator Loss: 4.492438, Discriminator Loss: 0.411817Step 10000: Generator Loss: 5.906080, Discriminator Loss: 0.088082</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Testing</span></span><br><span class="line"><span class="comment"># Generate images from noise, using the generator network.</span></span><br><span class="line">n = <span class="number">6</span></span><br><span class="line">canvas = np.empty((<span class="number">28</span> * n, <span class="number">28</span> * n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    <span class="comment"># Noise input.</span></span><br><span class="line">    z = np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[n, noise_dim])</span><br><span class="line">    <span class="comment"># Generate image from noise.</span></span><br><span class="line">    g = sess.run(gen_sample, feed_dict=&#123;noise_input: z, is_training:<span class="keyword">False</span>&#125;)</span><br><span class="line">    <span class="comment"># Rescale values to the original [0, 1] (from tanh -&gt; [-1, 1])</span></span><br><span class="line">    g = (g + <span class="number">1.</span>) / <span class="number">2.</span></span><br><span class="line">    <span class="comment"># Reverse colours for better display</span></span><br><span class="line">    g = <span class="number">-1</span> * (g - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># Draw the generated digits</span></span><br><span class="line">        canvas[i * <span class="number">28</span>:(i + <span class="number">1</span>) * <span class="number">28</span>, j * <span class="number">28</span>:(j + <span class="number">1</span>) * <span class="number">28</span>] = g[j].reshape([<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n, n))</span><br><span class="line">plt.imshow(canvas, origin=<span class="string">"upper"</span>, cmap=<span class="string">"gray"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/c614ca83379dc0e98f9d49ff17e80894.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/u013146742/article/details/51986575" target="_blank" rel="noopener">RELU 激活函数及其他相关的函数</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;DCGAN简介&quot;&gt;&lt;a href=&quot;#DCGAN简介&quot; class=&quot;headerlink&quot; title=&quot;DCGAN简介&quot;&gt;&lt;/a&gt;DCGAN简介&lt;/h3&gt;&lt;p&gt;DCGAN在GAN的基础上优化了网络结构，加入了 &lt;code&gt;conv&lt;/code&gt;，&lt;code&gt;batch_norm&lt;/code&gt; 等层，使得网络更容易训练，网络结构如下：&lt;br&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/67d781d61c77b7d1985f57ecc932a1e3.png&quot; alt=&quot;DCGAN网络结构图&quot;&gt;&lt;br&gt;注意：本图只是示例，与下面实际网络参数不对应。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow神经网络之GAN</title>
    <link href="http://xywang93.github.io.git/2018/03/14/DeepLearning/TensorFlow/14Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BGAN/"/>
    <id>http://xywang93.github.io.git/2018/03/14/DeepLearning/TensorFlow/14Tensorflow神经网络之GAN/</id>
    <published>2018-03-13T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:38.425Z</updated>
    
    <content type="html"><![CDATA[<h3 id="生成对抗网络简介"><a href="#生成对抗网络简介" class="headerlink" title="生成对抗网络简介"></a>生成对抗网络简介</h3><p>生成对抗网络（<code>GAN</code>）启发自博弈论中的二人零和博弈（two-player game），<strong>类似于周伯通的绝学——“左右互搏”</strong>。<code>GAN</code> 模型中的两位博弈方分别由生成式模型（<code>generative model</code>）和判别式模型（<code>discriminative model</code>）充当。生成模型 <code>G</code> 捕捉样本数据的分布，用服从某一分布（均匀分布，高斯分布等）的噪声 <code>z</code> 生成一个类似真实训练数据的样本，追求效果是越像真实样本越好；判别模型 <code>D</code> 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，<code>D</code> 输出大概率，否则，<code>D</code> 输出小概率。可以做如下类比：生成网络 <code>G</code> 好比假币制造团伙，专门制造假币，判别网络 <code>D</code> 好比警察，专门检测使用的货币是真币还是假币，<code>G</code> 的目标是想方设法生成和真币一样的货币，使得 <code>D</code> 判别不出来，<code>D</code> 的目标是想方设法检测出来 <code>G</code> 生成的假币。随着训练时间的增加，判别模型与生成模型的能力都相应的提升！</p><p>具体生成网络的示意图如下所示：<br><img src="http://xukeqiniu.xukeai.cn/bbafdb123a7be9bb43f06bdee819dd86.png" alt="生成对抗网络结构示意图"></p><a id="more"></a><h3 id="Tensorflow生成对抗网络实现"><a href="#Tensorflow生成对抗网络实现" class="headerlink" title="Tensorflow生成对抗网络实现"></a>Tensorflow生成对抗网络实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, absolute_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入mnist数据集</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training Params</span></span><br><span class="line">num_steps = <span class="number">70000</span> <span class="comment">#总迭代次数</span></span><br><span class="line">batch_size = <span class="number">128</span>  <span class="comment"># 批量大小</span></span><br><span class="line">learning_rate = <span class="number">0.0002</span> <span class="comment">#学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Params</span></span><br><span class="line">image_dim = <span class="number">784</span> <span class="comment"># 28*28 pixels，生成器的输出层节点数，也是判别器的输入</span></span><br><span class="line">gen_hidden_dim = <span class="number">256</span> <span class="comment"># 生成器隐藏层节点数</span></span><br><span class="line">disc_hidden_dim = <span class="number">256</span> <span class="comment"># 判别器隐藏层节点数</span></span><br><span class="line">noise_dim = <span class="number">100</span> <span class="comment"># Noise data points 生成器输入节点数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Xavier 初始化方式（更适合有ReLU的网络训练）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glorot_init</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.random_normal(shape=shape, stddev=<span class="number">1.</span> / tf.sqrt(shape[<span class="number">0</span>] / <span class="number">2.</span>))</span><br></pre></td></tr></table></figure><p>Xavier 初始化方式方差：</p><p><img src="http://xukeqiniu.xukeai.cn/e6d9383d4eb0fac2f7d1183a592deceb.png" alt=""></p><p>这里的参数是标准差。</p><h4 id="设置每一层的权重与偏置"><a href="#设置每一层的权重与偏置" class="headerlink" title="设置每一层的权重与偏置"></a>设置每一层的权重与偏置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置每一层的权重（Xavier初始化）与偏置（初始化为零）</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'gen_hidden1'</span>: tf.Variable(glorot_init([noise_dim, gen_hidden_dim])),<span class="comment">#（100 - 256）</span></span><br><span class="line">    <span class="string">'gen_out'</span>: tf.Variable(glorot_init([gen_hidden_dim, image_dim])), <span class="comment">#（256 - 784）</span></span><br><span class="line">    <span class="string">'disc_hidden1'</span>: tf.Variable(glorot_init([image_dim, disc_hidden_dim])),<span class="comment">#（784 - 256）</span></span><br><span class="line">    <span class="string">'disc_out'</span>: tf.Variable(glorot_init([disc_hidden_dim, <span class="number">1</span>])),<span class="comment">#（256 - 1）</span></span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'gen_hidden1'</span>: tf.Variable(tf.zeros([gen_hidden_dim])),</span><br><span class="line">    <span class="string">'gen_out'</span>: tf.Variable(tf.zeros([image_dim])),</span><br><span class="line">    <span class="string">'disc_hidden1'</span>: tf.Variable(tf.zeros([disc_hidden_dim])),</span><br><span class="line">    <span class="string">'disc_out'</span>: tf.Variable(tf.zeros([<span class="number">1</span>])),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="定义生成对抗网络"><a href="#定义生成对抗网络" class="headerlink" title="定义生成对抗网络"></a>定义生成对抗网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义生成器函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 输入x是1x100的矩阵，weights['gen_hidden1']是100x256的矩阵，矩阵相乘结果是1x256的矩阵，生成器隐藏层含256个节点</span></span><br><span class="line">    hidden_layer = tf.matmul(x, weights[<span class="string">'gen_hidden1'</span>])</span><br><span class="line">    <span class="comment"># biases['gen_hidden1']是1x256的矩阵，生成器隐藏层含256个节点</span></span><br><span class="line">    hidden_layer = tf.add(hidden_layer, biases[<span class="string">'gen_hidden1'</span>])</span><br><span class="line">    <span class="comment"># 激活函数 relu</span></span><br><span class="line">    hidden_layer = tf.nn.relu(hidden_layer)</span><br><span class="line">    <span class="comment"># hidden_layer是1x256的矩阵，weights['gen_out']是256x784的矩阵，矩阵相乘结果是1x784的矩阵，生成器输出层含784个节点</span></span><br><span class="line">    out_layer = tf.matmul(hidden_layer, weights[<span class="string">'gen_out'</span>])</span><br><span class="line">    <span class="comment"># biases['gen_out']是1x784的矩阵，生成器输出层含784个节点</span></span><br><span class="line">    out_layer = tf.add(out_layer, biases[<span class="string">'gen_out'</span>])</span><br><span class="line">    <span class="comment"># 激活函数 sigmoid</span></span><br><span class="line">    out_layer = tf.nn.sigmoid(out_layer)</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义判别器函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># 输入x是生成器生成的1x784的矩阵（生成的图片），weights['disc_hidden1']是784x256的矩阵，矩阵相乘结果是1x256的矩阵，判别器隐藏层含256个节点</span></span><br><span class="line">    hidden_layer = tf.matmul(x, weights[<span class="string">'disc_hidden1'</span>])</span><br><span class="line">    <span class="comment"># biases['disc_hidden1']是1x256的矩阵，生成器隐藏层含256个节点</span></span><br><span class="line">    hidden_layer = tf.add(hidden_layer, biases[<span class="string">'disc_hidden1'</span>])</span><br><span class="line">    <span class="comment"># 激活函数 relu</span></span><br><span class="line">    hidden_layer = tf.nn.relu(hidden_layer)</span><br><span class="line">    <span class="comment"># hidden_layer是1x256的矩阵，weights['disc_out']是256x1的矩阵，矩阵相乘结果是一个数，判别器输出层含1个节点</span></span><br><span class="line">    out_layer = tf.matmul(hidden_layer, weights[<span class="string">'disc_out'</span>])</span><br><span class="line">    <span class="comment"># biases['disc_out']是一个数，判别器输出层含1个节点</span></span><br><span class="line">    out_layer = tf.add(out_layer, biases[<span class="string">'disc_out'</span>])</span><br><span class="line">    <span class="comment"># 激活函数 sigmoid</span></span><br><span class="line">    out_layer = tf.nn.sigmoid(out_layer)</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建网络</span></span><br><span class="line"><span class="comment"># 网络输入</span></span><br><span class="line">gen_input = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, noise_dim], name=<span class="string">'input_noise'</span>) <span class="comment"># 生成器 输入噪点 batch*100，none是一个空值，后面赋值batch_size</span></span><br><span class="line">disc_input = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, image_dim], name=<span class="string">'disc_input'</span>) <span class="comment"># 判别器 输入真实图像 batch*784</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建生成器（generator）</span></span><br><span class="line">gen_sample = generator(gen_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建两个判别器（一个是真实图像输入，一个是生成图像）</span></span><br><span class="line">disc_real = discriminator(disc_input) <span class="comment"># 真实图像</span></span><br><span class="line">disc_fake = discriminator(gen_sample) <span class="comment"># 通过生成器生成的图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line"><span class="comment"># 关于GAN的理论推导，可参见 [^1]</span></span><br><span class="line">gen_loss = -tf.reduce_mean(tf.log(disc_fake)) <span class="comment"># 生成器损失函数</span></span><br><span class="line">disc_loss = -tf.reduce_mean(tf.log(disc_real) + tf.log(<span class="number">1.</span> - disc_fake)) <span class="comment"># 判别器损失函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建优化器（采用Adam方法），可参见 [^2]</span></span><br><span class="line">optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training Variables for each optimizer</span></span><br><span class="line"><span class="comment"># By default in TensorFlow, all variables are updated by each optimizer, so we</span></span><br><span class="line"><span class="comment"># need to precise for each one of them the specific variables to update.</span></span><br><span class="line"><span class="comment"># 生成网络的变量</span></span><br><span class="line">gen_vars = [weights[<span class="string">'gen_hidden1'</span>], weights[<span class="string">'gen_out'</span>],</span><br><span class="line">            biases[<span class="string">'gen_hidden1'</span>], biases[<span class="string">'gen_out'</span>]]</span><br><span class="line"><span class="comment"># 判别网络的变量</span></span><br><span class="line">disc_vars = [weights[<span class="string">'disc_hidden1'</span>], weights[<span class="string">'disc_out'</span>],</span><br><span class="line">            biases[<span class="string">'disc_hidden1'</span>], biases[<span class="string">'disc_out'</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练操作</span></span><br><span class="line">train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)</span><br><span class="line">train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 变量全局初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><p>GAN的网络结构类似于多层感知机：</p><p><img src="http://xukeqiniu.xukeai.cn/0e39fd772bfa170145f493cacadab129.png" alt=""></p><h4 id="训练生成对抗网络"><a href="#训练生成对抗网络" class="headerlink" title="训练生成对抗网络"></a>训练生成对抗网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="comment"># 创建一个会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_steps+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># 准备数据</span></span><br><span class="line">    <span class="comment"># 拿到下一批次的 MNIST 数据 (仅需要图像, 不需要标签)</span></span><br><span class="line">    batch_x, _ = mnist.train.next_batch(batch_size) <span class="comment"># 判别器输入 真实图像，batch_*784</span></span><br><span class="line">    <span class="comment"># 给生成器生成噪点数据</span></span><br><span class="line">    z = np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[batch_size, noise_dim]) <span class="comment"># 生成器输入 噪声，batch*100</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    feed_dict = &#123;disc_input: batch_x, gen_input: z&#125; <span class="comment">#给placeholder填入值</span></span><br><span class="line">    _, _, gl, dl = sess.run([train_gen, train_disc, gen_loss, disc_loss],</span><br><span class="line">                            feed_dict=feed_dict)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">'Step %i: Generator Loss: %f, Discriminator Loss: %f'</span> % (i, gl, dl))</span><br></pre></td></tr></table></figure><hr><pre><code>Step 1: Generator Loss: 0.223592, Discriminator Loss: 2.090910Step 2000: Generator Loss: 4.678916, Discriminator Loss: 0.041115Step 4000: Generator Loss: 3.605874, Discriminator Loss: 0.068698Step 6000: Generator Loss: 3.845584, Discriminator Loss: 0.190420Step 8000: Generator Loss: 4.470613, Discriminator Loss: 0.117488Step 10000: Generator Loss: 3.813103, Discriminator Loss: 0.146255Step 12000: Generator Loss: 2.991248, Discriminator Loss: 0.392258Step 14000: Generator Loss: 3.769275, Discriminator Loss: 0.153639Step 16000: Generator Loss: 4.366917, Discriminator Loss: 0.206618Step 18000: Generator Loss: 4.052875, Discriminator Loss: 0.225112Step 20000: Generator Loss: 3.574747, Discriminator Loss: 0.362798Step 22000: Generator Loss: 3.760236, Discriminator Loss: 0.188211Step 24000: Generator Loss: 3.055995, Discriminator Loss: 0.354645Step 26000: Generator Loss: 3.619049, Discriminator Loss: 0.211489Step 28000: Generator Loss: 3.523777, Discriminator Loss: 0.273607Step 30000: Generator Loss: 3.889854, Discriminator Loss: 0.286803Step 32000: Generator Loss: 3.106094, Discriminator Loss: 0.298111Step 34000: Generator Loss: 3.548391, Discriminator Loss: 0.343262Step 36000: Generator Loss: 3.081174, Discriminator Loss: 0.332788Step 38000: Generator Loss: 2.946176, Discriminator Loss: 0.335102Step 40000: Generator Loss: 3.078653, Discriminator Loss: 0.465524Step 42000: Generator Loss: 2.601799, Discriminator Loss: 0.409574Step 44000: Generator Loss: 3.168177, Discriminator Loss: 0.325075Step 46000: Generator Loss: 2.601811, Discriminator Loss: 0.428143Step 48000: Generator Loss: 2.853810, Discriminator Loss: 0.403768Step 50000: Generator Loss: 2.690175, Discriminator Loss: 0.483180Step 52000: Generator Loss: 3.278867, Discriminator Loss: 0.375016Step 54000: Generator Loss: 2.869437, Discriminator Loss: 0.477840Step 56000: Generator Loss: 2.561056, Discriminator Loss: 0.449300Step 58000: Generator Loss: 2.814199, Discriminator Loss: 0.484522Step 60000: Generator Loss: 2.469474, Discriminator Loss: 0.428359Step 62000: Generator Loss: 2.721684, Discriminator Loss: 0.494090Step 64000: Generator Loss: 2.491284, Discriminator Loss: 0.654795Step 66000: Generator Loss: 2.725388, Discriminator Loss: 0.423149Step 68000: Generator Loss: 2.758215, Discriminator Loss: 0.513224Step 70000: Generator Loss: 3.072056, Discriminator Loss: 0.481437</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="comment"># 通过训练出的生成网络输入噪点，生成图像</span></span><br><span class="line">n = <span class="number">6</span></span><br><span class="line">canvas = np.empty((<span class="number">28</span> * n, <span class="number">28</span> * n))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    <span class="comment"># 噪点输入</span></span><br><span class="line">    z = np.random.uniform(<span class="number">-1.</span>, <span class="number">1.</span>, size=[n, noise_dim])</span><br><span class="line">    <span class="comment"># 生成图像</span></span><br><span class="line">    g = sess.run(gen_sample, feed_dict=&#123;gen_input: z&#125;)</span><br><span class="line">    <span class="comment"># 颜色反转便于显示</span></span><br><span class="line">    g = <span class="number">-1</span> * (g - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># 绘制生成的手写体数字</span></span><br><span class="line">        canvas[i * <span class="number">28</span>:(i + <span class="number">1</span>) * <span class="number">28</span>, j * <span class="number">28</span>:(j + <span class="number">1</span>) * <span class="number">28</span>] = g[j].reshape([<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(n, n))</span><br><span class="line">plt.imshow(canvas, origin=<span class="string">"upper"</span>, cmap=<span class="string">"gray"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://xukeqiniu.xukeai.cn/853bf244485839cf44f495ab5afc1f91.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://www.jiqizhixin.com/articles/2017-10-1-1" target="_blank" rel="noopener">机器之心GitHub项目：GAN完整理论推导与实现，Perfect！</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）</a></p><p>[3] <a href="https://www.cnblogs.com/Charles-Wan/p/6238033.html" target="_blank" rel="noopener">不要怂，就是GAN (生成式对抗网络) （一）： GAN 简介</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;生成对抗网络简介&quot;&gt;&lt;a href=&quot;#生成对抗网络简介&quot; class=&quot;headerlink&quot; title=&quot;生成对抗网络简介&quot;&gt;&lt;/a&gt;生成对抗网络简介&lt;/h3&gt;&lt;p&gt;生成对抗网络（&lt;code&gt;GAN&lt;/code&gt;）启发自博弈论中的二人零和博弈（two-player game），&lt;strong&gt;类似于周伯通的绝学——“左右互搏”&lt;/strong&gt;。&lt;code&gt;GAN&lt;/code&gt; 模型中的两位博弈方分别由生成式模型（&lt;code&gt;generative model&lt;/code&gt;）和判别式模型（&lt;code&gt;discriminative model&lt;/code&gt;）充当。生成模型 &lt;code&gt;G&lt;/code&gt; 捕捉样本数据的分布，用服从某一分布（均匀分布，高斯分布等）的噪声 &lt;code&gt;z&lt;/code&gt; 生成一个类似真实训练数据的样本，追求效果是越像真实样本越好；判别模型 &lt;code&gt;D&lt;/code&gt; 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，&lt;code&gt;D&lt;/code&gt; 输出大概率，否则，&lt;code&gt;D&lt;/code&gt; 输出小概率。可以做如下类比：生成网络 &lt;code&gt;G&lt;/code&gt; 好比假币制造团伙，专门制造假币，判别网络 &lt;code&gt;D&lt;/code&gt; 好比警察，专门检测使用的货币是真币还是假币，&lt;code&gt;G&lt;/code&gt; 的目标是想方设法生成和真币一样的货币，使得 &lt;code&gt;D&lt;/code&gt; 判别不出来，&lt;code&gt;D&lt;/code&gt; 的目标是想方设法检测出来 &lt;code&gt;G&lt;/code&gt; 生成的假币。随着训练时间的增加，判别模型与生成模型的能力都相应的提升！&lt;/p&gt;
&lt;p&gt;具体生成网络的示意图如下所示：&lt;br&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/bbafdb123a7be9bb43f06bdee819dd86.png&quot; alt=&quot;生成对抗网络结构示意图&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow神经网络之LSTM</title>
    <link href="http://xywang93.github.io.git/2018/03/13/DeepLearning/TensorFlow/13Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BLSTM/"/>
    <id>http://xywang93.github.io.git/2018/03/13/DeepLearning/TensorFlow/13Tensorflow神经网络之LSTM/</id>
    <published>2018-03-12T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:38.421Z</updated>
    
    <content type="html"><![CDATA[<h3 id="LSTM-简介"><a href="#LSTM-简介" class="headerlink" title="LSTM 简介"></a>LSTM 简介</h3><h4 id="公式-LSTM"><a href="#公式-LSTM" class="headerlink" title="公式 LSTM"></a>公式 LSTM</h4><p><code>LSTM</code>作为门控循环神经网络因此我们从门控单元切入理解。主要包括：</p><ul><li>输入门：It</li><li>遗忘门：Ft</li><li>输出门：Ot</li><li>候选细胞：~Ct</li><li>细胞：Ct</li><li>隐含状态：Ht</li></ul><p>假设隐含状态长度为h，数据Xt是一个样本数为n、特征向量维度为x的批量数据，其计算如下所示（W和b表示权重和偏置）：</p><p><img src="http://xukeqiniu.xukeai.cn/44981d27690d8e9d4c4878f34409dfe0.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/bd180c0fdffc0557b48e84d99b28fe56.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/a57ff99931e6bd62068d381e593a14a7.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/6a0256f97e28c3b099a63391cc97aae7.png" alt=""></p><p>最后的输出其实只有两个，一个是输出，一个是状态，输出就是Ht，而状态为(Ct,Ht)，其他都是中间计算过程。[^2]</p><p><img src="http://xukeqiniu.xukeai.cn/2244cf361736f4ddd46ccbcec6baa459.png" alt=""></p><a id="more"></a><h4 id="图示-LSTM"><a href="#图示-LSTM" class="headerlink" title="图示 LSTM"></a>图示 LSTM</h4><p><img src="http://xukeqiniu.xukeai.cn/2572534e9bc1dff727d3485cf8f958e4.png" alt=""><br><img src="http://xukeqiniu.xukeai.cn/83b5c59a84db028eddf567d68b5344bb.png" alt=""></p><ul><li>遗忘门</li></ul><p><img src="http://xukeqiniu.xukeai.cn/36ae3aec84e5e94a0a30a7534840af22.png" alt=""></p><ul><li>输入门</li></ul><p><img src="http://xukeqiniu.xukeai.cn/1d3b7ab425df98f2f564bb951c9c2878.png" alt=""></p><ul><li>当前状态</li></ul><p><img src="http://xukeqiniu.xukeai.cn/d9a47f577060d86970701838d18779e7.png" alt=""></p><ul><li>输出层</li></ul><p><img src="http://xukeqiniu.xukeai.cn/199537faf0d0cbbd0771a4e9013d180a.png" alt=""></p><h4 id="Tensorflow-LSTM"><a href="#Tensorflow-LSTM" class="headerlink" title="Tensorflow LSTM"></a>Tensorflow LSTM</h4><p>tensorflow 提供了LSTM 实现的一个 basic 版本，不包含 LSTM 的一些高级扩展，同时也提供了一个标准接口，其中包含了 LSTM 的扩展。分别为：tf.nn.rnn_cell.BasicLSTMCell()，tf.nn.rnn_cell.LSTMCell()，我们这里实现一个基本版本。[^1]</p><h3 id="Tensorflow-实现-LSTM"><a href="#Tensorflow-实现-LSTM" class="headerlink" title="Tensorflow 实现 LSTM"></a>Tensorflow 实现 LSTM</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> rnn</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练参数</span></span><br><span class="line">learning_rate = <span class="number">0.001</span> <span class="comment"># 学习率</span></span><br><span class="line">training_steps = <span class="number">10000</span> <span class="comment"># 总迭代次数</span></span><br><span class="line">batch_size = <span class="number">128</span> <span class="comment"># 批量大小</span></span><br><span class="line">display_step = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络参数</span></span><br><span class="line">num_input = <span class="number">28</span> <span class="comment"># MNIST数据集图片: 28*28</span></span><br><span class="line">timesteps = <span class="number">28</span> <span class="comment"># timesteps</span></span><br><span class="line">num_hidden = <span class="number">128</span> <span class="comment"># 隐藏层神经元数</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># MNIST 数据集类别数 (0-9 digits)</span></span><br></pre></td></tr></table></figure><h4 id="构建-LSTM-网络"><a href="#构建-LSTM-网络" class="headerlink" title="构建 LSTM 网络"></a>构建 LSTM 网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义输入</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, timesteps, num_input])</span><br><span class="line">Y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, num_classes])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义权重和偏置</span></span><br><span class="line"><span class="comment"># weights矩阵[128, 10]</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([num_hidden, num_classes]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([num_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义LSTM网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LSTM</span><span class="params">(x, weights, biases)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prepare data shape to match `rnn` function requirements</span></span><br><span class="line">    <span class="comment"># 输入数据x的shape: (batch_size, timesteps, n_input)</span></span><br><span class="line">    <span class="comment"># 需要的shape: 按 timesteps 切片，得到 timesteps 个 (batch_size, n_input)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对x进行切分</span></span><br><span class="line">    <span class="comment"># tf.unstack(value,num=None,axis=0,name='unstack')</span></span><br><span class="line">    <span class="comment"># value：要进行分割的tensor</span></span><br><span class="line">    <span class="comment"># axis：整数，打算进行切分的维度</span></span><br><span class="line">    <span class="comment"># num：整数，axis（打算切分）维度的长度</span></span><br><span class="line">    x = tf.unstack(x, timesteps, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个lstm cell，即上面图示LSTM中的A</span></span><br><span class="line">    <span class="comment"># n_hidden表示神经元的个数，forget_bias就是LSTM们的忘记系数，如果等于1，就是不会忘记任何信息。如果等于0，就都忘记。</span></span><br><span class="line">    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到 lstm cell 输出</span></span><br><span class="line">    <span class="comment"># 输出output和states</span></span><br><span class="line">    <span class="comment"># outputs是一个长度为T的列表，通过outputs[-1]取出最后的输出</span></span><br><span class="line">    <span class="comment"># state是最后的状态</span></span><br><span class="line">    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 线性激活</span></span><br><span class="line">    <span class="comment"># 矩阵乘法</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(outputs[<span class="number">-1</span>], weights[<span class="string">'out'</span>]) + biases[<span class="string">'out'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">logits = LSTM(X, weights, biases)</span><br><span class="line">prediction = tf.nn.softmax(logits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">    logits=logits, labels=Y))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">train_op = optimizer.minimize(loss_op)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评估(with test logits, for dropout to be disabled)</span></span><br><span class="line">correct_pred = tf.equal(tf.argmax(prediction, <span class="number">1</span>), tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化全局变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h4 id="训练-测试"><a href="#训练-测试" class="headerlink" title="训练+测试"></a>训练+测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run the initializer</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, training_steps+<span class="number">1</span>):</span><br><span class="line">        batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">        <span class="comment"># Reshape data to get 28 seq of 28 elements</span></span><br><span class="line">        batch_x = batch_x.reshape((batch_size, timesteps, num_input))</span><br><span class="line">        <span class="comment"># Run optimization op (backprop)</span></span><br><span class="line">        sess.run(train_op, feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">        <span class="keyword">if</span> step % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># Calculate batch loss and accuracy</span></span><br><span class="line">            loss, acc = sess.run([loss_op, accuracy], feed_dict=&#123;X: batch_x,</span><br><span class="line">                                                                 Y: batch_y&#125;)</span><br><span class="line">            print(<span class="string">"Step "</span> + str(step) + <span class="string">", Minibatch Loss= "</span> + \</span><br><span class="line">                  <span class="string">"&#123;:.4f&#125;"</span>.format(loss) + <span class="string">", Training Accuracy= "</span> + \</span><br><span class="line">                  <span class="string">"&#123;:.3f&#125;"</span>.format(acc))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate accuracy for 128 mnist test images</span></span><br><span class="line">    test_len = <span class="number">128</span></span><br><span class="line">    test_data = mnist.test.images[:test_len].reshape((<span class="number">-1</span>, timesteps, num_input))</span><br><span class="line">    test_label = mnist.test.labels[:test_len]</span><br><span class="line">    print(<span class="string">"Testing Accuracy:"</span>, \</span><br><span class="line">        sess.run(accuracy, feed_dict=&#123;X: test_data, Y: test_label&#125;))</span><br></pre></td></tr></table></figure><hr><pre><code>Step 1, Minibatch Loss= 2.8645, Training Accuracy= 0.062Step 200, Minibatch Loss= 2.1180, Training Accuracy= 0.227Step 400, Minibatch Loss= 1.9726, Training Accuracy= 0.344Step 600, Minibatch Loss= 1.7784, Training Accuracy= 0.445Step 800, Minibatch Loss= 1.5500, Training Accuracy= 0.547Step 1000, Minibatch Loss= 1.5882, Training Accuracy= 0.453Step 1200, Minibatch Loss= 1.5326, Training Accuracy= 0.555Step 1400, Minibatch Loss= 1.3682, Training Accuracy= 0.570Step 1600, Minibatch Loss= 1.3374, Training Accuracy= 0.594Step 1800, Minibatch Loss= 1.1551, Training Accuracy= 0.648Step 2000, Minibatch Loss= 1.2116, Training Accuracy= 0.633Step 2200, Minibatch Loss= 1.1292, Training Accuracy= 0.609Step 2400, Minibatch Loss= 1.0862, Training Accuracy= 0.680Step 2600, Minibatch Loss= 1.0501, Training Accuracy= 0.672Step 2800, Minibatch Loss= 1.0487, Training Accuracy= 0.688Step 3000, Minibatch Loss= 1.0223, Training Accuracy= 0.727Step 3200, Minibatch Loss= 1.0418, Training Accuracy= 0.695Step 3400, Minibatch Loss= 0.8273, Training Accuracy= 0.719Step 3600, Minibatch Loss= 0.9088, Training Accuracy= 0.727Step 3800, Minibatch Loss= 0.9243, Training Accuracy= 0.750Step 4000, Minibatch Loss= 0.8085, Training Accuracy= 0.703Step 4200, Minibatch Loss= 0.8466, Training Accuracy= 0.711Step 4400, Minibatch Loss= 0.8973, Training Accuracy= 0.734Step 4600, Minibatch Loss= 0.7647, Training Accuracy= 0.750Step 4800, Minibatch Loss= 0.9088, Training Accuracy= 0.742Step 5000, Minibatch Loss= 0.7906, Training Accuracy= 0.742Step 5200, Minibatch Loss= 0.7275, Training Accuracy= 0.781Step 5400, Minibatch Loss= 0.7488, Training Accuracy= 0.789Step 5600, Minibatch Loss= 0.7517, Training Accuracy= 0.758Step 5800, Minibatch Loss= 0.7778, Training Accuracy= 0.797Step 6000, Minibatch Loss= 0.6736, Training Accuracy= 0.742Step 6200, Minibatch Loss= 0.6552, Training Accuracy= 0.773Step 6400, Minibatch Loss= 0.5746, Training Accuracy= 0.828Step 6600, Minibatch Loss= 0.8102, Training Accuracy= 0.727Step 6800, Minibatch Loss= 0.6669, Training Accuracy= 0.773Step 7000, Minibatch Loss= 0.6524, Training Accuracy= 0.766Step 7200, Minibatch Loss= 0.6481, Training Accuracy= 0.805Step 7400, Minibatch Loss= 0.5743, Training Accuracy= 0.828Step 7600, Minibatch Loss= 0.6983, Training Accuracy= 0.773Step 7800, Minibatch Loss= 0.5552, Training Accuracy= 0.828Step 8000, Minibatch Loss= 0.5728, Training Accuracy= 0.820Step 8200, Minibatch Loss= 0.5587, Training Accuracy= 0.789Step 8400, Minibatch Loss= 0.5205, Training Accuracy= 0.836Step 8600, Minibatch Loss= 0.4266, Training Accuracy= 0.906Step 8800, Minibatch Loss= 0.7197, Training Accuracy= 0.812Step 9000, Minibatch Loss= 0.4216, Training Accuracy= 0.852Step 9200, Minibatch Loss= 0.4448, Training Accuracy= 0.844Step 9400, Minibatch Loss= 0.3577, Training Accuracy= 0.891Step 9600, Minibatch Loss= 0.4034, Training Accuracy= 0.883Step 9800, Minibatch Loss= 0.4747, Training Accuracy= 0.828Step 10000, Minibatch Loss= 0.5763, Training Accuracy= 0.805Optimization Finished!Testing Accuracy: 0.875</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/u012436149/article/details/52887091" target="_blank" rel="noopener">tensorflow学习笔记（六）：LSTM 与 GRU</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/28919765" target="_blank" rel="noopener">学会区分 RNN 的 output 和 state</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;LSTM-简介&quot;&gt;&lt;a href=&quot;#LSTM-简介&quot; class=&quot;headerlink&quot; title=&quot;LSTM 简介&quot;&gt;&lt;/a&gt;LSTM 简介&lt;/h3&gt;&lt;h4 id=&quot;公式-LSTM&quot;&gt;&lt;a href=&quot;#公式-LSTM&quot; class=&quot;headerlink&quot; title=&quot;公式 LSTM&quot;&gt;&lt;/a&gt;公式 LSTM&lt;/h4&gt;&lt;p&gt;&lt;code&gt;LSTM&lt;/code&gt;作为门控循环神经网络因此我们从门控单元切入理解。主要包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入门：It&lt;/li&gt;
&lt;li&gt;遗忘门：Ft&lt;/li&gt;
&lt;li&gt;输出门：Ot&lt;/li&gt;
&lt;li&gt;候选细胞：~Ct&lt;/li&gt;
&lt;li&gt;细胞：Ct&lt;/li&gt;
&lt;li&gt;隐含状态：Ht&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设隐含状态长度为h，数据Xt是一个样本数为n、特征向量维度为x的批量数据，其计算如下所示（W和b表示权重和偏置）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/44981d27690d8e9d4c4878f34409dfe0.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/bd180c0fdffc0557b48e84d99b28fe56.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/a57ff99931e6bd62068d381e593a14a7.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/6a0256f97e28c3b099a63391cc97aae7.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后的输出其实只有两个，一个是输出，一个是状态，输出就是Ht，而状态为(Ct,Ht)，其他都是中间计算过程。[^2]&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/2244cf361736f4ddd46ccbcec6baa459.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>TensorBoard高级篇</title>
    <link href="http://xywang93.github.io.git/2018/03/12/DeepLearning/TensorFlow/12TensorBoard%E9%AB%98%E7%BA%A7%E7%AF%87/"/>
    <id>http://xywang93.github.io.git/2018/03/12/DeepLearning/TensorFlow/12TensorBoard高级篇/</id>
    <published>2018-03-11T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:24.394Z</updated>
    
    <content type="html"><![CDATA[<h3 id="多层感知机的Tensorboard可视化"><a href="#多层感知机的Tensorboard可视化" class="headerlink" title="多层感知机的Tensorboard可视化"></a>多层感知机的Tensorboard可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><a id="more"></a><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line">logs_path = <span class="string">'./log/example/'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span> <span class="comment"># 1st layer number of features</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span> <span class="comment"># 2nd layer number of features</span></span><br><span class="line">n_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">n_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line"><span class="comment"># mnist data image of shape 28*28=784</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">'InputData'</span>)</span><br><span class="line"><span class="comment"># 0-9 digits recognition =&gt; 10 classes</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>], name=<span class="string">'LabelData'</span>)</span><br></pre></td></tr></table></figure><h4 id="创建多层感知机函数"><a href="#创建多层感知机函数" class="headerlink" title="创建多层感知机函数"></a>创建多层感知机函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multilayer_perceptron</span><span class="params">(x, weights, biases)</span>:</span></span><br><span class="line">    <span class="comment"># Hidden layer with RELU activation</span></span><br><span class="line">    layer_1 = tf.add(tf.matmul(x, weights[<span class="string">'w1'</span>]), biases[<span class="string">'b1'</span>])</span><br><span class="line">    layer_1 = tf.nn.relu(layer_1)</span><br><span class="line">    <span class="comment"># Create a summary to visualize the first layer ReLU activation</span></span><br><span class="line">    tf.summary.histogram(<span class="string">"relu1"</span>, layer_1)</span><br><span class="line">    <span class="comment"># Hidden layer with RELU activation</span></span><br><span class="line">    layer_2 = tf.add(tf.matmul(layer_1, weights[<span class="string">'w2'</span>]), biases[<span class="string">'b2'</span>])</span><br><span class="line">    layer_2 = tf.nn.relu(layer_2)</span><br><span class="line">    <span class="comment"># Create another summary to visualize the second layer ReLU activation</span></span><br><span class="line">    tf.summary.histogram(<span class="string">"relu2"</span>, layer_2)</span><br><span class="line">    <span class="comment"># Output layer</span></span><br><span class="line">    out_layer = tf.add(tf.matmul(layer_2, weights[<span class="string">'w3'</span>]), biases[<span class="string">'b3'</span>])</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Store layers weight &amp; bias</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'w1'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1]), name=<span class="string">'W1'</span>),</span><br><span class="line">    <span class="string">'w2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name=<span class="string">'W2'</span>),</span><br><span class="line">    <span class="string">'w3'</span>: tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name=<span class="string">'W3'</span>)</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'b1'</span>: tf.Variable(tf.random_normal([n_hidden_1]), name=<span class="string">'b1'</span>),</span><br><span class="line">    <span class="string">'b2'</span>: tf.Variable(tf.random_normal([n_hidden_2]), name=<span class="string">'b2'</span>),</span><br><span class="line">    <span class="string">'b3'</span>: tf.Variable(tf.random_normal([n_classes]), name=<span class="string">'b3'</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="创建模型和操作（模型-损失函数-优化-准确率）"><a href="#创建模型和操作（模型-损失函数-优化-准确率）" class="headerlink" title="创建模型和操作（模型+损失函数+优化+准确率）"></a>创建模型和操作（模型+损失函数+优化+准确率）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Encapsulating all ops into scopes, making Tensorboard's Graph</span></span><br><span class="line"><span class="comment"># Visualization more convenient</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Model'</span>):</span><br><span class="line">    <span class="comment"># Build model</span></span><br><span class="line">    pred = multilayer_perceptron(x, weights, biases)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Loss'</span>):</span><br><span class="line">    <span class="comment"># Softmax Cross entropy (cost function)</span></span><br><span class="line">    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'SGD'</span>):</span><br><span class="line">    <span class="comment"># Gradient Descent</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line">    <span class="comment"># Op to calculate every variable gradient</span></span><br><span class="line">    grads = tf.gradients(loss, tf.trainable_variables())</span><br><span class="line">    grads = list(zip(grads, tf.trainable_variables()))</span><br><span class="line">    <span class="comment"># Op to update all variables according to their gradient</span></span><br><span class="line">    apply_grads = optimizer.apply_gradients(grads_and_vars=grads)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Accuracy'</span>):</span><br><span class="line">    <span class="comment"># Accuracy</span></span><br><span class="line">    acc = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    acc = tf.reduce_mean(tf.cast(acc, tf.float32))</span><br></pre></td></tr></table></figure><h4 id="初始化并合并操作"><a href="#初始化并合并操作" class="headerlink" title="初始化并合并操作"></a>初始化并合并操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a summary to monitor cost tensor</span></span><br><span class="line">tf.summary.scalar(<span class="string">"loss"</span>, loss)</span><br><span class="line"><span class="comment"># Create a summary to monitor accuracy tensor</span></span><br><span class="line">tf.summary.scalar(<span class="string">"accuracy"</span>, acc)</span><br><span class="line"><span class="comment"># Create summaries to visualize weights</span></span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> tf.trainable_variables():</span><br><span class="line">    tf.summary.histogram(var.name, var)</span><br><span class="line"><span class="comment"># Summarize all gradients</span></span><br><span class="line"><span class="keyword">for</span> grad, var <span class="keyword">in</span> grads:</span><br><span class="line">    tf.summary.histogram(var.name + <span class="string">'/gradient'</span>, grad)</span><br><span class="line"><span class="comment"># Merge all summaries into a single op</span></span><br><span class="line">merged_summary_op = tf.summary.merge_all()</span><br></pre></td></tr></table></figure><hr><pre><code>INFO:tensorflow:Summary name W1:0 is illegal; using W1_0 instead.INFO:tensorflow:Summary name W2:0 is illegal; using W2_0 instead.INFO:tensorflow:Summary name W3:0 is illegal; using W3_0 instead.INFO:tensorflow:Summary name b1:0 is illegal; using b1_0 instead.INFO:tensorflow:Summary name b2:0 is illegal; using b2_0 instead.INFO:tensorflow:Summary name b3:0 is illegal; using b3_0 instead.INFO:tensorflow:Summary name W1:0/gradient is illegal; using W1_0/gradient instead.INFO:tensorflow:Summary name W2:0/gradient is illegal; using W2_0/gradient instead.INFO:tensorflow:Summary name W3:0/gradient is illegal; using W3_0/gradient instead.INFO:tensorflow:Summary name b1:0/gradient is illegal; using b1_0/gradient instead.INFO:tensorflow:Summary name b2:0/gradient is illegal; using b2_0/gradient instead.INFO:tensorflow:Summary name b3:0/gradient is illegal; using b3_0/gradient instead.</code></pre><h4 id="训练并保存log"><a href="#训练并保存log" class="headerlink" title="训练并保存log"></a>训练并保存log</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run the initializer</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># op to write logs to Tensorboard</span></span><br><span class="line">    summary_writer = tf.summary.FileWriter(logs_path,</span><br><span class="line">                                            graph=tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop), cost op (to get loss value)</span></span><br><span class="line">            <span class="comment"># and summary nodes</span></span><br><span class="line">            _, c, summary = sess.run([apply_grads, loss, merged_summary_op],</span><br><span class="line">                                     feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># Write logs at every iteration</span></span><br><span class="line">            summary_writer.add_summary(summary, epoch * total_batch + i)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    <span class="comment"># Calculate accuracy</span></span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, acc.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Run the command line:\n"</span> \</span><br><span class="line">          <span class="string">"--&gt; tensorboard --logdir=./log "</span> \</span><br><span class="line">          <span class="string">"\nThen open http://0.0.0.0:6006/ into your web browser"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Epoch: 0001 cost= 82.491150440Epoch: 0002 cost= 11.219711702Epoch: 0003 cost= 6.885841494Epoch: 0004 cost= 4.898687713Epoch: 0005 cost= 3.742709111Epoch: 0006 cost= 2.969850923Epoch: 0007 cost= 2.429568350Epoch: 0008 cost= 2.024799560Epoch: 0009 cost= 1.742192560Epoch: 0010 cost= 1.494883727Epoch: 0011 cost= 1.313867836Epoch: 0012 cost= 1.153405372Epoch: 0013 cost= 1.022956383Epoch: 0014 cost= 0.917282970Epoch: 0015 cost= 0.831443023Epoch: 0016 cost= 0.739466778Epoch: 0017 cost= 0.660427638Epoch: 0018 cost= 0.606233582Epoch: 0019 cost= 0.547995506Epoch: 0020 cost= 0.506534999Epoch: 0021 cost= 0.462353780Epoch: 0022 cost= 0.424939641Epoch: 0023 cost= 0.399291764Epoch: 0024 cost= 0.364750651Epoch: 0025 cost= 0.334185596Optimization Finished!Accuracy: 0.9215Run the command line:--&gt; tensorboard --logdir=./logThen open http://0.0.0.0:6006/ into your web browser</code></pre><h4 id="损失和准确率折线图"><a href="#损失和准确率折线图" class="headerlink" title="损失和准确率折线图"></a>损失和准确率折线图</h4><p><img src="http://xukeqiniu.xukeai.cn/d9addb5e916784c4dc58a5fd3b7d19b2.png" alt="Loss and Accuracy Visualization"></p><h4 id="计算图模型的可视化"><a href="#计算图模型的可视化" class="headerlink" title="计算图模型的可视化"></a>计算图模型的可视化</h4><p><img src="http://xukeqiniu.xukeai.cn/bb45bab3de233fb2ff3611af6cacb160.png" alt="Computation Graph——Model and SGD"><br><img src="http://xukeqiniu.xukeai.cn/b79932f2eb49429dbcd7af11abdc8c1c.png" alt="Computation Graph——Loss and Accuracy "></p><h4 id="权重及其梯度直方图"><a href="#权重及其梯度直方图" class="headerlink" title="权重及其梯度直方图"></a>权重及其梯度直方图</h4><p><img src="http://xukeqiniu.xukeai.cn/58f1c3f448e14df1632d8482b77c4701.png" alt="Weights and Gradients Visualization"></p><h4 id="偏置及其梯度直方图"><a href="#偏置及其梯度直方图" class="headerlink" title="偏置及其梯度直方图"></a>偏置及其梯度直方图</h4><p><img src="http://xukeqiniu.xukeai.cn/9ee31a697512aaa3b8803e17d70eaa1d.png" alt="bias and Gradients Visualization"></p><h4 id="FeatureMap-直方图"><a href="#FeatureMap-直方图" class="headerlink" title="FeatureMap 直方图"></a>FeatureMap 直方图</h4><p><img src="http://xukeqiniu.xukeai.cn/255b33e56d2ab12b375d529f52fcef74.png" alt="FeatureMap Visualization"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[TensorBoard: 图表可视化]<a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html" target="_blank" rel="noopener">http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;多层感知机的Tensorboard可视化&quot;&gt;&lt;a href=&quot;#多层感知机的Tensorboard可视化&quot; class=&quot;headerlink&quot; title=&quot;多层感知机的Tensorboard可视化&quot;&gt;&lt;/a&gt;多层感知机的Tensorboard可视化&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; __future__ &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; print_function&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tensorflow &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; tf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;导入数据集&quot;&gt;&lt;a href=&quot;#导入数据集&quot; class=&quot;headerlink&quot; title=&quot;导入数据集&quot;&gt;&lt;/a&gt;导入数据集&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Import MNIST data&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; tensorflow.examples.tutorials.mnist &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; input_data&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mnist = input_data.read_data_sets(&lt;span class=&quot;string&quot;&gt;&quot;./data/&quot;&lt;/span&gt;, one_hot=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code&gt;Extracting ./data/train-images-idx3-ubyte.gz
Extracting ./data/train-labels-idx1-ubyte.gz
Extracting ./data/t10k-images-idx3-ubyte.gz
Extracting ./data/t10k-labels-idx1-ubyte.gz
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>TensorBoard基础篇</title>
    <link href="http://xywang93.github.io.git/2018/03/11/DeepLearning/TensorFlow/11TensorBoard%E5%9F%BA%E7%A1%80%E7%AF%87/"/>
    <id>http://xywang93.github.io.git/2018/03/11/DeepLearning/TensorFlow/11TensorBoard基础篇/</id>
    <published>2018-03-10T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:24.390Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TensorBoard简介"><a href="#TensorBoard简介" class="headerlink" title="TensorBoard简介"></a>TensorBoard简介</h3><p>TensorBoard是Tensorflow自带的一个强大的可视化工具，也是一个web应用程序套件。在众多机器学习库中，Tensorflow是目前唯一自带可视化工具的库，这也是Tensorflow的一个优点。学会使用TensorBoard，将可以帮助我们构建复杂模型。</p><p>这里需要理解“可视化”的意义。“可视化”也叫做数据可视化。是关于数据之视觉表现形式的研究。这种数据的视觉表现形式被定义为一种以某种概要形式抽提出来的信息，包括相应信息单位的各种属性和变量。例如我们需要可视化算法运行的错误率，那么我们可以取算法每次训练的错误率，绘制成折线图或曲线图，来表达训练过程中错误率的变化。可视化的方法有很多种。但无论哪一种，均是对数据进行摘要(summary)与处理。</p><p>通常使用TensorBoard有三个步骤：</p><ul><li>首先需要在需要可视化的相关部位添加可视化代码，即创建摘要、添加摘要；</li><li>其次运行代码，可以生成了一个或多个事件文件(event files)；</li><li>最后启动TensorBoard的Web服务器。</li></ul><p>完成以上三个步骤，就可以在浏览器中可视化结果，Web服务器将会分析这个事件文件中的内容，并在浏览器中将结果绘制出来。</p><p>如果我们已经拥有了一个事件文件，也可以直接利用TensorBoard查看这个事件文件中的摘要。<br>TensorBoard视图如下所示：<br><img src="http://xukeqiniu.xukeai.cn/graph_vis_animation.gif" alt="TensorBoard示意图"></p><a id="more"></a><h3 id="Logistic回归的Tensorboard可视化"><a href="#Logistic回归的Tensorboard可视化" class="headerlink" title="Logistic回归的Tensorboard可视化"></a>Logistic回归的Tensorboard可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MINST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_epoch = <span class="number">1</span></span><br><span class="line">logs_path = <span class="string">'./log/example/'</span> <span class="comment"># log存放位置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line"><span class="comment"># mnist data image of shape 28*28=784</span></span><br><span class="line"><span class="comment">#（name=''将在Tensorboard中显示）</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>], name=<span class="string">'InputData'</span>) <span class="comment">#输入数据（InputData）</span></span><br><span class="line"><span class="comment"># 0-9 digits recognition =&gt; 10 classes</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>], name=<span class="string">'LabelData'</span>) <span class="comment"># 输出标签（LabelData）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set model weights</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]), name=<span class="string">'Weights'</span>) <span class="comment">#权重（Weights）</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]), name=<span class="string">'Bias'</span>) <span class="comment">#偏置（Bias）</span></span><br></pre></td></tr></table></figure><h4 id="构建模型和操作（模型-损失函数-优化-准确率）"><a href="#构建模型和操作（模型-损失函数-优化-准确率）" class="headerlink" title="构建模型和操作（模型+损失函数+优化+准确率）"></a>构建模型和操作（模型+损失函数+优化+准确率）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Construct model and encapsulating all ops into scopes, making</span></span><br><span class="line"><span class="comment"># Tensorboard's Graph visualization more convenient</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Model'</span>):</span><br><span class="line">    <span class="comment"># Model</span></span><br><span class="line">    pred = tf.nn.softmax(tf.matmul(x, W) + b) <span class="comment"># Softmax</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Loss'</span>):</span><br><span class="line">    <span class="comment"># Minimize error using cross entropy</span></span><br><span class="line">    cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'SGD'</span>):</span><br><span class="line">    <span class="comment"># Gradient Descent</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'Accuracy'</span>):</span><br><span class="line">    <span class="comment"># Accuracy</span></span><br><span class="line">    acc = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    acc = tf.reduce_mean(tf.cast(acc, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the variables</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a summary to monitor cost tensor</span></span><br><span class="line">tf.summary.scalar(<span class="string">"loss"</span>, cost)</span><br><span class="line"><span class="comment"># Create a summary to monitor accuracy tensor</span></span><br><span class="line">tf.summary.scalar(<span class="string">"accuracy"</span>, acc)</span><br><span class="line"><span class="comment"># Merge all summaries into a single op</span></span><br><span class="line">merged_summary_op = tf.summary.merge_all()</span><br></pre></td></tr></table></figure><h4 id="训练并保存log"><a href="#训练并保存log" class="headerlink" title="训练并保存log"></a>训练并保存log</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start Training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># op to write logs to Tensorboard</span></span><br><span class="line">    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop), cost op (to get loss value)</span></span><br><span class="line">            <span class="comment"># and summary nodes</span></span><br><span class="line">            _, c, summary = sess.run([optimizer, cost, merged_summary_op],</span><br><span class="line">                                     feed_dict=&#123;x: batch_xs, y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># Write logs at every iteration</span></span><br><span class="line">            summary_writer.add_summary(summary, epoch * total_batch + i)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_epoch == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    <span class="comment"># Calculate accuracy</span></span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, acc.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Run the command line:\n"</span> \</span><br><span class="line">          <span class="string">"--&gt; tensorboard --logdir=./log"</span> \</span><br><span class="line">          <span class="string">"\nThen open http://0.0.0.0:6006/ into your web browser"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Epoch: 0001 cost= 1.183717763Epoch: 0002 cost= 0.665147323Epoch: 0003 cost= 0.552818966Epoch: 0004 cost= 0.498699070Epoch: 0005 cost= 0.465521080Epoch: 0006 cost= 0.442596199Epoch: 0007 cost= 0.425560050Epoch: 0008 cost= 0.412205354Epoch: 0009 cost= 0.401337254Epoch: 0010 cost= 0.392412475Epoch: 0011 cost= 0.384738669Epoch: 0012 cost= 0.378180920Epoch: 0013 cost= 0.372407395Epoch: 0014 cost= 0.367316018Epoch: 0015 cost= 0.362715464Epoch: 0016 cost= 0.358595766Epoch: 0017 cost= 0.354887394Epoch: 0018 cost= 0.351458600Epoch: 0019 cost= 0.348339875Epoch: 0020 cost= 0.345448156Epoch: 0021 cost= 0.342770365Epoch: 0022 cost= 0.340232303Epoch: 0023 cost= 0.337901928Epoch: 0024 cost= 0.335753958Epoch: 0025 cost= 0.333657109Optimization Finished!Accuracy: 0.9136Run the command line:--&gt; tensorboard --logdir=./logThen open http://0.0.0.0:6006/ into your web browser</code></pre><h4 id="损失和准确率的可视化"><a href="#损失和准确率的可视化" class="headerlink" title="损失和准确率的可视化"></a>损失和准确率的可视化</h4><p><img src="http://xukeqiniu.xukeai.cn/dfd03c5ec422bd903841637b67f97910.png" alt="Loss and Accuracy Visualization"></p><h4 id="计算图模型的基本单元"><a href="#计算图模型的基本单元" class="headerlink" title="计算图模型的基本单元"></a>计算图模型的基本单元</h4><p><img src="http://xukeqiniu.xukeai.cn/0d397a70c516c7fe3f7ceeec0511793f.png" alt=""></p><h4 id="计算图模型的可视化"><a href="#计算图模型的可视化" class="headerlink" title="计算图模型的可视化"></a>计算图模型的可视化</h4><p><img src="http://xukeqiniu.xukeai.cn/45577b0533ec74f923dd92bd4d09519b.png" alt="Graph Visualization"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[TensorBoard: 图表可视化]<a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html" target="_blank" rel="noopener">http://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/graph_viz.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;TensorBoard简介&quot;&gt;&lt;a href=&quot;#TensorBoard简介&quot; class=&quot;headerlink&quot; title=&quot;TensorBoard简介&quot;&gt;&lt;/a&gt;TensorBoard简介&lt;/h3&gt;&lt;p&gt;TensorBoard是Tensorflow自带的一个强大的可视化工具，也是一个web应用程序套件。在众多机器学习库中，Tensorflow是目前唯一自带可视化工具的库，这也是Tensorflow的一个优点。学会使用TensorBoard，将可以帮助我们构建复杂模型。&lt;/p&gt;
&lt;p&gt;这里需要理解“可视化”的意义。“可视化”也叫做数据可视化。是关于数据之视觉表现形式的研究。这种数据的视觉表现形式被定义为一种以某种概要形式抽提出来的信息，包括相应信息单位的各种属性和变量。例如我们需要可视化算法运行的错误率，那么我们可以取算法每次训练的错误率，绘制成折线图或曲线图，来表达训练过程中错误率的变化。可视化的方法有很多种。但无论哪一种，均是对数据进行摘要(summary)与处理。&lt;/p&gt;
&lt;p&gt;通常使用TensorBoard有三个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先需要在需要可视化的相关部位添加可视化代码，即创建摘要、添加摘要；&lt;/li&gt;
&lt;li&gt;其次运行代码，可以生成了一个或多个事件文件(event files)；&lt;/li&gt;
&lt;li&gt;最后启动TensorBoard的Web服务器。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;完成以上三个步骤，就可以在浏览器中可视化结果，Web服务器将会分析这个事件文件中的内容，并在浏览器中将结果绘制出来。&lt;/p&gt;
&lt;p&gt;如果我们已经拥有了一个事件文件，也可以直接利用TensorBoard查看这个事件文件中的摘要。&lt;br&gt;TensorBoard视图如下所示：&lt;br&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/graph_vis_animation.gif&quot; alt=&quot;TensorBoard示意图&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow模型的保存与读取</title>
    <link href="http://xywang93.github.io.git/2018/03/10/DeepLearning/TensorFlow/10Tensorflow%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96/"/>
    <id>http://xywang93.github.io.git/2018/03/10/DeepLearning/TensorFlow/10Tensorflow模型的保存与读取/</id>
    <published>2018-03-09T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:24.386Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>首先，我们从一个直观的例子，讲解如何实现Tensorflow模型参数的保存以及保存后模型的读取。<br>然后，我们在之前多层感知机的基础上进行模型的参数保存，以及参数的读取。该项技术可以用于Tensorflow分段训练模型以及对经典模型进行<code>fine tuning</code>（微调）</p><a id="more"></a><h3 id="Tensorflow-模型的保存与读取（直观）"><a href="#Tensorflow-模型的保存与读取（直观）" class="headerlink" title="Tensorflow 模型的保存与读取（直观）"></a>Tensorflow 模型的保存与读取（直观）</h3><h4 id="模型参数存储"><a href="#模型参数存储" class="headerlink" title="模型参数存储"></a>模型参数存储</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成v1与v2变量</span></span><br><span class="line">v1 = tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">2</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line"><span class="comment"># 全局初始化</span></span><br><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># 调用Saver方法（重要）</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"V1:"</span>,sess.run(v1))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"V2:"</span>,sess.run(v2))</span><br><span class="line">    <span class="comment"># 存储Session工作空间</span></span><br><span class="line">    saver_path = saver.save(sess, <span class="string">"./save/model.ckpt"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Model saved in file: "</span>, saver_path)</span><br></pre></td></tr></table></figure><pre><code>V1: [[1.2366687 0.4373563]]V2: [[-0.9465265  -0.7147392  -2.421146  ] [-0.48401725  0.40536404  0.21300188]]Model saved in file:  ./save/model.ckpt</code></pre><p>模型存储的文件格式如下图所示：<br><img src="http://xukeqiniu.xukeai.cn/db94d565e96ee5ba244853e52ca1b675.png" alt="模型存储文件"></p><h4 id="模型参数读取"><a href="#模型参数读取" class="headerlink" title="模型参数读取"></a>模型参数读取</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">v1 = tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">2</span>]), name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>]), name=<span class="string">"v2"</span>)</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"./save/model.ckpt"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"V1:"</span>,sess.run(v1))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"V2:"</span>,sess.run(v2))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Model restored"</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>INFO:tensorflow:Restoring parameters from ./save/model.ckptV1: [[1.2366687 0.4373563]]V2: [[-0.9465265  -0.7147392  -2.421146  ] [-0.48401725  0.40536404  0.21300188]]Model restored</code></pre><h3 id="Tensorflow-模型的保存与读取（多层感知机）"><a href="#Tensorflow-模型的保存与读取（多层感知机）" class="headerlink" title="Tensorflow 模型的保存与读取（多层感知机）"></a>Tensorflow 模型的保存与读取（多层感知机）</h3><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import MINST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="创建多层感知机模型"><a href="#创建多层感知机模型" class="headerlink" title="创建多层感知机模型"></a>创建多层感知机模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练参数设置</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br><span class="line">model_path = <span class="string">"./save/model.ckpt"</span> <span class="comment">#模型存储路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络参数设置</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span> <span class="comment"># 1st layer number of features</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span> <span class="comment"># 2nd layer number of features</span></span><br><span class="line">n_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">n_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph input</span></span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_input])</span><br><span class="line">y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_classes])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create model</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multilayer_perceptron</span><span class="params">(x, weights, biases)</span>:</span></span><br><span class="line">    <span class="comment"># Hidden layer with RELU activation</span></span><br><span class="line">    layer_1 = tf.add(tf.matmul(x, weights[<span class="string">'h1'</span>]), biases[<span class="string">'b1'</span>])</span><br><span class="line">    layer_1 = tf.nn.relu(layer_1)</span><br><span class="line">    <span class="comment"># Hidden layer with RELU activation</span></span><br><span class="line">    layer_2 = tf.add(tf.matmul(layer_1, weights[<span class="string">'h2'</span>]), biases[<span class="string">'b2'</span>])</span><br><span class="line">    layer_2 = tf.nn.relu(layer_2)</span><br><span class="line">    <span class="comment"># Output layer with linear activation</span></span><br><span class="line">    out_layer = tf.matmul(layer_2, weights[<span class="string">'out'</span>]) + biases[<span class="string">'out'</span>]</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Store layers weight &amp; bias</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'h1'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1])),</span><br><span class="line">    <span class="string">'h2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_hidden_2, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'b1'</span>: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    <span class="string">'b2'</span>: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct model</span></span><br><span class="line">pred = multilayer_perceptron(x, weights, biases)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss and optimizer</span></span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the variables</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h4 id="调用Saver方法"><a href="#调用Saver方法" class="headerlink" title="调用Saver方法"></a>调用Saver方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 'Saver' 操作用于保存与读取所有的变量</span></span><br><span class="line">saver = tf.train.Saver()</span><br></pre></td></tr></table></figure><h4 id="第一次训练（训练完成保存参数）"><a href="#第一次训练（训练完成保存参数）" class="headerlink" title="第一次训练（训练完成保存参数）"></a>第一次训练（训练完成保存参数）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Running first session</span></span><br><span class="line">print(<span class="string">"Starting 1st session..."</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Initialize variables</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle（迭代三次）</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_x,</span><br><span class="line">                                                          y: batch_y&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, \</span><br><span class="line">                <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">    print(<span class="string">"First Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Calculate accuracy</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型参数到硬盘上</span></span><br><span class="line">    save_path = saver.save(sess, model_path)</span><br><span class="line">    print(<span class="string">"Model saved in file: %s"</span> % save_path)</span><br></pre></td></tr></table></figure><hr><pre><code>Starting 1st session...Epoch: 0001 cost= 172.468734065Epoch: 0002 cost= 43.036823805Epoch: 0003 cost= 26.978232009First Optimization Finished!Accuracy: 0.9084Model saved in file: ./save/model.ckpt</code></pre><h4 id="第二次训练（加载第一次参数）"><a href="#第二次训练（加载第一次参数）" class="headerlink" title="第二次训练（加载第一次参数）"></a>第二次训练（加载第一次参数）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Running a new session</span></span><br><span class="line">print(<span class="string">"Starting 2nd session..."</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Initialize variables</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Restore model weights from previously saved model</span></span><br><span class="line">    load_path = saver.restore(sess, model_path)</span><br><span class="line">    print(<span class="string">"Model restored from file: %s"</span> % save_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Resume training</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">7</span>):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples / batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_x,</span><br><span class="line">                                                          y: batch_y&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>, \</span><br><span class="line">                <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line">    print(<span class="string">"Second Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Calculate accuracy</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy.eval(</span><br><span class="line">        &#123;x: mnist.test.images, y: mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure><hr><pre><code>Starting 2nd session...INFO:tensorflow:Restoring parameters from ./save/model.ckptModel restored from file: ./save/model.ckptEpoch: 0001 cost= 18.712020244Epoch: 0002 cost= 13.624892972Epoch: 0003 cost= 10.156988694Epoch: 0004 cost= 7.652410518Epoch: 0005 cost= 5.658380691Epoch: 0006 cost= 4.276506317Epoch: 0007 cost= 3.249772967Second Optimization Finished!Accuracy: 0.9381</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">TensorFlow-Examples</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;首先，我们从一个直观的例子，讲解如何实现Tensorflow模型参数的保存以及保存后模型的读取。&lt;br&gt;然后，我们在之前多层感知机的基础上进行模型的参数保存，以及参数的读取。该项技术可以用于Tensorflow分段训练模型以及对经典模型进行&lt;code&gt;fine tuning&lt;/code&gt;（微调）&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow神经网络之卷积神经网络</title>
    <link href="http://xywang93.github.io.git/2018/03/09/DeepLearning/TensorFlow/09Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://xywang93.github.io.git/2018/03/09/DeepLearning/TensorFlow/09Tensorflow神经网络之卷积神经网络/</id>
    <published>2018-03-08T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:38.417Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Tensorflow卷积神经网络实现"><a href="#Tensorflow卷积神经网络实现" class="headerlink" title="Tensorflow卷积神经网络实现"></a>Tensorflow卷积神经网络实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, absolute_import</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><a id="more"></a><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">num_steps = <span class="number">2000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">num_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br><span class="line">dropout = <span class="number">0.25</span> <span class="comment"># Dropout, probability to drop a unit</span></span><br></pre></td></tr></table></figure><h4 id="定义CNN模型"><a href="#定义CNN模型" class="headerlink" title="定义CNN模型"></a>定义CNN模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the neural network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_net</span><span class="params">(x_dict, n_classes, dropout, reuse, is_training)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define a scope for reusing the variables</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'ConvNet'</span>, reuse=reuse):</span><br><span class="line">        <span class="comment"># TF Estimator input is a dict, in case of multiple inputs</span></span><br><span class="line">        x = x_dict[<span class="string">'images'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># MNIST data input is a 1-D vector of 784 features (28*28 pixels)</span></span><br><span class="line">        <span class="comment"># Reshape to match picture format [Height x Width x Channel]</span></span><br><span class="line">        <span class="comment"># Tensor input become 4-D: [Batch Size, Height, Width, Channel]</span></span><br><span class="line">        x = tf.reshape(x, shape=[<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convolution Layer with 32 filters and a kernel size of 5</span></span><br><span class="line">        conv1 = tf.layers.conv2d(x, <span class="number">32</span>, <span class="number">5</span>, activation=tf.nn.relu)</span><br><span class="line">        <span class="comment"># Max Pooling (down-sampling) with strides of 2 and kernel size of 2</span></span><br><span class="line">        conv1 = tf.layers.max_pooling2d(conv1, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convolution Layer with 64 filters and a kernel size of 3</span></span><br><span class="line">        conv2 = tf.layers.conv2d(conv1, <span class="number">64</span>, <span class="number">3</span>, activation=tf.nn.relu)</span><br><span class="line">        <span class="comment"># Max Pooling (down-sampling) with strides of 2 and kernel size of 2</span></span><br><span class="line">        conv2 = tf.layers.max_pooling2d(conv2, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Flatten the data to a 1-D vector for the fully connected layer</span></span><br><span class="line">        fc1 = tf.contrib.layers.flatten(conv2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Fully connected layer (in tf contrib folder for now)</span></span><br><span class="line">        fc1 = tf.layers.dense(fc1, <span class="number">1024</span>)</span><br><span class="line">        <span class="comment"># Apply Dropout (if is_training is False, dropout is not applied)</span></span><br><span class="line">        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Output layer, class prediction</span></span><br><span class="line">        out = tf.layers.dense(fc1, n_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><blockquote><p><strong>补充：tf.nn，tf.layers， tf.contrib模块区别</strong> [^1]<br>tf.nn，tf.layers， tf.contrib模块有很多功能是重复的，尤其是卷积操作，在使用的时候，我们可以根据需要现在不同的模块。但有些时候可以一起混用。<br>下面是对三个模块的简述：</p><ul><li>tf.nn ：提供神经网络相关操作的支持，包括卷积操作（conv）、池化操作（pooling）、归一化、loss、分类操作、embedding、RNN、Evaluation。</li><li>tf.layers：主要提供的高层的神经网络，主要和卷积相关的，个人感觉是对tf.nn的进一步封装，tf.nn会更底层一些。</li><li>tf.contrib：tf.contrib.layers提供够将计算图中的  网络层、正则化、摘要操作、是构建计算图的高级操作，但是tf.contrib包含不稳定和实验代码，有可能以后API会改变。<br>以上三个模块的封装程度是逐个递进的。</li></ul><p><strong>补充：TensorFlow layers模块</strong>  [^2]</p><h5 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h5><p>Convolution 有多个方法，如 conv1d()、conv2d()、conv3d()，分别代表一维、二维、三维卷积，另外还有 conv2d_transpose()、conv3d_transpose()，分别代表二维和三维反卷积，还有 separable_conv2d() 方法代表二维深度可分离卷积。它们定义在 tensorflow/python/layers/convolutional.py 中，其用法都是类似的，在这里以 conv2d() 方法为例进行说明。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt; conv2d(</span><br><span class="line">&gt;     inputs,</span><br><span class="line">&gt;     filters,</span><br><span class="line">&gt;     kernel_size,</span><br><span class="line">&gt;     strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">&gt;     padding=<span class="string">'valid'</span>,</span><br><span class="line">&gt;     data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">&gt;     dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">&gt;     activation=<span class="keyword">None</span>,</span><br><span class="line">&gt;     use_bias=<span class="keyword">True</span>,</span><br><span class="line">&gt;     kernel_initializer=<span class="keyword">None</span>,</span><br><span class="line">&gt;     bias_initializer=tf.zeros_initializer(),</span><br><span class="line">&gt;     kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;     bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;     activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;     kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">&gt;     bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">&gt;     trainable=<span class="keyword">True</span>,</span><br><span class="line">&gt;     name=<span class="keyword">None</span>,</span><br><span class="line">&gt;     reuse=<span class="keyword">None</span></span><br><span class="line">&gt; )</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p><p>参数说明如下：</p><ul><li>inputs：必需，即需要进行操作的输入数据。</li><li>filters：必需，是一个数字，代表了输出通道的个数，即 output_channels。</li><li>kernel_size：必需，卷积核大小，必须是一个数字（高和宽都是此数字）或者长度为 2 的列表（分别代表高、宽）。</li><li>strides：可选，默认为 (1, 1)，卷积步长，必须是一个数字（高和宽都是此数字）或者长度为 2 的列表（分别代表高、宽）。</li><li>padding：可选，默认为 valid，padding 的模式，有 valid 和 same 两种，大小写不区分。</li><li>data_format：可选，默认 channels_last，分为 channels_last 和 channels_first 两种模式，代表了输入数据的维度类型，如果是 channels_last，那么输入数据的 shape 为 (batch, height, width, channels)，如果是 channels_first，那么输入数据的 shape 为 (batch, channels, height, width)。</li><li>dilation_rate：可选，默认为 (1, 1)，卷积的扩张率，如当扩张率为 2 时，卷积核内部就会有边距，3×3 的卷积核就会变成 5×5。</li><li>activation：可选，默认为 None，如果为 None 则是线性激活。</li><li>use_bias：可选，默认为 True，是否使用偏置。</li><li>kernel_initializer：可选，默认为 None，即权重的初始化方法，如果为 None，则使用默认的 Xavier 初始化方法。</li><li>bias_initializer：可选，默认为零值初始化，即偏置的初始化方法。</li><li>kernel_regularizer：可选，默认为 None，施加在权重上的正则项。</li><li>bias_regularizer：可选，默认为 None，施加在偏置上的正则项。</li><li>activity_regularizer：可选，默认为 None，施加在输出上的正则项。</li><li>kernel_constraint，可选，默认为 None，施加在权重上的约束项。</li><li>bias_constraint，可选，默认为 None，施加在偏置上的约束项。</li><li>trainable：可选，默认为 True，布尔类型，如果为 True，则将变量添加到 GraphKeys.TRAINABLE_VARIABLES 中。</li><li>name：可选，默认为 None，卷积层的名称。</li><li>reuse：可选，默认为 None，布尔类型，如果为 True，那么如果 name 相同时，会重复利用。</li><li>返回值： 卷积后的 Tensor。</li></ul><p><strong>注意，这里只需要给出输入数据，输出通道数，卷积核大小即可。</strong></p><h5 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h5><p>layers 模块提供了多个池化方法，这几个池化方法都是类似的，包括 max_pooling1d()、max_pooling2d()、max_pooling3d()、average_pooling1d()、average_pooling2d()、average_pooling3d()，分别代表一维二维三维最大和平均池化方法，它们都定义在 tensorflow/python/layers/pooling.py 中，这里以 &gt; max_pooling2d() 方法为例进行介绍。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; max_pooling2d(</span><br><span class="line">&gt;     inputs,</span><br><span class="line">&gt;     pool_size,</span><br><span class="line">&gt;     strides,</span><br><span class="line">&gt;     padding=<span class="string">'valid'</span>,</span><br><span class="line">&gt;     data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">&gt;     name=<span class="keyword">None</span></span><br><span class="line">&gt; )</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p><p>参数说明如下：</p><ul><li>inputs: 必需，即需要池化的输入对象，必须是 4 维的。</li><li>pool_size：必需，池化窗口大小，必须是一个数字（高和宽都是此数字）或者长度为 2 的列表（分别代表高、宽）。</li><li>strides：必需，池化步长，必须是一个数字（高和宽都是此数字）或者长度为 2 的列表（分别代表高、宽）。</li><li>padding：可选，默认 valid，padding 的方法，valid 或者 same，大小写不区分。</li><li>data_format：可选，默认 channels_last，分为 channels_last 和 channels_first 两种模式，代表了输入数据的维度类型，如果是 channels_last，那么输入数据的 shape 为 (batch, height, width, channels)，如果是 channels_first，那么输入数据的 shape 为 (batch, channels, height, width)。</li><li>name：可选，默认 None，池化层的名称。</li><li>返回值： 经过池化处理后的 Tensor。</li></ul><h5 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h5><p>dropout 是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃，可以用来防止过拟合，layers 模块中提供了 dropout() 方法来实现这一操作，定义在 tensorflow/python/layers/core.py。下面我们来说明一下它的用法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; dropout(</span><br><span class="line">&gt;     inputs,</span><br><span class="line">&gt;     rate=<span class="number">0.5</span>,</span><br><span class="line">&gt;     noise_shape=<span class="keyword">None</span>,</span><br><span class="line">&gt;     seed=<span class="keyword">None</span>,</span><br><span class="line">&gt;     training=<span class="keyword">False</span>,</span><br><span class="line">&gt;     name=<span class="keyword">None</span></span><br><span class="line">&gt; )</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p><p>参数说明如下：</p><ul><li>inputs：必须，即输入数据。</li><li>rate：可选，默认为 0.5，即 dropout rate，如设置为 0.1，则意味着会丢弃 10% 的神经元。</li><li>noise_shape：可选，默认为 None，int32 类型的一维 Tensor，它代表了 dropout mask 的 shape，dropout mask 会与 inputs 相乘对 inputs 做转换，例如 inputs 的 shape 为 (batch_size, timesteps, features)，但我们想要 droput mask 在所有 timesteps 都是相同的，我们可以设置 noise_shape=[batch_size, 1, features]。</li><li>seed：可选，默认为 None，即产生随机熟的种子值。</li><li>training：可选，默认为 False，布尔类型，即代表了是否标志位 training 模式。</li><li>name：可选，默认为 None，dropout 层的名称。</li><li>返回： 经过 dropout 层之后的 Tensor。</li></ul></blockquote><h4 id="定义模型函数"><a href="#定义模型函数" class="headerlink" title="定义模型函数"></a>定义模型函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the model function (following TF Estimator Template)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the neural network</span></span><br><span class="line">    <span class="comment"># Because Dropout have different behavior at training and prediction time, we</span></span><br><span class="line">    <span class="comment"># need to create 2 distinct computation graphs that still share the same weights.</span></span><br><span class="line">    logits_train = conv_net(features, num_classes, dropout, reuse=<span class="keyword">False</span>, is_training=<span class="keyword">True</span>)</span><br><span class="line">    logits_test = conv_net(features, num_classes, dropout, reuse=<span class="keyword">True</span>, is_training=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predictions</span></span><br><span class="line">    pred_classes = tf.argmax(logits_test, axis=<span class="number">1</span>)</span><br><span class="line">    pred_probas = tf.nn.softmax(logits_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If prediction mode, early return</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, predictions=pred_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define loss and optimizer</span></span><br><span class="line">    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))</span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Evaluate the accuracy of the model</span></span><br><span class="line">    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TF Estimators requires to return a EstimatorSpec, that specify</span></span><br><span class="line">    <span class="comment"># the different ops for training, evaluating, ...</span></span><br><span class="line">    estim_specs = tf.estimator.EstimatorSpec(</span><br><span class="line">      mode=mode,</span><br><span class="line">      predictions=pred_classes,</span><br><span class="line">      loss=loss_op,</span><br><span class="line">      train_op=train_op,</span><br><span class="line">      eval_metric_ops=&#123;<span class="string">'accuracy'</span>: acc_op&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> estim_specs</span><br></pre></td></tr></table></figure><h4 id="创建评估器"><a href="#创建评估器" class="headerlink" title="创建评估器"></a>创建评估器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the Estimator</span></span><br><span class="line">model = tf.estimator.Estimator(model_fn)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Using default config.WARNING:tensorflow:Using temporary folder as model directory: C:\Users\xywang\AppData\Local\Temp\tmp8i1k3w75INFO:tensorflow:Using config: {&apos;_model_dir&apos;: &apos;C:\\Users\\xywang\\AppData\\Local\\Temp\\tmp8i1k3w75&apos;, &apos;_tf_random_seed&apos;: None, &apos;_save_summary_steps&apos;: 100, &apos;_save_checkpoints_steps&apos;: None, &apos;_save_checkpoints_secs&apos;: 600, &apos;_session_config&apos;: None, &apos;_keep_checkpoint_max&apos;: 5, &apos;_keep_checkpoint_every_n_hours&apos;: 10000, &apos;_log_step_count_steps&apos;: 100, &apos;_service&apos;: None, &apos;_cluster_spec&apos;: &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F84714B780&gt;, &apos;_task_type&apos;: &apos;worker&apos;, &apos;_task_id&apos;: 0, &apos;_global_id_in_cluster&apos;: 0, &apos;_master&apos;: &apos;&apos;, &apos;_evaluation_master&apos;: &apos;&apos;, &apos;_is_chief&apos;: True, &apos;_num_ps_replicas&apos;: 0, &apos;_num_worker_replicas&apos;: 1}</code></pre><h4 id="定义输入方法"><a href="#定义输入方法" class="headerlink" title="定义输入方法"></a>定义输入方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the input function for training</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: mnist.train.images&#125;, y=mnist.train.labels,</span><br><span class="line">    batch_size=batch_size, num_epochs=<span class="keyword">None</span>, shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the Model</span></span><br><span class="line">model.train(input_fn, steps=num_steps)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Create CheckpointSaverHook.INFO:tensorflow:Graph was finalized.INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.INFO:tensorflow:Saving checkpoints for 1 into C:\Users\xywang\AppData\Local\Temp\tmp8i1k3w75\model.ckpt.INFO:tensorflow:loss = 2.310159, step = 1INFO:tensorflow:global_step/sec: 7.94691INFO:tensorflow:loss = 0.15775274, step = 101 (12.585 sec)INFO:tensorflow:global_step/sec: 7.43979INFO:tensorflow:loss = 0.051440004, step = 201 (13.440 sec)INFO:tensorflow:global_step/sec: 8.26849INFO:tensorflow:loss = 0.07565387, step = 301 (12.095 sec)INFO:tensorflow:global_step/sec: 8.47324INFO:tensorflow:loss = 0.043410238, step = 401 (11.802 sec)INFO:tensorflow:global_step/sec: 7.94311INFO:tensorflow:loss = 0.048961755, step = 501 (12.590 sec)INFO:tensorflow:global_step/sec: 8.58757INFO:tensorflow:loss = 0.024859685, step = 601 (11.645 sec)INFO:tensorflow:global_step/sec: 8.39987INFO:tensorflow:loss = 0.07183821, step = 701 (11.904 sec)INFO:tensorflow:global_step/sec: 8.6733INFO:tensorflow:loss = 0.007703744, step = 801 (11.530 sec)INFO:tensorflow:global_step/sec: 8.25551INFO:tensorflow:loss = 0.02502199, step = 901 (12.113 sec)INFO:tensorflow:global_step/sec: 7.98054INFO:tensorflow:loss = 0.019118268, step = 1001 (12.563 sec)INFO:tensorflow:global_step/sec: 8.3921INFO:tensorflow:loss = 0.009793495, step = 1101 (11.884 sec)INFO:tensorflow:global_step/sec: 7.6179INFO:tensorflow:loss = 0.08203622, step = 1201 (13.127 sec)INFO:tensorflow:global_step/sec: 8.35142INFO:tensorflow:loss = 0.03721855, step = 1301 (11.975 sec)INFO:tensorflow:global_step/sec: 8.33818INFO:tensorflow:loss = 0.025231175, step = 1401 (11.992 sec)INFO:tensorflow:global_step/sec: 8.6748INFO:tensorflow:loss = 0.026730753, step = 1501 (11.528 sec)INFO:tensorflow:global_step/sec: 8.43105INFO:tensorflow:loss = 0.008975061, step = 1601 (11.862 sec)INFO:tensorflow:global_step/sec: 8.46893INFO:tensorflow:loss = 0.011308375, step = 1701 (11.807 sec)INFO:tensorflow:global_step/sec: 8.34723INFO:tensorflow:loss = 0.007505517, step = 1801 (11.980 sec)INFO:tensorflow:global_step/sec: 8.38929INFO:tensorflow:loss = 0.021354698, step = 1901 (11.920 sec)INFO:tensorflow:Saving checkpoints for 2000 into C:\Users\xywang\AppData\Local\Temp\tmp8i1k3w75\model.ckpt.INFO:tensorflow:Loss for final step: 0.011493968.tensorflow.python.estimator.estimator.Estimator at 0x1f84570c710</code></pre><h4 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate the Model</span></span><br><span class="line"><span class="comment"># Define the input function for evaluating</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: mnist.test.images&#125;, y=mnist.test.labels,</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># Use the Estimator 'evaluate' method</span></span><br><span class="line">model.evaluate(input_fn)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Starting evaluation at 2018-04-11-09:41:50INFO:tensorflow:Graph was finalized.INFO:tensorflow:Restoring parameters from C:\Users\xywang\AppData\Local\Temp\tmp8i1k3w75\model.ckpt-2000INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.INFO:tensorflow:Finished evaluation at 2018-04-11-09:41:53INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9868, global_step = 2000, loss = 0.043212146{&apos;accuracy&apos;: 0.9868, &apos;global_step&apos;: 2000, &apos;loss&apos;: 0.043212146}</code></pre><h4 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predict single images</span></span><br><span class="line">n_images = <span class="number">1</span></span><br><span class="line"><span class="comment"># Get images from test set</span></span><br><span class="line">test_images = mnist.test.images[:n_images]</span><br><span class="line"><span class="comment"># Prepare the input data</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: test_images&#125;, shuffle=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># Use the model to predict the images class</span></span><br><span class="line">preds = list(model.predict(input_fn))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_images):</span><br><span class="line">    plt.imshow(np.reshape(test_images[i], [<span class="number">28</span>, <span class="number">28</span>]), cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"Model prediction:"</span>, preds[i])</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Graph was finalized.INFO:tensorflow:Restoring parameters from C:\Users\xywang\AppData\Local\Temp\tmp8i1k3w75\model.ckpt-2000INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.</code></pre><p><img src="http://xukeqiniu.xukeai.cn/tf_7output_21_1.png" alt=""></p><pre><code>Model prediction: 7</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/u014365862/article/details/77833481" target="_blank" rel="noopener">tf API 研读1：tf.nn，tf.layers， tf.contrib概述</a></p><p>[2] <a href="https://cuiqingcai.com/5715.html" target="_blank" rel="noopener">TensorFlow layers模块用法</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Tensorflow卷积神经网络实现&quot;&gt;&lt;a href=&quot;#Tensorflow卷积神经网络实现&quot; class=&quot;headerlink&quot; title=&quot;Tensorflow卷积神经网络实现&quot;&gt;&lt;/a&gt;Tensorflow卷积神经网络实现&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; __future__ &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; division, print_function, absolute_import&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tensorflow &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; tf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;导入数据集&quot;&gt;&lt;a href=&quot;#导入数据集&quot; class=&quot;headerlink&quot; title=&quot;导入数据集&quot;&gt;&lt;/a&gt;导入数据集&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Import MNIST data&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; tensorflow.examples.tutorials.mnist &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; input_data&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mnist = input_data.read_data_sets(&lt;span class=&quot;string&quot;&gt;&quot;./data/&quot;&lt;/span&gt;, one_hot=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;pre&gt;&lt;code&gt;Extracting ./data/train-images-idx3-ubyte.gz
Extracting ./data/train-labels-idx1-ubyte.gz
Extracting ./data/t10k-images-idx3-ubyte.gz
Extracting ./data/t10k-labels-idx1-ubyte.gz
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow神经网络之多层感知机Eager API</title>
    <link href="http://xywang93.github.io.git/2018/03/08/DeepLearning/TensorFlow/08Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BAEager%20API/"/>
    <id>http://xywang93.github.io.git/2018/03/08/DeepLearning/TensorFlow/08Tensorflow神经网络之多层感知机Eager API/</id>
    <published>2018-03-07T16:00:00.000Z</published>
    <updated>2018-07-30T14:51:38.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Tensorflow多层感知机Eager-API"><a href="#Tensorflow多层感知机Eager-API" class="headerlink" title="Tensorflow多层感知机Eager API"></a>Tensorflow多层感知机Eager API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br></pre></td></tr></table></figure><h4 id="设置-Eager-API"><a href="#设置-Eager-API" class="headerlink" title="设置 Eager API"></a>设置 Eager API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set Eager API</span></span><br><span class="line">tfe.enable_eager_execution()</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="定义参数"><a href="#定义参数" class="headerlink" title="定义参数"></a>定义参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span> <span class="comment"># 1st layer number of neurons</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span> <span class="comment"># 2nd layer number of neurons</span></span><br><span class="line">num_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br></pre></td></tr></table></figure><h4 id="数据拆分成批"><a href="#数据拆分成批" class="headerlink" title="数据拆分成批"></a>数据拆分成批</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Using TF Dataset to split data into batches</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (mnist.train.images, mnist.train.labels)).batch(batch_size)</span><br><span class="line">dataset_iter = tfe.Iterator(dataset)</span><br></pre></td></tr></table></figure><h4 id="定义多层感知机模型"><a href="#定义多层感知机模型" class="headerlink" title="定义多层感知机模型"></a>定义多层感知机模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the neural network. To use eager API and tf.layers API together,</span></span><br><span class="line"><span class="comment"># we must instantiate a tfe.Network class as follow:</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span><span class="params">(tfe.Network)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Define each layer</span></span><br><span class="line">        super(NeuralNet, self).__init__()</span><br><span class="line">        <span class="comment"># Hidden fully connected layer with 256 neurons</span></span><br><span class="line">        self.layer1 = self.track_layer(</span><br><span class="line">            tf.layers.Dense(n_hidden_1, activation=tf.nn.relu))</span><br><span class="line">        <span class="comment"># Hidden fully connected layer with 256 neurons</span></span><br><span class="line">        self.layer2 = self.track_layer(</span><br><span class="line">            tf.layers.Dense(n_hidden_2, activation=tf.nn.relu))</span><br><span class="line">        <span class="comment"># Output fully connected layer with a neuron for each class</span></span><br><span class="line">        self.out_layer = self.track_layer(tf.layers.Dense(num_classes))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        <span class="keyword">return</span> self.out_layer(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">neural_net = NeuralNet()</span><br></pre></td></tr></table></figure><h4 id="定义损失函数-优化方法-准确率"><a href="#定义损失函数-优化方法-准确率" class="headerlink" title="定义损失函数+优化方法+准确率"></a>定义损失函数+优化方法+准确率</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Cross-Entropy loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(inference_fn, inputs, labels)</span>:</span></span><br><span class="line">    <span class="comment"># Using sparse_softmax cross entropy</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=inference_fn(inputs), labels=labels))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(inference_fn, inputs, labels)</span>:</span></span><br><span class="line">    prediction = tf.nn.softmax(inference_fn(inputs))</span><br><span class="line">    correct_pred = tf.equal(tf.argmax(prediction, <span class="number">1</span>), labels)</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># SGD Optimizer</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute gradients</span></span><br><span class="line">grad = tfe.implicit_gradients(loss_fn)</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training</span></span><br><span class="line">average_loss = <span class="number">0.</span></span><br><span class="line">average_acc = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Iterate through the dataset</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        d = dataset_iter.next()</span><br><span class="line">    <span class="keyword">except</span> StopIteration:</span><br><span class="line">        <span class="comment"># Refill queue</span></span><br><span class="line">        dataset_iter = tfe.Iterator(dataset)</span><br><span class="line">        d = dataset_iter.next()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Images</span></span><br><span class="line">    x_batch = d[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Labels</span></span><br><span class="line">    y_batch = tf.cast(d[<span class="number">1</span>], dtype=tf.int64)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the batch loss</span></span><br><span class="line">    batch_loss = loss_fn(neural_net, x_batch, y_batch)</span><br><span class="line">    average_loss += batch_loss</span><br><span class="line">    <span class="comment"># Compute the batch accuracy</span></span><br><span class="line">    batch_accuracy = accuracy_fn(neural_net, x_batch, y_batch)</span><br><span class="line">    average_acc += batch_accuracy</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Display the initial cost, before optimizing</span></span><br><span class="line">        print(<span class="string">"Initial loss= &#123;:.9f&#125;"</span>.format(average_loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the variables following gradients info</span></span><br><span class="line">    optimizer.apply_gradients(grad(neural_net, x_batch, y_batch))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display info</span></span><br><span class="line">    <span class="keyword">if</span> (step + <span class="number">1</span>) % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">            average_loss /= display_step</span><br><span class="line">            average_acc /= display_step</span><br><span class="line">        print(<span class="string">"Step:"</span>, <span class="string">'%04d'</span> % (step + <span class="number">1</span>), <span class="string">" loss="</span>,</span><br><span class="line">              <span class="string">"&#123;:.9f&#125;"</span>.format(average_loss), <span class="string">" accuracy="</span>,</span><br><span class="line">              <span class="string">"&#123;:.4f&#125;"</span>.format(average_acc))</span><br><span class="line">        average_loss = <span class="number">0.</span></span><br><span class="line">        average_acc = <span class="number">0.</span></span><br></pre></td></tr></table></figure><hr><pre><code>Initial loss= 2.362281322Step: 0001  loss= 2.362281322  accuracy= 0.0391Step: 0100  loss= 0.583163500  accuracy= 0.8291Step: 0200  loss= 0.247603565  accuracy= 0.9281Step: 0300  loss= 0.214451462  accuracy= 0.9360Step: 0400  loss= 0.182251021  accuracy= 0.9452Step: 0500  loss= 0.139149994  accuracy= 0.9585Step: 0600  loss= 0.120601922  accuracy= 0.9649Step: 0700  loss= 0.114957660  accuracy= 0.9655Step: 0800  loss= 0.111238368  accuracy= 0.9660Step: 0900  loss= 0.085549861  accuracy= 0.9754Step: 1000  loss= 0.079464689  accuracy= 0.9752</code></pre><h4 id="测试评估"><a href="#测试评估" class="headerlink" title="测试评估"></a>测试评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate model on the test image set</span></span><br><span class="line">testX = mnist.test.images</span><br><span class="line">testY = mnist.test.labels</span><br><span class="line"></span><br><span class="line">test_acc = accuracy_fn(neural_net, testX, testY)</span><br><span class="line">print(<span class="string">"Testset Accuracy: &#123;:.4f&#125;"</span>.format(test_acc))</span><br></pre></td></tr></table></figure><hr><pre><code>Testset Accuracy: 0.9707</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">TensorFlow-Examples</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Tensorflow多层感知机Eager-API&quot;&gt;&lt;a href=&quot;#Tensorflow多层感知机Eager-API&quot; class=&quot;headerlink&quot; title=&quot;Tensorflow多层感知机Eager API&quot;&gt;&lt;/a&gt;Tensorflow多层感知机Eager API&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; __future__ &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; print_function&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tensorflow &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; tf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tensorflow.contrib.eager &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; tfe&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;设置-Eager-API&quot;&gt;&lt;a href=&quot;#设置-Eager-API&quot; class=&quot;headerlink&quot; title=&quot;设置 Eager API&quot;&gt;&lt;/a&gt;设置 Eager API&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Set Eager API&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tfe.enable_eager_execution()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow神经网络之多层感知机</title>
    <link href="http://xywang93.github.io.git/2018/03/07/DeepLearning/TensorFlow/07Tensorflow%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>http://xywang93.github.io.git/2018/03/07/DeepLearning/TensorFlow/07Tensorflow神经网络之多层感知机/</id>
    <published>2018-03-06T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:38.413Z</updated>
    
    <content type="html"><![CDATA[<h3 id="多层感知机简介"><a href="#多层感知机简介" class="headerlink" title="多层感知机简介"></a>多层感知机简介</h3><h5 id="多层感知机模型"><a href="#多层感知机模型" class="headerlink" title="多层感知机模型"></a>多层感知机模型</h5><p><img src="http://xukeqiniu.xukeai.cn/0e39fd772bfa170145f493cacadab129.png" alt=""></p><p>这里定义含有两个隐含层的模型，隐含层输出均为256个节点，输入784（MNIST数据集图片大小28*28），输出10。</p><a id="more"></a><h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><p>比较常用的是 ReLU：relu(x)=max(x,0)，本例中没有加激活函数。</p><h5 id="softmax（同前面的logistic回归）"><a href="#softmax（同前面的logistic回归）" class="headerlink" title="softmax（同前面的logistic回归）"></a>softmax（同前面的logistic回归）</h5><h5 id="损失函数：交叉熵"><a href="#损失函数：交叉熵" class="headerlink" title="损失函数：交叉熵"></a>损失函数：交叉熵</h5><h3 id="Tensorflow实现多层感知机"><a href="#Tensorflow实现多层感知机" class="headerlink" title="Tensorflow实现多层感知机"></a>Tensorflow实现多层感知机</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">display_step = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line">n_hidden_1 = <span class="number">256</span> <span class="comment"># 1st layer number of neurons</span></span><br><span class="line">n_hidden_2 = <span class="number">256</span> <span class="comment"># 2nd layer number of neurons</span></span><br><span class="line">num_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></span><br></pre></td></tr></table></figure><h4 id="定义多层感知机模型"><a href="#定义多层感知机模型" class="headerlink" title="定义多层感知机模型"></a>定义多层感知机模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the neural network</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neural_net</span><span class="params">(x_dict)</span>:</span></span><br><span class="line">    <span class="comment"># TF Estimator input is a dict, in case of multiple inputs</span></span><br><span class="line">    x = x_dict[<span class="string">'images'</span>]</span><br><span class="line">    <span class="comment"># Hidden fully connected layer with 256 neurons</span></span><br><span class="line">    layer_1 = tf.layers.dense(x, n_hidden_1)</span><br><span class="line">    <span class="comment"># Hidden fully connected layer with 256 neurons</span></span><br><span class="line">    layer_2 = tf.layers.dense(layer_1, n_hidden_2)</span><br><span class="line">    <span class="comment"># Output fully connected layer with a neuron for each class</span></span><br><span class="line">    out_layer = tf.layers.dense(layer_2, num_classes)</span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br></pre></td></tr></table></figure><blockquote><p><strong>补充：dense</strong> [^2]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;dense(</span><br><span class="line">&gt;    inputs,</span><br><span class="line">&gt;    units,</span><br><span class="line">&gt;    activation=<span class="keyword">None</span>,</span><br><span class="line">&gt;    use_bias=<span class="keyword">True</span>,</span><br><span class="line">&gt;    kernel_initializer=<span class="keyword">None</span>,</span><br><span class="line">&gt;    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">&gt;    kernel_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;    bias_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;    activity_regularizer=<span class="keyword">None</span>,</span><br><span class="line">&gt;    kernel_constraint=<span class="keyword">None</span>,</span><br><span class="line">&gt;    bias_constraint=<span class="keyword">None</span>,</span><br><span class="line">&gt;    trainable=<span class="keyword">True</span>,</span><br><span class="line">&gt;    name=<span class="keyword">None</span>,</span><br><span class="line">&gt;    reuse=<span class="keyword">None</span></span><br><span class="line">&gt;)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p><p>参数说明如下：</p><ul><li>inputs：必需，即需要进行操作的输入数据。</li><li>units：必须，即神经元的数量。</li><li>activation：可选，默认为 None，如果为 None 则是线性激&gt;活。</li><li>use_bias：可选，默认为 True，是否使用偏置。</li><li>kernel_initializer：可选，默认为 None，即权重的初始化&gt;方法，如果为 None，则使用默认的 Xavier 初始化方法。</li><li>bias_initializer：可选，默认为零值初始化，即偏置的初始&gt;化方法。</li><li>kernel_regularizer：可选，默认为 None，施加在权重上的&gt;正则项。</li><li>bias_regularizer：可选，默认为 None，施加在偏置上的正&gt;则项。</li><li>activity_regularizer：可选，默认为 None，施加在输出上&gt;的正则项。</li><li>kernel_constraint，可选，默认为 None，施加在权重上的&gt;约束项。</li><li>bias_constraint，可选，默认为 None，施加在偏置上的约束&gt;项。</li><li>trainable：可选，默认为 True，布尔类型，如果为 True，&gt;则将变量添加到 GraphKeys.TRAINABLE_VARIABLES 中。</li><li>name：可选，默认为 None，卷积层的名称。</li><li>reuse：可选，默认为 None，布尔类型，如果为 True，那么&gt;如果 name 相同时，会重复利用。</li><li>返回值： 全连接网络处理后的 Tensor。</li></ul><p><strong>上面的代码中，第三个参数为空，所以这里采用线性激活</strong></p></blockquote><h4 id="定义模型函数"><a href="#定义模型函数" class="headerlink" title="定义模型函数"></a>定义模型函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the model function (following TF Estimator Template)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the neural network</span></span><br><span class="line">    logits = neural_net(features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predictions</span></span><br><span class="line">    pred_classes = tf.argmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">    pred_probas = tf.nn.softmax(logits)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If prediction mode, early return</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, predictions=pred_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define loss and optimizer</span></span><br><span class="line">    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Evaluate the accuracy of the model</span></span><br><span class="line">    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># TF Estimators requires to return a EstimatorSpec, that specify</span></span><br><span class="line">    <span class="comment"># the different ops for training, evaluating, ...</span></span><br><span class="line">    estim_specs = tf.estimator.EstimatorSpec(</span><br><span class="line">      mode=mode,</span><br><span class="line">      predictions=pred_classes,</span><br><span class="line">      loss=loss_op,</span><br><span class="line">      train_op=train_op,</span><br><span class="line">      eval_metric_ops=&#123;<span class="string">'accuracy'</span>: acc_op&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> estim_specs</span><br></pre></td></tr></table></figure><h4 id="构建评估器"><a href="#构建评估器" class="headerlink" title="构建评估器"></a>构建评估器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the Estimator</span></span><br><span class="line">model = tf.estimator.Estimator(model_fn)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Using default config.WARNING:tensorflow:Using temporary folder as model directory: C:\Users\xywang\AppData\Local\Temp\tmp995gddibINFO:tensorflow:Using config: {&apos;_model_dir&apos;: &apos;C:\\Users\\xywang\\AppData\\Local\\Temp\\tmp995gddib&apos;, &apos;_tf_random_seed&apos;: None, &apos;_save_summary_steps&apos;: 100, &apos;_save_checkpoints_steps&apos;: None, &apos;_save_checkpoints_secs&apos;: 600, &apos;_session_config&apos;: None, &apos;_keep_checkpoint_max&apos;: 5, &apos;_keep_checkpoint_every_n_hours&apos;: 10000, &apos;_log_step_count_steps&apos;: 100, &apos;_service&apos;: None, &apos;_cluster_spec&apos;: &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x0000024BBEE7A1D0&gt;, &apos;_task_type&apos;: &apos;worker&apos;, &apos;_task_id&apos;: 0, &apos;_global_id_in_cluster&apos;: 0, &apos;_master&apos;: &apos;&apos;, &apos;_evaluation_master&apos;: &apos;&apos;, &apos;_is_chief&apos;: True, &apos;_num_ps_replicas&apos;: 0, &apos;_num_worker_replicas&apos;: 1}</code></pre><blockquote><p><strong>补充：tf.estimator</strong><br>tf.estimator就是已经预定义好的模型，自带train，evaluate，predict。<br>其编程范式为：</p><ul><li>定义算法模型，比如多层感知机，CNN；</li><li>定义模型函数（model_fn），包括构建graph，定义损失函数、优化器，估计准确率等，返回结果分训练和测试两种情况；</li><li><p>构建评估器；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; model = tf.estimator.Estimator(model_fn)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li><li><p>用 tf.estimator.inputs.numpy_input_fn 把 input_fn 传入 model，就可调用 model.train, model.evaluate, model.predict 了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; input_fn = tf.estimator.inputs.numpy_input_fn()</span><br><span class="line">&gt; model.train(input_fn)</span><br><span class="line">&gt; model.evaluate(input_fn)</span><br><span class="line">&gt; model.predict(input_fn)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li></ul><p>Estimator 是一种更高层次的封装，它把一些基本算法的算法&gt;模型和模型函数预定义好，你只需要传入参数即可。</p><h5 id="input-fn-1"><a href="#input-fn-1" class="headerlink" title="input_fn [^1]"></a>input_fn [^1]</h5><p>一般来讲，input_fn方法做两件事：</p><p>1.数据预处理，如洗脏数据，归整数据等。没有就空着。</p><p>2.返回feature_cols, labels。</p><ul><li>feature_cols：一个dict，key为feature名，value&gt;为feature值。</li><li>lables: 对应的分类标签。</li></ul><p>可以将多种对象转换为tensorflow对象，常见的为将Numpy&gt;转tensorflow对象。比如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">&gt; <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt; <span class="comment">#numpy input_fn.</span></span><br><span class="line">&gt; x_data =[&#123;<span class="string">"feature1"</span>: <span class="number">2</span>, <span class="string">"features2"</span>:<span class="number">6</span>&#125;,</span><br><span class="line">&gt;          &#123;<span class="string">"feature1"</span>: <span class="number">1</span>, <span class="string">"features2"</span>:<span class="number">5</span>&#125;,</span><br><span class="line">&gt;          &#123;<span class="string">"feature1"</span>: <span class="number">4</span>, <span class="string">"features2"</span>:<span class="number">8</span>&#125;]</span><br><span class="line">&gt; y_data = [<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">&gt; my_input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">&gt;     x=&#123;<span class="string">"x"</span>: np.array(x_data)&#125;,</span><br><span class="line">&gt;     y=np.array(y_data),</span><br><span class="line">&gt;     shuffle = <span class="keyword">True</span>)</span><br><span class="line">&gt; <span class="comment">#得到的是一个名为my_input_fn的function对象。</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p></blockquote><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define the input function for training</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: mnist.train.images&#125;, y=mnist.train.labels,</span><br><span class="line">    batch_size=batch_size, num_epochs=<span class="keyword">None</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Train the Model</span></span><br><span class="line">model.train(input_fn, steps=num_steps)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Create CheckpointSaverHook.INFO:tensorflow:Graph was finalized.INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.INFO:tensorflow:Saving checkpoints for 1 into C:\Users\xywang\AppData\Local\Temp\tmp995gddib\model.ckpt.INFO:tensorflow:loss = 2.4631722, step = 1INFO:tensorflow:global_step/sec: 248.501INFO:tensorflow:loss = 0.42414927, step = 101 (0.388 sec)INFO:tensorflow:global_step/sec: 301.862INFO:tensorflow:loss = 0.48539022, step = 201 (0.331 sec)INFO:tensorflow:global_step/sec: 310.3INFO:tensorflow:loss = 0.2779913, step = 301 (0.323 sec)INFO:tensorflow:global_step/sec: 303.697INFO:tensorflow:loss = 0.20052063, step = 401 (0.329 sec)INFO:tensorflow:global_step/sec: 322.311INFO:tensorflow:loss = 0.5092098, step = 501 (0.309 sec)INFO:tensorflow:global_step/sec: 337.555INFO:tensorflow:loss = 0.28386787, step = 601 (0.297 sec)INFO:tensorflow:global_step/sec: 322.309INFO:tensorflow:loss = 0.36957514, step = 701 (0.309 sec)INFO:tensorflow:global_step/sec: 334.17INFO:tensorflow:loss = 0.28504127, step = 801 (0.300 sec)INFO:tensorflow:global_step/sec: 319.222INFO:tensorflow:loss = 0.37339848, step = 901 (0.312 sec)INFO:tensorflow:Saving checkpoints for 1000 into C:\Users\xywang\AppData\Local\Temp\tmp995gddib\model.ckpt.INFO:tensorflow:Loss for final step: 0.2043538.tensorflow.python.estimator.estimator.Estimator at 0x24bbee72160</code></pre><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate the Model</span></span><br><span class="line"><span class="comment"># Define the input function for evaluating</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: mnist.test.images&#125;, y=mnist.test.labels,</span><br><span class="line">    batch_size=batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># Use the Estimator 'evaluate' method</span></span><br><span class="line">model.evaluate(input_fn)</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Starting evaluation at 2018-04-11-08:04:38INFO:tensorflow:Graph was finalized.INFO:tensorflow:Restoring parameters from C:\Users\xywang\AppData\Local\Temp\tmp995gddib\model.ckpt-1000INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.INFO:tensorflow:Finished evaluation at 2018-04-11-08:04:38INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9149, global_step = 1000, loss = 0.29386005{&apos;accuracy&apos;: 0.9149, &apos;global_step&apos;: 1000, &apos;loss&apos;: 0.29386005}</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predict single images</span></span><br><span class="line">n_images = <span class="number">1</span></span><br><span class="line"><span class="comment"># Get images from test set</span></span><br><span class="line">test_images = mnist.test.images[:n_images]</span><br><span class="line"><span class="comment"># Prepare the input data</span></span><br><span class="line">input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;<span class="string">'images'</span>: test_images&#125;, shuffle=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># Use the model to predict the images class</span></span><br><span class="line">preds = list(model.predict(input_fn))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_images):</span><br><span class="line">    plt.imshow(np.reshape(test_images[i], [<span class="number">28</span>, <span class="number">28</span>]), cmap=<span class="string">'gray'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"Model prediction:"</span>, preds[i])</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Calling model_fn.INFO:tensorflow:Done calling model_fn.INFO:tensorflow:Graph was finalized.INFO:tensorflow:Restoring parameters from C:\Users\xywang\AppData\Local\Temp\tmp995gddib\model.ckpt-1000INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.</code></pre><p><img src="http://xukeqiniu.xukeai.cn/tf_output_21_1.png" alt=""></p><pre><code>Model prediction: 7</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/vagrantabc2017/article/details/77482891" target="_blank" rel="noopener">在tf.estimator中构建inpu_fn解读</a></p><p>[2] <a href="https://cuiqingcai.com/5715.html" target="_blank" rel="noopener">TensorFlow layers模块用法</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;多层感知机简介&quot;&gt;&lt;a href=&quot;#多层感知机简介&quot; class=&quot;headerlink&quot; title=&quot;多层感知机简介&quot;&gt;&lt;/a&gt;多层感知机简介&lt;/h3&gt;&lt;h5 id=&quot;多层感知机模型&quot;&gt;&lt;a href=&quot;#多层感知机模型&quot; class=&quot;headerlink&quot; title=&quot;多层感知机模型&quot;&gt;&lt;/a&gt;多层感知机模型&lt;/h5&gt;&lt;p&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/0e39fd772bfa170145f493cacadab129.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;这里定义含有两个隐含层的模型，隐含层输出均为256个节点，输入784（MNIST数据集图片大小28*28），输出10。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow基本模型之K-means</title>
    <link href="http://xywang93.github.io.git/2018/03/06/DeepLearning/TensorFlow/06Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8BK-means/"/>
    <id>http://xywang93.github.io.git/2018/03/06/DeepLearning/TensorFlow/06Tensorflow基本模型之K-means/</id>
    <published>2018-03-05T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:24.369Z</updated>
    
    <content type="html"><![CDATA[<h3 id="K-Means算法简介"><a href="#K-Means算法简介" class="headerlink" title="K-Means算法简介"></a>K-Means算法简介</h3><p><code>K-MEANS</code>算法是输入聚类个数<code>k</code>，以及包含 <code>n</code>个数据对象的数据库，输出满足方差最小标准<code>k</code>个聚类的一种算法。属于一种经典的无监督学习算法。<br>示意图如下所示：<br><img src="http://xukeqiniu.xukeai.cn/111fa6ed0ddea2c909efa2d1094e5035.png" alt="K-Means算法示意图"><br><code>k-means</code> 算法接受输入量 <code>k</code> ；然后将<code>n</code>个数据对象划分为 <code>k</code>个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。聚类相似度是利用各聚类中对象的均值所获得一个“中心对象”（引力中心）来进行计算的。</p><p>基本步骤：</p><p>（1） 从 n个数据对象任意选择 k 个对象作为初始聚类中心；</p><p>（2） 根据每个聚类对象的均值（中心对象），计算每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分；</p><p>（3） 重新计算每个（有变化）聚类的均值（中心对象）；</p><p>（4） 计算标准测度函数，当满足一定条件，如函数收敛时，则算法终止；如果条件不满足则回到步骤（2）。</p><a id="more"></a><h3 id="TensorFlow的K-Means实现"><a href="#TensorFlow的K-Means实现" class="headerlink" title="TensorFlow的K-Means实现"></a>TensorFlow的K-Means实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.factorization <span class="keyword">import</span> KMeans<span class="comment">#导入KMeans函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ignore all GPUs, tf random forest does not benefit from it.</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">""</span></span><br></pre></td></tr></table></figure><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">full_data_x = mnist.train.images</span><br></pre></td></tr></table></figure><hr><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">num_steps = <span class="number">50</span> <span class="comment"># Total steps to train</span></span><br><span class="line">batch_size = <span class="number">1024</span> <span class="comment"># The number of samples per batch</span></span><br><span class="line">k = <span class="number">25</span> <span class="comment"># The number of clusters</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># The 10 digits</span></span><br><span class="line">num_features = <span class="number">784</span> <span class="comment"># Each image is 28x28 pixels</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input images</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_features])</span><br><span class="line"><span class="comment"># Labels (for assigning a label to a centroid and testing)</span></span><br><span class="line">Y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_classes])</span><br><span class="line"></span><br><span class="line"><span class="comment"># K-Means Parameters</span></span><br><span class="line"><span class="comment"># 距离度量的方式采用余弦距离（余弦相似度）</span></span><br><span class="line">kmeans = KMeans(inputs=X, num_clusters=k, distance_metric=<span class="string">'cosine'</span>,</span><br><span class="line">                use_mini_batch=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h4 id="构建K-means图模型"><a href="#构建K-means图模型" class="headerlink" title="构建K-means图模型"></a>构建K-means图模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build KMeans graph</span></span><br><span class="line">(all_scores, cluster_idx, scores, cluster_centers_initialized,init_op,train_op) = kmeans.training_graph()</span><br><span class="line">cluster_idx = cluster_idx[<span class="number">0</span>] <span class="comment"># fix for cluster_idx being a tuple</span></span><br><span class="line">avg_distance = tf.reduce_mean(scores)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init_vars = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start TensorFlow session</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the initializer</span></span><br><span class="line">sess.run(init_vars, feed_dict=&#123;X: full_data_x&#125;)</span><br><span class="line">sess.run(init_op, feed_dict=&#123;X: full_data_x&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_steps + <span class="number">1</span>):</span><br><span class="line">    _, d, idx = sess.run([train_op, avg_distance, cluster_idx],</span><br><span class="line">                         feed_dict=&#123;X: full_data_x&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">1</span>:</span><br><span class="line">        print(<span class="string">"Step %i, Avg Distance: %f"</span> % (i, d))</span><br></pre></td></tr></table></figure><hr><pre><code>Step 1, Avg Distance: 0.341471Step 10, Avg Distance: 0.221609Step 20, Avg Distance: 0.220328Step 30, Avg Distance: 0.219776Step 40, Avg Distance: 0.219419Step 50, Avg Distance: 0.219154</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assign a label to each centroid</span></span><br><span class="line"><span class="comment"># Count total number of labels per centroid, using the label of each training</span></span><br><span class="line"><span class="comment"># sample to their closest centroid (given by 'idx')</span></span><br><span class="line">counts = np.zeros(shape=(k, num_classes))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(idx)):</span><br><span class="line">    counts[idx[i]] += mnist.train.labels[i]</span><br><span class="line"><span class="comment"># Assign the most frequent label to the centroid</span></span><br><span class="line">labels_map = [np.argmax(c) <span class="keyword">for</span> c <span class="keyword">in</span> counts]</span><br><span class="line">labels_map = tf.convert_to_tensor(labels_map)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluation ops</span></span><br><span class="line"><span class="comment"># Lookup: centroid_id -&gt; label</span></span><br><span class="line">cluster_label = tf.nn.embedding_lookup(labels_map, cluster_idx)</span><br><span class="line"><span class="comment"># Compute accuracy</span></span><br><span class="line">correct_prediction = tf.equal(cluster_label, tf.cast(tf.argmax(Y, <span class="number">1</span>), tf.int32))</span><br><span class="line">accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Model</span></span><br><span class="line">test_x, test_y = mnist.test.images, mnist.test.labels</span><br><span class="line">print(<span class="string">"Test Accuracy:"</span>, sess.run(accuracy_op, feed_dict=&#123;X: test_x, Y: test_y&#125;))</span><br></pre></td></tr></table></figure><hr><pre><code>Test Accuracy: 0.7127</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://baike.baidu.com/item/K-MEANS算法/594631?fr=aladdin" target="_blank" rel="noopener">百度百科——K-MEANS算法</a></p><p><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">TensorFlow-Examples</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;K-Means算法简介&quot;&gt;&lt;a href=&quot;#K-Means算法简介&quot; class=&quot;headerlink&quot; title=&quot;K-Means算法简介&quot;&gt;&lt;/a&gt;K-Means算法简介&lt;/h3&gt;&lt;p&gt;&lt;code&gt;K-MEANS&lt;/code&gt;算法是输入聚类个数&lt;code&gt;k&lt;/code&gt;，以及包含 &lt;code&gt;n&lt;/code&gt;个数据对象的数据库，输出满足方差最小标准&lt;code&gt;k&lt;/code&gt;个聚类的一种算法。属于一种经典的无监督学习算法。&lt;br&gt;示意图如下所示：&lt;br&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/111fa6ed0ddea2c909efa2d1094e5035.png&quot; alt=&quot;K-Means算法示意图&quot;&gt;&lt;br&gt;&lt;code&gt;k-means&lt;/code&gt; 算法接受输入量 &lt;code&gt;k&lt;/code&gt; ；然后将&lt;code&gt;n&lt;/code&gt;个数据对象划分为 &lt;code&gt;k&lt;/code&gt;个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。聚类相似度是利用各聚类中对象的均值所获得一个“中心对象”（引力中心）来进行计算的。&lt;/p&gt;
&lt;p&gt;基本步骤：&lt;/p&gt;
&lt;p&gt;（1） 从 n个数据对象任意选择 k 个对象作为初始聚类中心；&lt;/p&gt;
&lt;p&gt;（2） 根据每个聚类对象的均值（中心对象），计算每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分；&lt;/p&gt;
&lt;p&gt;（3） 重新计算每个（有变化）聚类的均值（中心对象）；&lt;/p&gt;
&lt;p&gt;（4） 计算标准测度函数，当满足一定条件，如函数收敛时，则算法终止；如果条件不满足则回到步骤（2）。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow基本模型之随机森林</title>
    <link href="http://xywang93.github.io.git/2018/03/05/DeepLearning/TensorFlow/05Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <id>http://xywang93.github.io.git/2018/03/05/DeepLearning/TensorFlow/05Tensorflow基本模型之随机森林/</id>
    <published>2018-03-04T16:00:00.000Z</published>
    <updated>2018-07-30T14:51:12.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="随机森林简介"><a href="#随机森林简介" class="headerlink" title="随机森林简介"></a>随机森林简介</h3><p>随机森林是一种集成学习方法。训练时每个树分类器从样本集里面随机有放回的抽取一部分进行训练。预测时将要分类的样本带入一个个树分类器，然后以少数服从多数的原则，表决出这个样本的最终分类类型。[^4]</p><p>设有N个样本，M个变量(维度)个数，该算法具体流程如下：</p><ol><li>确定一个值m，它用来表示每个树分类器选取多少个变量；</li><li>从数据集中有放回的抽取 k 个样本集，用它们创建 k 个树分类器。另外还伴随生成了 k 个袋外数据，用来后面做检测。</li><li>输入待分类样本之后，每个树分类器都会对它进行分类，然后所有分类器按照少数服从多数原则，确定分类结果。</li></ol><p>重要参数：</p><ol><li>预选变量个数 (即框架流程中的m)；</li><li>随机森林中树的个数。</li></ol><a id="more"></a><h3 id="Tensorflow-随机森林"><a href="#Tensorflow-随机森林" class="headerlink" title="Tensorflow 随机森林"></a>Tensorflow 随机森林</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.ops <span class="keyword">import</span> resources</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.tensor_forest.python <span class="keyword">import</span> tensor_forest</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ignore all GPUs, tf random forest does not benefit from it.</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">""</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>补充：__futrure__</strong>[^1]<br>简单来说，Python 的每个新版本都会增加一些新的功能，或者对原来的功能作一些改动。有些改动是不兼容旧版本的。从 Python 2.7 到 Python 3 就有不兼容的一些改动，如果你想在 Python 2.7 中使用 Python 3 的新特性，那么你就需要从__future__模块导入。</p><ul><li>division<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span>/<span class="number">3</span> = <span class="number">3</span>  <span class="comment"># python2.7中，不导入__future__</span></span><br><span class="line"><span class="number">10</span>/<span class="number">3</span> = <span class="number">3.3333333333333335</span>  <span class="comment"># python2.7中，导入__future__</span></span><br></pre></td></tr></table></figure></li></ul><p>很容易看出来，2.7中默认的整数除法是结果向下取整，而导入了<strong>future</strong>之后除法就是真正的除法了。这也是python2和python3的一个重要区别。</p><ul><li>absolute_import<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python2.7中，在默认情况下，导入模块是相对导入的（relative import），比如说</span></span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> .json <span class="keyword">import</span> json_dump</span><br></pre></td></tr></table></figure></li></ul><p>这些以’.’点导入的是相对导入，而绝对导入（absolute import）则是指从系统路径sys.path最底层的模块导入。比如:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> sys</span><br></pre></td></tr></table></figure></p><ul><li>print_function<br>这个就是最经典的python2和python3的区别了，python2中print不需要括号，而在python3中则需要。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"Hello world"</span> <span class="comment"># python2.7</span></span><br><span class="line">print(<span class="string">"Hello world"</span>) <span class="comment"># python3</span></span><br></pre></td></tr></table></figure></li></ul></blockquote><h4 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">num_steps = <span class="number">500</span> <span class="comment"># Total steps to train</span></span><br><span class="line">batch_size = <span class="number">1024</span> <span class="comment"># The number of samples per batch</span></span><br><span class="line">num_classes = <span class="number">10</span> <span class="comment"># The 10 digits</span></span><br><span class="line">num_features = <span class="number">784</span> <span class="comment"># Each image is 28x28 pixels</span></span><br><span class="line">num_trees = <span class="number">10</span></span><br><span class="line">max_nodes = <span class="number">1000</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>补充：Estimator API</strong><br>Estimator 跟 Dataset 都是 Tensorflow 中的高级API。<br>Estimator（评估器）是一种创建 TensorFlow 模型的高级方法，它包括了用于常见机器学习任务的预制模型，当然，你也可以使用它们来创建你的自定义模型。[^3]<br>contrib.tensor_forest 详细的实现了随机森林算法（Random Forests）评估器，并对外提供 high-level API。你只需传入 params 到构造器，params 使用 params.fill() 来填充，而不用传入所有的超参数，Tensor Forest 自己的 RandomForestGraphs 就能使用这些参数来构建整幅图。[^2]</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Input and Target data</span></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_features])</span><br><span class="line"><span class="comment"># For random forest, labels must be integers (the class id)</span></span><br><span class="line">Y = tf.placeholder(tf.int32, shape=[<span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Random Forest Parameters</span></span><br><span class="line">hparams = tensor_forest.ForestHParams(num_classes=num_classes,</span><br><span class="line">                                      num_features=num_features,</span><br><span class="line">                                      num_trees=num_trees,</span><br><span class="line">                                      max_nodes=max_nodes).fill()</span><br><span class="line"><span class="comment"># Build the Random Forest</span></span><br><span class="line">forest_graph = tensor_forest.RandomForestGraphs(hparams)</span><br></pre></td></tr></table></figure><p>INFO:tensorflow:Constructing forest with params =<br>INFO:tensorflow:{‘num_trees’: 10, ‘max_nodes’: 1000, ‘bagging_fraction’: 1.0, ‘feature_bagging_fraction’: 1.0, ‘num_splits_to_consider’: 28, ‘max_fertile_nodes’: 0, ‘split_after_samples’: 250, ‘valid_leaf_threshold’: 1, ‘dominate_method’: ‘bootstrap’, ‘dominate_fraction’: 0.99, ‘model_name’: ‘all_dense’, ‘split_finish_name’: ‘basic’, ‘split_pruning_name’: ‘none’, ‘collate_examples’: False, ‘checkpoint_stats’: False, ‘use_running_stats_method’: False, ‘initialize_average_splits’: False, ‘inference_tree_paths’: False, ‘param_file’: None, ‘split_name’: ‘less_or_equal’, ‘early_finish_check_every_samples’: 0, ‘prune_every_samples’: 0, ‘num_classes’: 10, ‘num_features’: 784, ‘bagged_num_features’: 784, ‘bagged_features’: None, ‘regression’: False, ‘num_outputs’: 1, ‘num_output_columns’: 11, ‘base_random_seed’: 0, ‘leaf_model_type’: 0, ‘stats_model_type’: 0, ‘finish_type’: 0, ‘pruning_type’: 0, ‘split_type’: 0}</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get training graph and loss</span></span><br><span class="line">train_op = forest_graph.training_graph(X, Y)</span><br><span class="line">loss_op = forest_graph.training_loss(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Measure the accuracy</span></span><br><span class="line">infer_op, _, _ = forest_graph.inference_graph(X)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(infer_op, <span class="number">1</span>), tf.cast(Y, tf.int64))</span><br><span class="line">accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value) and forest resources</span></span><br><span class="line">init_vars = tf.group(tf.global_variables_initializer(),</span><br><span class="line">    resources.initialize_resources(resources.shared_resources()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start TensorFlow session</span></span><br><span class="line">sess = tf.train.MonitoredSession()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the initializer</span></span><br><span class="line">sess.run(init_vars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, num_steps + <span class="number">1</span>):</span><br><span class="line">    <span class="comment"># Prepare Data</span></span><br><span class="line">    <span class="comment"># Get the next batch of MNIST data (only images are needed, not labels)</span></span><br><span class="line">    batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">    _, l = sess.run([train_op, loss_op], feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span> <span class="keyword">or</span> i == <span class="number">1</span>:</span><br><span class="line">        acc = sess.run(accuracy_op, feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">        print(<span class="string">'Step %i, Loss: %f, Acc: %f'</span> % (i, l, acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test Model</span></span><br><span class="line">test_x, test_y = mnist.test.images, mnist.test.labels</span><br><span class="line">print(<span class="string">"Test Accuracy:"</span>, sess.run(accuracy_op, feed_dict=&#123;X: test_x, Y: test_y&#125;))</span><br></pre></td></tr></table></figure><pre><code>INFO:tensorflow:Graph was finalized.INFO:tensorflow:Running local_init_op.INFO:tensorflow:Done running local_init_op.Step 1, Loss: -1.000000, Acc: 0.411133Step 50, Loss: -254.800003, Acc: 0.892578Step 100, Loss: -538.799988, Acc: 0.915039Step 150, Loss: -826.599976, Acc: 0.922852Step 200, Loss: -1001.000000, Acc: 0.926758Step 250, Loss: -1001.000000, Acc: 0.919922Step 300, Loss: -1001.000000, Acc: 0.933594Step 350, Loss: -1001.000000, Acc: 0.916992Step 400, Loss: -1001.000000, Acc: 0.916992Step 450, Loss: -1001.000000, Acc: 0.927734Step 500, Loss: -1001.000000, Acc: 0.917969Test Accuracy: 0.9212</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/langb2014/article/details/53305246#t1" target="_blank" rel="noopener">Python __future__ 模块</a></p><p>[2] <a href="http://www.doc88.com/p-1834979585771.html" target="_blank" rel="noopener">【机器学习】在TensorFlow中构建自定义Estimator：深度解析TensorFlow组件Estimator</a></p><p>[3] <a href="http://www.sohu.com/a/191717118_390227" target="_blank" rel="noopener">TensorFlow 1.3的Datasets和Estimator知多少？谷歌大神来解答</a></p><p>[4] <a href="https://www.cnblogs.com/muchen/p/6883263.html" target="_blank" rel="noopener">穆晨：随机森林(Random Forest)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;随机森林简介&quot;&gt;&lt;a href=&quot;#随机森林简介&quot; class=&quot;headerlink&quot; title=&quot;随机森林简介&quot;&gt;&lt;/a&gt;随机森林简介&lt;/h3&gt;&lt;p&gt;随机森林是一种集成学习方法。训练时每个树分类器从样本集里面随机有放回的抽取一部分进行训练。预测时将要分类的样本带入一个个树分类器，然后以少数服从多数的原则，表决出这个样本的最终分类类型。[^4]&lt;/p&gt;
&lt;p&gt;设有N个样本，M个变量(维度)个数，该算法具体流程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;确定一个值m，它用来表示每个树分类器选取多少个变量；&lt;/li&gt;
&lt;li&gt;从数据集中有放回的抽取 k 个样本集，用它们创建 k 个树分类器。另外还伴随生成了 k 个袋外数据，用来后面做检测。&lt;/li&gt;
&lt;li&gt;输入待分类样本之后，每个树分类器都会对它进行分类，然后所有分类器按照少数服从多数原则，确定分类结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;重要参数：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;预选变量个数 (即框架流程中的m)；&lt;/li&gt;
&lt;li&gt;随机森林中树的个数。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow基本模型之最近邻</title>
    <link href="http://xywang93.github.io.git/2018/03/04/DeepLearning/TensorFlow/04TensorFlow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8B%E6%9C%80%E8%BF%91%E9%82%BB/"/>
    <id>http://xywang93.github.io.git/2018/03/04/DeepLearning/TensorFlow/04TensorFlow基本模型之最近邻/</id>
    <published>2018-03-03T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:38.409Z</updated>
    
    <content type="html"><![CDATA[<h3 id="最近邻算法简介"><a href="#最近邻算法简介" class="headerlink" title="最近邻算法简介"></a>最近邻算法简介</h3><p>k近邻模型的核心就是使用一种距离度量，获得距离目标点最近的k个点，根据分类决策规则，决定目标点的分类。[2]</p><p>距离度量(L1范数)：<br><img src="http://xukeqiniu.xukeai.cn/c795dda5fdeb34d262274efb237e1b3b.png" alt=""></p><p>K值选择：这里k为10。</p><p>分类决策规则：k近邻的分类决策规则是最为常见的简单多数规则，也就是在最近的K个点中，哪个标签数目最多，就把目标点的标签归于哪一类。</p><a id="more"></a><h3 id="Tensorflow-最近邻"><a href="#Tensorflow-最近邻" class="headerlink" title="Tensorflow 最近邻"></a>Tensorflow 最近邻</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h4 id="导入-mnist数据集"><a href="#导入-mnist数据集" class="headerlink" title="导入 mnist数据集"></a>导入 mnist数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MINST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In this example, we limit mnist data</span></span><br><span class="line">Xtr, Ytr = mnist.train.next_batch(<span class="number">5000</span>) <span class="comment">#5000 for training (nn candidates)</span></span><br><span class="line">Xte, Yte = mnist.test.next_batch(<span class="number">10</span>) <span class="comment">#10 for testing</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line">xtr = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">xte = tf.placeholder(<span class="string">"float"</span>, [<span class="number">784</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Nearest Neighbor calculation using L1 Distance</span></span><br><span class="line"><span class="comment"># Calculate L1 Distance</span></span><br><span class="line">distance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Prediction: Get min distance index (Nearest neighbor)</span></span><br><span class="line">pred = tf.argmin(distance, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><blockquote><p><strong>补充：Tenosrflow中基本算术运算函数:</strong>[1]</p><ul><li>tf.add(x,y,name=None) # 求和运算</li><li>tf.subtract(x,y,name=None) # 减法运算</li><li>tf.multiply(x,y,name=None) #乘法运算</li><li>tf.div(x,y,name=None)    #除法运算</li><li>tf.mod(x,y,name=None) # 取模运算</li><li>tf.abs(x,name=None) ＃求绝对值</li><li>tf.negative(x,name=None) ＃取负运算（ｙ＝－ｘ）</li><li>tf.sign(x,name=None) ＃返回符合ｘ大于０，则返回１，小于０，则返回－１</li><li>tf.reciprocal(x,name=None)　＃取反运算</li><li>tf.square(x,name=None)　＃计算平方</li><li>tf.round(x,name=None)　＃舍入最接近的整数</li><li>tf.pow(x,y,name=None)　＃幂次方</li></ul></blockquote><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">accuracy = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over test data</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(Xte)):</span><br><span class="line">        <span class="comment"># Get nearest neighbor</span></span><br><span class="line">        <span class="comment"># 5000个样本点分别和10个测试点计算距离</span></span><br><span class="line">        nn_index = sess.run(pred, feed_dict=&#123;xtr: Xtr, xte: Xte[i, :]&#125;)</span><br><span class="line">        print(nn_index)</span><br><span class="line">        <span class="comment"># Get nearest neighbor class label and compare it to its true label</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Test"</span>, i, <span class="string">"Prediction:"</span>, np.argmax(Ytr[nn_index]), \</span><br><span class="line">            <span class="string">"True Class:"</span>, np.argmax(Yte[i]))</span><br><span class="line">        <span class="comment"># Calculate accuracy</span></span><br><span class="line">        <span class="keyword">if</span> np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):</span><br><span class="line">            accuracy += <span class="number">1.</span>/len(Xte)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Done!"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy)</span><br></pre></td></tr></table></figure><pre><code>190Test 0 Prediction: 9 True Class: 9475Test 1 Prediction: 5 True Class: 53152Test 2 Prediction: 7 True Class: 72413Test 3 Prediction: 2 True Class: 21088Test 4 Prediction: 2 True Class: 21427Test 5 Prediction: 2 True Class: 24743Test 6 Prediction: 7 True Class: 74826Test 7 Prediction: 6 True Class: 64099Test 8 Prediction: 5 True Class: 52421Test 9 Prediction: 5 True Class: 5Done!Accuracy: 0.9999999999999999</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://blog.csdn.net/zSean/article/details/75097937" target="_blank" rel="noopener">Tenosrflow中基本算术运算函数</a></p><p>[2] <a href="https://blog.csdn.net/qq_35082030/article/details/60965320" target="_blank" rel="noopener">统计学习方法——K近邻模型</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;最近邻算法简介&quot;&gt;&lt;a href=&quot;#最近邻算法简介&quot; class=&quot;headerlink&quot; title=&quot;最近邻算法简介&quot;&gt;&lt;/a&gt;最近邻算法简介&lt;/h3&gt;&lt;p&gt;k近邻模型的核心就是使用一种距离度量，获得距离目标点最近的k个点，根据分类决策规则，决定目标点的分类。[2]&lt;/p&gt;
&lt;p&gt;距离度量(L1范数)：&lt;br&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/c795dda5fdeb34d262274efb237e1b3b.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;K值选择：这里k为10。&lt;/p&gt;
&lt;p&gt;分类决策规则：k近邻的分类决策规则是最为常见的简单多数规则，也就是在最近的K个点中，哪个标签数目最多，就把目标点的标签归于哪一类。&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow基本模型之Logistic回归</title>
    <link href="http://xywang93.github.io.git/2018/03/03/DeepLearning/TensorFlow/03Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8BLogistic%E5%9B%9E%E5%BD%92/"/>
    <id>http://xywang93.github.io.git/2018/03/03/DeepLearning/TensorFlow/03Tensorflow基本模型之Logistic回归/</id>
    <published>2018-03-02T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:38.406Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Logistic-回归-简介"><a href="#Logistic-回归-简介" class="headerlink" title="Logistic 回归 简介"></a>Logistic 回归 简介</h3><h4 id="Logistic模型"><a href="#Logistic模型" class="headerlink" title="Logistic模型"></a>Logistic模型</h4><p><img src="http://xukeqiniu.xukeai.cn/1b2eecc284aec268c9f7cc1907376184.png" alt="Logistic模型"></p><p><img src="http://xukeqiniu.xukeai.cn/65de6987629cad90b435ca677a87b5a5.png" alt="Logistic模型图解"></p><a id="more"></a><h4 id="损失函数（交叉熵损失）"><a href="#损失函数（交叉熵损失）" class="headerlink" title="损失函数（交叉熵损失）"></a>损失函数（交叉熵损失）</h4><p><img src="http://xukeqiniu.xukeai.cn/46bfcfb36ca46ba0b0c62d2ef98d9886.png" alt="交叉熵"></p><h4 id="softmax多分类"><a href="#softmax多分类" class="headerlink" title="softmax多分类"></a>softmax多分类</h4><p><img src="http://xukeqiniu.xukeai.cn/7aa15bacea22f7e5f58bf50ae9faf7cb.png" alt="softmax"></p><h3 id="Tensorflow-Logistic回归"><a href="#Tensorflow-Logistic回归" class="headerlink" title="Tensorflow Logistic回归"></a>Tensorflow Logistic回归</h3><h4 id="导入-mnist数据集"><a href="#导入-mnist数据集" class="headerlink" title="导入 mnist数据集"></a>导入 mnist数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Import MINST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">25</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">1</span></span><br></pre></td></tr></table></figure><h4 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf Graph Input</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>]) <span class="comment"># mnist data image of shape 28*28=784  # tf.placeholder(dtype, shape=None, name=None)</span></span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>]) <span class="comment"># 0-9 digits recognition =&gt; 10 classes</span></span><br><span class="line"><span class="comment"># Set model weights</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"><span class="comment"># Construct model</span></span><br><span class="line">pred = tf.nn.softmax(tf.matmul(x, W) + b) <span class="comment"># Softmax</span></span><br></pre></td></tr></table></figure><h4 id="定义损失函数（交叉熵）"><a href="#定义损失函数（交叉熵）" class="headerlink" title="定义损失函数（交叉熵）"></a>定义损失函数（交叉熵）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Minimize error using cross entropy</span></span><br><span class="line">cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><blockquote><p><strong> 补充: reduction_indices参数[1]</strong></p><p>在tensorflow的使用中，经常会使用tf.reduce_mean,tf.reduce_sum等函数，在函数中，有一个reduction_indices参数，表示函数的处理维度，直接上图，一目了然：<br><img src="http://xukeqiniu.xukeai.cn/0737761612df365b3afce08b9d619655.png" alt=""></p><p>tf.reduce_sum(x)  ==&gt; 如果不指定第二个参数，那么就在所有的元素求和</p><p>tf.reduce_sum(x, 0)  ==&gt; 指定第二个参数为0，则第一维的元素求和，即每一列求和</p><p>tf.reduce_sum(x, 1) ==&gt; 指定第二个参数为1，则第二维的元素求和，即每一行求和</p></blockquote><h4 id="设置优化器（SGD）"><a href="#设置优化器（SGD）" class="headerlink" title="设置优化器（SGD）"></a>设置优化器（SGD）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gradient Descent</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the variables (i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Training cycle</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        avg_cost = <span class="number">0.</span></span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        <span class="comment"># Loop over all batches</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># Fit training using batch data</span></span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_xs,</span><br><span class="line">                                                          y: batch_ys&#125;)</span><br><span class="line">            <span class="comment"># Compute average loss</span></span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">        <span class="comment"># Display logs per epoch step</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Calculate accuracy for 3000 examples</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) <span class="comment"># cast(x, dtype, name=None) 将x的数据格式转化成dtype.</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images[:<span class="number">3000</span>], y: mnist.test.labels[:<span class="number">3000</span>]&#125;))</span><br></pre></td></tr></table></figure><pre><code>Epoch: 0001 cost= 1.183862086Epoch: 0002 cost= 0.665185326Epoch: 0003 cost= 0.552937392Epoch: 0004 cost= 0.498404927Epoch: 0005 cost= 0.465865389Epoch: 0006 cost= 0.442440675Epoch: 0007 cost= 0.425578112Epoch: 0008 cost= 0.412035317Epoch: 0009 cost= 0.401478231Epoch: 0010 cost= 0.392347213Epoch: 0011 cost= 0.384493829Epoch: 0012 cost= 0.377989292Epoch: 0013 cost= 0.372704204Epoch: 0014 cost= 0.366971537Epoch: 0015 cost= 0.362937522Epoch: 0016 cost= 0.358783882Epoch: 0017 cost= 0.355023325Epoch: 0018 cost= 0.351152160Epoch: 0019 cost= 0.348280402Epoch: 0020 cost= 0.345466763Epoch: 0021 cost= 0.342640696Epoch: 0022 cost= 0.340194521Epoch: 0023 cost= 0.338306610Epoch: 0024 cost= 0.335532565Epoch: 0025 cost= 0.333705268Optimization Finished!Accuracy: 0.889</code></pre><blockquote><p><strong>补充 tf.argmax:[2]</strong></p><p>简单的说，tf.argmax就是返回最大的那个数值所在的下标。 </p><p>tf.argmax(array, 1)和tf.argmax(array, 0)的区别看下面的例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])</span><br><span class="line">np.argmax(test, 0)　　　＃输出：array([3, 3, 1]</span><br><span class="line">np.argmax(test, 1)　　　＃输出：array([2, 2, 0, 0]</span><br></pre></td></tr></table></figure></p></blockquote><ul><li><p>axis = 0:<br>　　你就这么想，0是最大的范围，所有的数组都要进行比较，只是比较的是这些数组相同位置上的数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test[0] = array([1, 2, 3])</span><br><span class="line">test[1] = array([2, 3, 4])</span><br><span class="line">test[2] = array([5, 4, 3])</span><br><span class="line">test[3] = array([8, 7, 2])</span><br><span class="line"># output   :    [3, 3, 1]</span><br></pre></td></tr></table></figure></li><li><p>axis = 1:<br>　　等于1的时候，比较范围缩小了，只会比较每个数组内的数的大小，结果也会根据有几个数组，产生几个结果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test[0] = array([1, 2, 3])  #2</span><br><span class="line">test[1] = array([2, 3, 4])  #2</span><br><span class="line">test[2] = array([5, 4, 3])  #0</span><br><span class="line">test[3] = array([8, 7, 2])  #0</span><br></pre></td></tr></table></figure></li></ul><h3 id="Tensorflow-Eager-API-Logistic回归"><a href="#Tensorflow-Eager-API-Logistic回归" class="headerlink" title="Tensorflow Eager API Logistic回归"></a>Tensorflow Eager API Logistic回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br></pre></td></tr></table></figure><h4 id="设置-Eager-API"><a href="#设置-Eager-API" class="headerlink" title="设置 Eager API"></a>设置 Eager API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set Eager API</span></span><br><span class="line">tfe.enable_eager_execution()</span><br></pre></td></tr></table></figure><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MNIST data</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>Extracting ./data/train-images-idx3-ubyte.gzExtracting ./data/train-labels-idx1-ubyte.gzExtracting ./data/t10k-images-idx3-ubyte.gzExtracting ./data/t10k-labels-idx1-ubyte.gz</code></pre><h4 id="设置变量"><a href="#设置变量" class="headerlink" title="设置变量"></a>设置变量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br><span class="line">display_step = <span class="number">100</span></span><br></pre></td></tr></table></figure><h4 id="调用-Dataset-API-读取数据-3"><a href="#调用-Dataset-API-读取数据-3" class="headerlink" title="调用 Dataset API 读取数据[3]"></a>调用 Dataset API 读取数据[3]</h4><p>Dataset API是TensorFlow 1.3版本中引入的一个新的模块，主要服务于数据读取，构建输入数据的pipeline。</p><p>如果想要用到Eager模式，就必须要使用Dataset API来读取数据。</p><p>之前有用 placeholder 读取数据，tf.data.Dataset.from_tensor_slices 是另一种方式，其主要作用是切分传入 Tensor 的第一个维度，生成相应的 dataset。以下面的例子为例，是对 mnist.train.images 按batch_size 进行切分。</p><p>在Eager模式中，创建Iterator的方式是通过 tfe.Iterator(dataset) 的形式直接创建Iterator并迭代。迭代时可以直接取出值，不需要使用sess.run()。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iterator for the dataset</span></span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">    (mnist.train.images, mnist.train.labels)).batch(batch_size)</span><br><span class="line">dataset_iter = tfe.Iterator(dataset)</span><br></pre></td></tr></table></figure><h4 id="定义模型（公式-损失函数-准确率计算）"><a href="#定义模型（公式-损失函数-准确率计算）" class="headerlink" title="定义模型（公式+损失函数+准确率计算）"></a>定义模型（公式+损失函数+准确率计算）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Variables</span></span><br><span class="line">W = tfe.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]), name=<span class="string">'weights'</span>)</span><br><span class="line">b = tfe.Variable(tf.zeros([<span class="number">10</span>]), name=<span class="string">'bias'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Logistic regression (Wx + b)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.matmul(inputs, W) + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cross-Entropy loss function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(inference_fn, inputs, labels)</span>:</span></span><br><span class="line">    <span class="comment"># Using sparse_softmax cross entropy</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        logits=inference_fn(inputs), labels=labels))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate accuracy</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy_fn</span><span class="params">(inference_fn, inputs, labels)</span>:</span></span><br><span class="line">    prediction = tf.nn.softmax(inference_fn(inputs))</span><br><span class="line">    correct_pred = tf.equal(tf.argmax(prediction, <span class="number">1</span>), labels)</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br></pre></td></tr></table></figure><blockquote><p><strong>补充:  tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None):</strong>[4]</p><ul><li>第一个参数logits：就是神经网络最后一层的输出，如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes</li><li>第二个参数labels：实际的标签，大小同上</li></ul><p>执行下面两步操作：<br><img src="http://xukeqiniu.xukeai.cn/4b8a342d4d19c2420e5458e1e1987900.png" alt=""></p><p>返回值是一个向量,对向量求 tf.reduce_mean，得到loss。</p></blockquote><h4 id="设置优化器（SGD）-1"><a href="#设置优化器（SGD）-1" class="headerlink" title="设置优化器（SGD）"></a>设置优化器（SGD）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGD Optimizer</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute gradients</span></span><br><span class="line">grad = tfe.implicit_gradients(loss_fn)</span><br></pre></td></tr></table></figure><h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training</span></span><br><span class="line">average_loss = <span class="number">0.</span></span><br><span class="line">average_acc = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Iterate through the dataset</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        d = dataset_iter.next()</span><br><span class="line">    <span class="keyword">except</span> StopIteration:  <span class="comment"># try...except，except用于处理异常</span></span><br><span class="line">        <span class="comment"># Refill queue</span></span><br><span class="line">        dataset_iter = tfe.Iterator(dataset)</span><br><span class="line">        d = dataset_iter.next()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Images</span></span><br><span class="line">    x_batch = d[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Labels</span></span><br><span class="line">    y_batch = tf.cast(d[<span class="number">1</span>], dtype=tf.int64)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the batch loss</span></span><br><span class="line">    batch_loss = loss_fn(logistic_regression, x_batch, y_batch)</span><br><span class="line">    average_loss += batch_loss</span><br><span class="line">    <span class="comment"># Compute the batch accuracy</span></span><br><span class="line">    batch_accuracy = accuracy_fn(logistic_regression, x_batch, y_batch)</span><br><span class="line">    average_acc += batch_accuracy</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># Display the initial cost, before optimizing</span></span><br><span class="line">        print(<span class="string">"Initial loss= &#123;:.9f&#125;"</span>.format(average_loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update the variables following gradients info</span></span><br><span class="line">    optimizer.apply_gradients(grad(logistic_regression, x_batch, y_batch))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display info</span></span><br><span class="line">    <span class="keyword">if</span> (step + <span class="number">1</span>) % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">            average_loss /= display_step</span><br><span class="line">            average_acc /= display_step</span><br><span class="line">        print(<span class="string">"Step:"</span>, <span class="string">'%04d'</span> % (step + <span class="number">1</span>), <span class="string">" loss="</span>,</span><br><span class="line">              <span class="string">"&#123;:.9f&#125;"</span>.format(average_loss), <span class="string">" accuracy="</span>,</span><br><span class="line">              <span class="string">"&#123;:.4f&#125;"</span>.format(average_acc))</span><br><span class="line">        average_loss = <span class="number">0.</span></span><br><span class="line">        average_acc = <span class="number">0.</span></span><br></pre></td></tr></table></figure><pre><code>Initial loss= 2.302585363Step: 0001  loss= 2.302585363  accuracy= 0.1172Step: 0100  loss= 0.952338576  accuracy= 0.7955Step: 0200  loss= 0.535867393  accuracy= 0.8712Step: 0300  loss= 0.485415280  accuracy= 0.8757Step: 0400  loss= 0.433947176  accuracy= 0.8843Step: 0500  loss= 0.381990731  accuracy= 0.8971Step: 0600  loss= 0.394154936  accuracy= 0.8947Step: 0700  loss= 0.391497582  accuracy= 0.8905Step: 0800  loss= 0.386373132  accuracy= 0.8945Step: 0900  loss= 0.332039326  accuracy= 0.9096Step: 1000  loss= 0.358993709  accuracy= 0.9002</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Evaluate model on the test image set</span></span><br><span class="line">testX = mnist.test.images</span><br><span class="line">testY = mnist.test.labels</span><br><span class="line"></span><br><span class="line">test_acc = accuracy_fn(logistic_regression, testX, testY)</span><br><span class="line">print(<span class="string">"Testset Accuracy: &#123;:.4f&#125;"</span>.format(test_acc))</span><br></pre></td></tr></table></figure><pre><code>Testset Accuracy: 0.9083</code></pre><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="https://www.cnblogs.com/likethanlove/p/6547405.html" target="_blank" rel="noopener">tensorflow reduction_indices理解</a></p><p>[2] <a href="https://blog.csdn.net/qq575379110/article/details/70538051" target="_blank" rel="noopener">tf.argmax()以及axis解析</a></p><p>[3] <a href="https://blog.csdn.net/kwame211/article/details/78579035" target="_blank" rel="noopener">TensorFlow全新的数据读取方式：Dataset API入门教程</a></p><p>[4] <a href="https://blog.csdn.net/mao_xiao_feng/article/details/53382790" target="_blank" rel="noopener">【TensorFlow】tf.nn.softmax_cross_entropy_with_logits的用法</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Logistic-回归-简介&quot;&gt;&lt;a href=&quot;#Logistic-回归-简介&quot; class=&quot;headerlink&quot; title=&quot;Logistic 回归 简介&quot;&gt;&lt;/a&gt;Logistic 回归 简介&lt;/h3&gt;&lt;h4 id=&quot;Logistic模型&quot;&gt;&lt;a href=&quot;#Logistic模型&quot; class=&quot;headerlink&quot; title=&quot;Logistic模型&quot;&gt;&lt;/a&gt;Logistic模型&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/1b2eecc284aec268c9f7cc1907376184.png&quot; alt=&quot;Logistic模型&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/65de6987629cad90b435ca677a87b5a5.png&quot; alt=&quot;Logistic模型图解&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow基本模型之线性回归</title>
    <link href="http://xywang93.github.io.git/2018/03/02/DeepLearning/TensorFlow/02Tensorflow%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://xywang93.github.io.git/2018/03/02/DeepLearning/TensorFlow/02Tensorflow基本模型之线性回归/</id>
    <published>2018-03-01T16:00:00.000Z</published>
    <updated>2019-01-18T08:02:24.362Z</updated>
    
    <content type="html"><![CDATA[<h3 id="线性回归简述"><a href="#线性回归简述" class="headerlink" title="线性回归简述"></a>线性回归简述</h3><p>在这里，我们仅仅讨论单变量的线型回归模型。不对回归算法进行过多的展开。重点放在<code>Tensorflow</code>的学习上。<br>下图展示的分别是：单变量线性回归模型的公式；学习的参数；损失函数（采用的均方误差）；目标函数的优化求解（SGD）。</p><p><img src="http://xukeqiniu.xukeai.cn/e6adf19554f7aa930416fa65b1c5ff2b.png" alt=""></p><a id="more"></a><h3 id="Tensorflow-线性回归"><a href="#Tensorflow-线性回归" class="headerlink" title="Tensorflow 线性回归"></a>Tensorflow 线性回归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">rng = np.random</span><br></pre></td></tr></table></figure><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数设置</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">training_epochs = <span class="number">1000</span></span><br><span class="line">display_step = <span class="number">50</span></span><br></pre></td></tr></table></figure><h4 id="生成训练数据"><a href="#生成训练数据" class="headerlink" title="生成训练数据"></a>生成训练数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line">train_X = np.asarray([<span class="number">3.3</span>,<span class="number">4.4</span>,<span class="number">5.5</span>,<span class="number">6.71</span>,<span class="number">6.93</span>,<span class="number">4.168</span>,<span class="number">9.779</span>,<span class="number">6.182</span>,<span class="number">7.59</span>,<span class="number">2.167</span>,</span><br><span class="line">                         <span class="number">7.042</span>,<span class="number">10.791</span>,<span class="number">5.313</span>,<span class="number">7.997</span>,<span class="number">5.654</span>,<span class="number">9.27</span>,<span class="number">3.1</span>])</span><br><span class="line">train_Y = np.asarray([<span class="number">1.7</span>,<span class="number">2.76</span>,<span class="number">2.09</span>,<span class="number">3.19</span>,<span class="number">1.694</span>,<span class="number">1.573</span>,<span class="number">3.366</span>,<span class="number">2.596</span>,<span class="number">2.53</span>,<span class="number">1.221</span>,</span><br><span class="line">                         <span class="number">2.827</span>,<span class="number">3.465</span>,<span class="number">1.65</span>,<span class="number">2.904</span>,<span class="number">2.42</span>,<span class="number">2.94</span>,<span class="number">1.3</span>])</span><br><span class="line">n_samples = train_X.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h4 id="构造线型回归模型"><a href="#构造线型回归模型" class="headerlink" title="构造线型回归模型"></a>构造线型回归模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf 图的输入</span></span><br><span class="line">X = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">Y = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line"><span class="comment"># 设置模型的权重与偏置</span></span><br><span class="line">W = tf.Variable(rng.randn(), name=<span class="string">"weight"</span>)</span><br><span class="line">b = tf.Variable(rng.randn(), name=<span class="string">"bias"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个线性模型</span></span><br><span class="line">pred = tf.add(tf.multiply(X, W), b)</span><br></pre></td></tr></table></figure><h4 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 损失函数设置为均方误差</span></span><br><span class="line">cost = tf.reduce_sum(tf.pow(pred-Y, <span class="number">2</span>))/(<span class="number">2</span>*n_samples)</span><br></pre></td></tr></table></figure><h4 id="定义优化方法"><a href="#定义优化方法" class="headerlink" title="定义优化方法"></a>定义优化方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化变量(i.e. assign their default value)</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fit all training data</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</span><br><span class="line">        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> zip(train_X, train_Y):</span><br><span class="line">            sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#现实每50轮迭代的结果</span></span><br><span class="line">        <span class="keyword">if</span> (epoch+<span class="number">1</span>) % display_step == <span class="number">0</span>:</span><br><span class="line">            c = sess.run(cost, feed_dict=&#123;X: train_X, Y:train_Y&#125;)</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, <span class="string">"&#123;:.9f&#125;"</span>.format(c), \</span><br><span class="line">                <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>, sess.run(b))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">    training_cost = sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Training cost="</span>, training_cost, <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>, sess.run(b), <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#绘图</span></span><br><span class="line">    plt.plot(train_X, train_Y, <span class="string">'ro'</span>, label=<span class="string">'Original data'</span>)</span><br><span class="line">    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=<span class="string">'Fitted line'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><hr><pre><code>Epoch: 0050 cost= 0.160369754 W= 0.41108337 b= -0.36027926Epoch: 0100 cost= 0.150733337 W= 0.40147883 b= -0.2911848Epoch: 0150 cost= 0.142209828 W= 0.39244553 b= -0.22619964Epoch: 0200 cost= 0.134670869 W= 0.38394934 b= -0.16507955Epoch: 0250 cost= 0.128002644 W= 0.37595856 b= -0.10759445Epoch: 0300 cost= 0.122104712 W= 0.36844307 b= -0.05352829Epoch: 0350 cost= 0.116888084 W= 0.3613746 b= -0.0026777028Epoch: 0400 cost= 0.112274118 W= 0.35472643 b= 0.04514854Epoch: 0450 cost= 0.108193211 W= 0.34847358 b= 0.09013041Epoch: 0500 cost= 0.104583815 W= 0.34259278 b= 0.13243689Epoch: 0550 cost= 0.101391472 W= 0.33706158 b= 0.17222734Epoch: 0600 cost= 0.098568030 W= 0.33185956 b= 0.20965117Epoch: 0650 cost= 0.096070863 W= 0.32696673 b= 0.2448493Epoch: 0700 cost= 0.093862340 W= 0.32236505 b= 0.2779539Epoch: 0750 cost= 0.091909051 W= 0.3180369 b= 0.3090903Epoch: 0800 cost= 0.090181611 W= 0.31396636 b= 0.33837357Epoch: 0850 cost= 0.088653855 W= 0.31013775 b= 0.365916Epoch: 0900 cost= 0.087302707 W= 0.3065368 b= 0.39182106Epoch: 0950 cost= 0.086107843 W= 0.30315018 b= 0.41618422Epoch: 1000 cost= 0.085051164 W= 0.29996493 b= 0.43909845Optimization Finished!Training cost= 0.085051164 W= 0.29996493 b= 0.43909845</code></pre><p><img src="http://xukeqiniu.xukeai.cn/2b84f0eb5acb81fa3b14cf10bfdd0fdf.png" alt="拟合曲线"></p><h3 id="Tensorflow-线性回归（Eager-API）"><a href="#Tensorflow-线性回归（Eager-API）" class="headerlink" title="Tensorflow 线性回归（Eager API）"></a>Tensorflow 线性回归（Eager API）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br></pre></td></tr></table></figure><h4 id="设置Eager-API"><a href="#设置Eager-API" class="headerlink" title="设置Eager API"></a>设置Eager API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set Eager API</span></span><br><span class="line">tfe.enable_eager_execution()</span><br></pre></td></tr></table></figure><h4 id="生成训练数据-1"><a href="#生成训练数据-1" class="headerlink" title="生成训练数据"></a>生成训练数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Training Data</span></span><br><span class="line">train_X = [<span class="number">3.3</span>, <span class="number">4.4</span>, <span class="number">5.5</span>, <span class="number">6.71</span>, <span class="number">6.93</span>, <span class="number">4.168</span>, <span class="number">9.779</span>, <span class="number">6.182</span>, <span class="number">7.59</span>, <span class="number">2.167</span>,</span><br><span class="line">           <span class="number">7.042</span>, <span class="number">10.791</span>, <span class="number">5.313</span>, <span class="number">7.997</span>, <span class="number">5.654</span>, <span class="number">9.27</span>, <span class="number">3.1</span>]</span><br><span class="line">train_Y = [<span class="number">1.7</span>, <span class="number">2.76</span>, <span class="number">2.09</span>, <span class="number">3.19</span>, <span class="number">1.694</span>, <span class="number">1.573</span>, <span class="number">3.366</span>, <span class="number">2.596</span>, <span class="number">2.53</span>, <span class="number">1.221</span>,</span><br><span class="line">           <span class="number">2.827</span>, <span class="number">3.465</span>, <span class="number">1.65</span>, <span class="number">2.904</span>, <span class="number">2.42</span>, <span class="number">2.94</span>, <span class="number">1.3</span>]</span><br><span class="line">n_samples = len(train_X)</span><br></pre></td></tr></table></figure><h4 id="设置参数"><a href="#设置参数" class="headerlink" title="设置参数"></a>设置参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">display_step = <span class="number">100</span></span><br><span class="line">num_steps = <span class="number">1000</span></span><br></pre></td></tr></table></figure><h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Weight and Bias</span></span><br><span class="line">W = tfe.Variable(np.random.randn())</span><br><span class="line">b = tfe.Variable(np.random.randn())</span><br></pre></td></tr></table></figure><h4 id="构建线性回归模型"><a href="#构建线性回归模型" class="headerlink" title="构建线性回归模型"></a>构建线性回归模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Linear regression (Wx + b)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> inputs * W + b</span><br></pre></td></tr></table></figure><h4 id="定义损失函数（均方误差）"><a href="#定义损失函数（均方误差）" class="headerlink" title="定义损失函数（均方误差）"></a>定义损失函数（均方误差）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mean square error</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_square_fn</span><span class="params">(model_fn, inputs, labels)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_sum(tf.pow(model_fn(inputs) - labels, <span class="number">2</span>)) / (<span class="number">2</span> * n_samples)</span><br></pre></td></tr></table></figure><h4 id="调用优化器（SGD）"><a href="#调用优化器（SGD）" class="headerlink" title="调用优化器（SGD）"></a>调用优化器（SGD）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGD Optimizer</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute gradients</span></span><br><span class="line">grad = tfe.implicit_gradients(mean_square_fn)</span><br></pre></td></tr></table></figure><h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initial cost, before optimizing</span></span><br><span class="line">print(<span class="string">"Initial cost= &#123;:.9f&#125;"</span>.format(</span><br><span class="line">    mean_square_fn(linear_regression, train_X, train_Y)),</span><br><span class="line">    <span class="string">"W="</span>, W.numpy(), <span class="string">"b="</span>, b.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line"></span><br><span class="line">    optimizer.apply_gradients(grad(linear_regression, train_X, train_Y))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (step + <span class="number">1</span>) % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (step + <span class="number">1</span>), <span class="string">"cost="</span>,</span><br><span class="line">              <span class="string">"&#123;:.9f&#125;"</span>.format(mean_square_fn(linear_regression, train_X, train_Y)),</span><br><span class="line">              <span class="string">"W="</span>, W.numpy(), <span class="string">"b="</span>, b.numpy())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Graphic display</span></span><br><span class="line">plt.plot(train_X, train_Y, <span class="string">'ro'</span>, label=<span class="string">'Original data'</span>)</span><br><span class="line">plt.plot(train_X, np.array(W * train_X + b), label=<span class="string">'Fitted line'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><pre><code>Initial cost= 34.570991516 W= -0.89701766 b= 0.09529222Epoch: 0001 cost= 10.462613106 W= -0.3445475 b= 0.17387688Epoch: 0100 cost= 0.090614140 W= 0.31795835 b= 0.32859808Epoch: 0200 cost= 0.087662779 W= 0.31037292 b= 0.38237545Epoch: 0300 cost= 0.085347883 W= 0.30365503 b= 0.4300023Epoch: 0400 cost= 0.083532237 W= 0.29770544 b= 0.472182Epoch: 0500 cost= 0.082108125 W= 0.29243633 b= 0.5095375Epoch: 0600 cost= 0.080991186 W= 0.28776988 b= 0.54262066Epoch: 0700 cost= 0.080115080 W= 0.28363708 b= 0.5719204Epoch: 0800 cost= 0.079427943 W= 0.27997696 b= 0.5978689Epoch: 0900 cost= 0.078888975 W= 0.27673545 b= 0.6208498Epoch: 1000 cost= 0.078466244 W= 0.27386472 b= 0.64120203</code></pre><p><img src="http://xukeqiniu.xukeai.cn/f0cd930820fa093c5b66e660710f6f77.png" alt="拟合曲线"></p><p><img src="http://xukeqiniu.xukeai.cn/1b2eecc284aec268c9f7cc1907376184.png" alt="Logistic模型"></p><p><img src="http://xukeqiniu.xukeai.cn/65de6987629cad90b435ca677a87b5a5.png" alt="Logistic模型图解"></p><p><img src="http://xukeqiniu.xukeai.cn/46bfcfb36ca46ba0b0c62d2ef98d9886.png" alt="交叉熵"></p><p><img src="http://xukeqiniu.xukeai.cn/7aa15bacea22f7e5f58bf50ae9faf7cb.png" alt="softmax"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="noopener">TensorFlow-Examples</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;线性回归简述&quot;&gt;&lt;a href=&quot;#线性回归简述&quot; class=&quot;headerlink&quot; title=&quot;线性回归简述&quot;&gt;&lt;/a&gt;线性回归简述&lt;/h3&gt;&lt;p&gt;在这里，我们仅仅讨论单变量的线型回归模型。不对回归算法进行过多的展开。重点放在&lt;code&gt;Tensorflow&lt;/code&gt;的学习上。&lt;br&gt;下图展示的分别是：单变量线性回归模型的公式；学习的参数；损失函数（采用的均方误差）；目标函数的优化求解（SGD）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://xukeqiniu.xukeai.cn/e6adf19554f7aa930416fa65b1c5ff2b.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/categories/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/categories/Deep-Learning/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="http://xywang93.github.io.git/tags/TensorFlow/"/>
    
      <category term="Deep Learning" scheme="http://xywang93.github.io.git/tags/Deep-Learning/"/>
    
  </entry>
  
</feed>
